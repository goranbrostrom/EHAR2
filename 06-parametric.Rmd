# Parametric Models

So far we have exclusively studied nonparametric methods for survival
analysis. This is a tradition that has its roots in medical (cancer)
research. In technical applications, on the other hand, parametric models
dominate; of special interest is the *Weibull* model \citep{ww51}. The text
book by \cite{jl03} is a good source for the study of parametric survival
models. 
More technical detail about parametric distributions is found in Appendix B .

In **eha** three kinds of parametric models are available; the
*proportional hazards*\index{model!parametric!PH} and the *accelerated failure
  time*\index{model!parametric!AFT} in continuous time, and the *discrete 
  time proportional hazards*\index{model!parametric!discrete}  models.


## Proportional Hazards Models

A proportional hazards family of distributions is generated from one
specific continuous distribution by multiplying the hazard function of that
distribution by a strictly positive constant, and letting that constant
vary over the full positive real line. So, if $h_0$ is the hazard function
corresponding to the generating distribution, the family of distributions
can be described by saying that $h$ is a member of the family if

\begin{equation*}
  h_1(t) = c h_0(t) \quad \text{for some } c > 0, \; \text{and all } t > 0.
\end{equation*}
Note that it is possible to choose any hazard function (on the positive
real line) as the generating function. The resulting proportional hazards
class of distributions may or may not be a well recognized family of
distributions. 

In **R**, the package **eha** can fit parametric proportional hazards
models. In the following subsections, the possibilities are examined.

The parametric distribution functions that can be used as the baseline
distribution in the function `phreg` are the *Weibull*,
*Piecewise constant hazard* (`pch`), *Lognormal*,
*Loglogistic*, 
*Extreme value* and the 
*Gompertz* distributions. The *lognormal* and
*loglogistic* distributions 
allow for hazard functions that are first increasing to a maximum and then
decreasing, while the other distributions all have monotone hazard
functions. See Figure \@ref(fig:6hazs) for a selection.

```{r 6hazs,fig.cap = "Selected hazard functions.",echo=FALSE,fig.height=5,message=FALSE}
library(eha)
oldpar <- par(mfrow = c(2, 2))
x <- seq(0, 10, length = 1000)
plot(x, hweibull(x, shape = 2, scale = 2), main = "Weibull, p = 2", type = "l", 
     ylab = "", xlab = "Time")
plot(x, hweibull(x, shape = 1/2, scale = 2), main = "Weibull, p = 1/2", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 2))
plot(x, hlnorm(x, shape = 1, scale = 2), main = "Lognormal", type = "l",
     ylab = "", xlab = "Time")
y <- hgompertz(x, scale = 5, shape = 1)
plot(x, y, main = "Gompertz", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, max(y)))
par(oldpar)
```

The available survival distributions are described in detail in Appendix \@ref(app:B).

In the following, we start by looking at a typical choice of survivor
distribution, the *Weibull* distribution, and a rather atypical
one, the *Lognormal* distribution. One reason for the popularity of
the *Weibull* survivor distribution is that the survivor and hazard functions
both have fairly simple closed forms, and the proportional hazards models
built from a *Weibull* distribution stays within the family of *Weibull*
distributions. The *Lognormal* distribution, on the other hand, does not have
any of these nice properties.

After studying the *Weibull* and *Lognormal* distributions, we look at some
typical data sets and find that different distributions step forward as
good ones in different situations. One exception is the 
*Piecewise constant  hazard (pch)* distribution, which is flexible enough to give a
good fit to any given distribution. It is in this sense close to the
non-parametric approach used in Cox regression. 


### The Weibull model

From Appendix B.3.4 we know that the family of *Weibull* distributions
may be defined by the family of hazard functions

\begin{equation}
h(t; p, \lambda) = \frac{p}{\lambda} \biggl(\frac{t}{\lambda}\biggr)^{p-1},
\quad t, p, \lambda > 0.
(\#eq:6weibull)
\end{equation}

If we start with

$$
h_1(t) = t^{p-1}, \quad p \ge 0, \; t > 0
$$

for a *fixed* $p$, and generate a proportional hazards family from there,

\begin{equation*}
h_c = c h_1(t), \quad c, t > 0,
\end{equation*}

we get

\begin{equation}\label{eq:6weibdist}
h_c = c t^{p-1} = \frac{p}{\lambda} \biggl(\frac{t}{\lambda}\biggr)^{p-1}
\end{equation}

by setting
$$
c = \frac{p}{\lambda^p},
$$

which shows that for each fixed $p > 0$, a proportional hazards family is
generated by varying $\lambda$ in \eqref{eq:6weibdist}. On the other hand,
if we pick two members from the family \eqref{eq:6weibdist} with
*different* values of $p$, they would not be proportional. 

To summarize, the *Weibull* family of distributions is not *one* family
of proportional hazards distributions, but a *collection* of families
of proportional hazards. The collection is indexed by $p > 0$. It is true,
though, that all the families are closed under the *Weibull* distribution. 

The proportional hazards regression model with a *Weibull* baseline
distribution as obtained by multiplying \@ref(eq:6weibull) by $\exp(\beta x)$:

\begin{equation}
  h(t; x, \lambda, p, \beta) = \frac{p}{\lambda}
  \biggl(\frac{t}{\lambda}\biggr)^{p-1} e^{\beta x}, \quad t > 0.
(\#eq:6weibreg)
\end{equation}

The function **phreg** in package **eha** fits models like
\@ref(eq:6weibreg) by default.

```{example name = "Length of birth intervals, Weibull model."}
```
The data set **fert** in the **R** package **eha** contains
  birth intervals for married women in 19th century Skellefteå. It is
  described in detail in Chapter 1.
  Here only the
  intervals starting with the first birth for each woman are considered.
  First the data are extracted and examined.

```{r fert6.12}
library(eha)
data(fert)
f12 <- fert[fert$parity == 1, ]
f12$Y <- Surv(f12$next.ivl, f12$event)
head(f12)
```

Some women never got a second child, for instance the first woman (**id  = 1**) above. 
Also, note the just created variable $Y$; it is printed
as a vector, with some values having a trailing``$+$''; Those are the
censored observations (**event = 0**). But $Y$ is really a matrix, in
this case with two columns. The first column equals **next.ivl** and the
second is **event**.

```{r Y12}
is.matrix(f12$Y)
dim(f12$Y)
```

Next, the proportional hazards *Weibull* regression is fit and the
likelihood ratio 
tests for an effect of each covariate is performed. 
Remember that the function *drop1*\index{Functions!*drop1*} is the
one to use for the latter task. 

```{r phweib}
fit.w <- phreg(Y ~ age + year + ses, data = f12)
drop1(fit.w, test = "Chisq")
```

Three variables are included, two are extremely significant, $age$,
mother's age at start of the interval, and $year$, the calendar year at
the start of the interval. The variable *ses*, socio-economic status,
is also statistically significant, but not to the extent of the other two. 

**Note:** When we say that a variable is "significant", we are rejecting the
hypothesis that the true parameter value attached to it is equal to zero.

Now take a look at the actual fit.

```{r fitw, echo = FALSE}
fit.w
kof <- fit.w$coef[1]
```

The estimated coefficient for **age**, 
`r round(kof, digits = 3)`, is negative, indicating lower
risk of a birth with increasing age. The effect of calendar time is
positive and indicating an increase in the intensity of giving birth of the
size approximately 5 per thousand. Regarding *socio-economic status*,
*farmer* wives have the highest fertility and the *upper* class the
lowest, when it comes to getting a second child. Finally, we see that the
estimate for "$\log(\text{shape})$" is positive ($0.299$) and significantly
different from zero. This means that the intensity is increasing with
duration, see Figure \@ref(fig:weibfert6).

```{r weibfert6, fig.cap="Estimated Weibull baseline distribution for length of interval between first and second birth.", fig.scap = "Weibull birth intervals"}
oldpar <- par(mfrow = c(1, 2))
plot(fit.w, fn = "haz")
plot(fit.w, fn = "cum")
par(oldpar)
``` 

$\Box$

### The Lognormal model

An example of a more complex family of distributions with the proportional
hazards property is to start with a
*lognormal* distribution.
The family of *lognormal* distributions is characterized by the fact that
taking the natural logarithm of a random variable from the family gives a
random variable from the family of *Normal* distributions. Both the
hazard and the survivor functions lack closed forms.
Furthermore, multiplying the hazard function by a constant not equal to 1
leads to a non-lognormal hazard function.

So, while we for the *Weibull* family of distributions found subfamilies with
proportional hazards by keeping $p$ fixed and varying $\lambda$, for the
*lognormal* distribution we cannot use the same trick. Instead, by
multiplying the hazard function by a constant we arrive at a three-parameter
family of distributions. 
The lognormal proportional hazards model is
possible to fit with the function **phreg**.

```{example, name = "Length of birth intervals, Lognormal model."}
```
  The data are already extracted and examined in the previous example.
  Next, the proportional hazards *lognormal* regression and the likelihood
  ratio tests for an effect of each covariate are given.

```{r phlognorm}
fit.lognorm <- phreg(Y ~ age + year + ses, data = f12, 
                     dist = "lognormal")
drop1(fit.lognorm, test = "Chisq")
``` 


Three variables are included, two are extremely significant. First the
variable **age**, mother's age at start of the interval, and second
  *year*, the calendar year at the start of the interval. The variable 
  *ses*, socio-economic status, is also significant, but to the extent of
the previous two. 

Now take a look at the actual fit.
```{r fitlogist}
fit.lognorm
``` 

```{r secret16,echo=FALSE}
kof <- fit.lognorm$coef[1]
``` 

The estimated coefficient for age, 
`r round(kof, digits = 3)`, is negative, indicating lower
risk of a birth with increasing age. The effect of calendar time is
positive and indicating an increase in the intensity of giving birth of the
size approximately 4 per thousand. Regarding *socio-economic status*,
farmer wives have the highest fertility and the unknown and
upper classes the 
lowest, when it comes to getting a second child. Finally, in
Figure \@ref(fig:lognormfert6) it is clear that the hazard function is first
increasing to a maximum around 3 years after the first birth and then
decreasing. This is quite a different picture compared to the Weibull fit
in Figure \@ref(fig:weibfert6)! This shows the weakness with the Weibull model; it
allows only for *monotone* (increasing or decreasing) hazard
functions.

```{r lognormfert6,fig.cap="Estimated lognormal baseline distribution for length of birth intervals.",echo=FALSE}
oldpar <- par(mfrow = c(1, 2))
plot(fit.lognorm, fn = "haz", main = "Hazard function")
plot(fit.lognorm, fn = "cum", main = "Cumulative hazard function")
par(oldpar)
``` 

It was created using
```{r crepl1,echo=TRUE,fig=FALSE}
oldpar <- par(mfrow = c(1, 2))
plot(fit.lognorm, fn = "haz", main = "Hazard function")
plot(fit.lognorm, fn = "cum", main = "Cumulative hazard function")
par(oldpar)
``` 
$\Box$


### The Loglogistic model

### The Extreme Value model

### The Gompertz model

## Comparing the Weibull and Lognormal fits

From looking at Figures \@ref(fig:weibfert6) and \@ref(fig:lognormfert6), it
seems obvious that at least one of the fits must be less good: The
*Weibull* cumulative hazards
curve is convex and the *lognormal* one is concave, and the estimates
of the two hazard
functions are very different in shape. There are two direct ways
of comparing the fits.

The first way to compare is to look at the maximized log likelihoods. For
the *Weibull* fit it is `r round(fit.w$loglik[2], digits = 2)` and for
the *lognormal* fit it is 
`r round(fit.lognorm$loglik[2], digits = 2)`. The rule is that the
largest value wins, so the *lognormal* model is the clear winner. Note that
we do not claim to perform a formal test here; a likelihood ratio test
would require that the two models we want to compare are nested, but that
is not the case here. This is (here) equivalent to using the AIC\index{AIC}
as a 
measure of comparison, because the two models have the same number of
parameters to estimate.

The second method of comparison is graphical. We can plot the cumulative
hazard functions\index{cumulative hazard function}
against the nonparametric estimate from a Cox
regression\index{Cox regression} 
fit, and judge which looks closer. It starts by fitting a Cox model with
the same covariates:\index{Functions!\fun{coxreg}}

```{r coxr6}
fit.cr <- coxreg(Y ~ age + year + ses, data = f12)
``` 

Then the function **check.dist** (from **eha*) is called. We start
with the *Weibull* fit:

```{r weibcheck6,fig.cap="Check of the Weibull model, birth intervals."}
check.dist(fit.cr, fit.w)
``` 

This fit (Figure \@ref(fig:weibcheck6)) looks very poor: One reason is that
the longest waiting time for an observed event is about 12 years, while
some women simply do not get more than one child, with a long waiting time
ending in a censoring as a result. The time span from around 12 to 25
simply has no events, and the nonparametric estimate of the hazard function
in that interval is zero. The Weibull distribution fit cannot cope with
that. 

One possibility is to try to fit a Weibull distribution only for the 12
first years of duration. Note below that after calling **age.window**,
$Y$ *must* be recreated! This is the danger with this "lazy" approach.

```{r trunc6f12cens}
f12$enter <- rep(0, NROW(f12))
f12.cens <- age.window(f12, c(0, 12), 
                       surv = c("enter", "next.ivl", "event"))
f12.cens$Y <- Surv(f12.cens$enter, f12.cens$next.ivl, 
                   f12.cens$event)
fit.wc <- phreg(Y ~ age + year + ses, data = f12.cens)
fit.c <- coxreg(Y ~ age + year + ses, data = f12.cens)
fit.wc
``` 

Compare the Weibull fit with what we had without cutting off
durations at 12. 
The comparison with the nonparametric cumulative hazards function is now
seen in Figure \@ref(fig:weibcheckcens6).

```{r weibcheckcens6, fig.cap = "Check of the Weibull model, birth intervals censored at 12.", fig.scap = "Check of the Weibull model, censored birth intervals."}
check.dist(fit.c, fit.wc)
``` 

This wasn't much better. Let's look what happens with the lognormal
distribution. 

We continue using the data censored at 12 and refit the lognormal model.

```{r trunc6f12cens.ln}
fit.lnc <- phreg(Y ~ age + year + ses, data = f12.cens, 
                 dist = "lognormal")
``` 

Then, in Figure \@ref(fig:lncheckcens6) we check the fit against the
nonparametric fit again.

```{r lncheckcens6, fig.cap="Check of the lognormal model, birth intervals censored at 12.",fig.scap="Check of the lognormal model, birth intervals"}
check.dist(fit.c, fit.lnc)
``` 

It is not very much better. In fact, this empirical distribution has
features that the ordinary parametric distributions cannot cope with: Its
hazard function starts off being zero for almost one year (birth intervals
are rarely shorter than one year), then it grows rapidly and soon decreases
towards zero again.

There is, however one parametric distribution that is flexible enough: The
piecewise constant hazard distribution.

### The piecewise constant hazard (pch) model{#pch6}

The pch distribution is flexible because you can add as many parameters as
you want. We will try it on the birth interval data. We start off with just
four intervals, and the non-censored data set.

```{r pch6fert}
fit.pch <- phreg(Surv(next.ivl, event) ~ age + year + ses, 
                 data = f12, dist = "pch", cuts = c(4, 8, 12))
fit.c <- coxreg(Surv(next.ivl, event) ~ age + year + ses, 
                data = f12)
fit.pch
``` 

Then we check the fit in Figure \@ref(fig:pchcheck6).

```{r pchcheck6,fig.cap="Check of the pch model, uncensored birth intervals."}
check.dist(fit.c, fit.pch)
``` 

This is not good enough. We need shorter intervals close to zero, so

```{r pchfert2}
fit.pch <- phreg(Surv(next.ivl, event) ~ age + year + ses, 
                 data = f12, dist = "pch", cuts = 1:13)
fit.pch
``` 

This gives us 14 intervals with constant baseline hazard, that is, 14
parameters to estimate only for the baseline hazard function! But the fit
is good, see Figure \@ref(fig:pch13check6). In fact, it is almost perfect!

```{r pch13check6,fig.cap = "Check of the pch model with thirteen constant hazard intervals, uncensored birth intervals." }
check.dist(fit.c, fit.pch)
``` 

One of the advantages with parametric models is that it is easy to study
and plot the hazard function, and not only the cumulative hazards function,
which is the dominant tool for (graphical) analysis in the case of the
nonparametric model of Cox regression\index{Cox regression}. For the
piecewise constant model 
just fitted we get the estimated hazard function in Figure \@ref(fig:pchhaz6).

```{r pchhaz6,fig.cap="The estimated hazard function for the length of birth intervals, piecewise constant hazard distribution."}
plot(fit.pch, fn = "haz")
``` 

The peak is reached during the third year of waiting, and after that the
intensity of giving birth drops fast, and it becomes zero after 12 years.

What are the implications for the estimates of the regression parameters of
the different choices of parametric model? Let us compare the regression
parameter estimates only for the three distributional assumptions, *Weibull*,
*lognormal*, and *pch*. See Table \@ref(tab:threecomp6).

```{r threecomp6,echo=FALSE}
##require(xtable)
##fit.c$coef
##fit.w$coef
##fit.lognorm$coef
##fit.pch$coef
ta <- cbind(fit.c$coef[1:5], fit.pch$coef[1:5], fit.w$coef[1:5], fit.lognorm$coef[1:5])
colnames(ta) <- c("Cox", "Pch", "Weibull", "Lognormal")
ta <- round(exp(ta), digits = 3)
knitr::kable(ta, booktabs = TRUE, caption = "Parameter estimates under different distribution assumptions for the birth intervals data.")
``` 

We can see that the differences are not large. However, the two assumptions
closest to the "truth", the Cox model and the Pch model, are very close,
while the other two have larger deviations. So an appropriate choice of
parametric model seems to be somewhat important, contrary to "common
knowledge", which says that it is not so important to have a good fit of
the baseline hazard, if the only interest lies in the regression parameters. 

### Testing the proportionality assumption with the Pch model

One advantage with the piecewise constant hazard model is that it is easy
to test 
the assumption of proportional hazards. But it cannot be done directly with
the **phreg** function. There is, however, a simple alternative. Before
we show how to do it, we must show how to utilize Poisson regression when
analyzing the pch model. We do it by reanalyzing the birth intervals data. 

First, the data set is split up after the cuts in *strata*. This
is done with the aid of the **survSplit** function in the **survival**
package. 

```{r splitsurv6}
f12 <- fert[fert$parity == 1, ]
f12$enter <- 0 # 0 expands to a vector
f12.split <- survival::survSplit(f12, cut = 1:13, start = "enter", 
                       end = "next.ivl", event = "event", 
                       episode = "ivl")
head(f12.split)
``` 

To see better what happened, we sort the new data frame by **id**
and **enter**.

```{r sortt}
f12.split <- f12.split[order(f12.split$id, f12.split$enter), ]
head(f12.split)
``` 

We see that a new variable **ivl**, is created. It tells us which
interval we are looking at. You may notice that the variables enter
and ivl happens to be equal here; it is because our intervals start
by 0, 1, 2, \ldots, 13, as given by cuts above.

In the Poisson regression to follow, we model the probability that 
$event = 1$. This probability will depend on the length of the studied
interval, here $next.ivl - enter$, which for our data mostly equals one.
The exact value we need is the logarithm of this difference, and it will be
entered as an *offset*\index{offset} in the model.

Finally, one parameter per interval is needed, corresponding to the
piecewise constant hazard function. This is accomplished by including `ivl` as
a *factor* in the model. Thus we get

```{r poispch6}
f12.split$offs <- log(f12.split$next.ivl - f12.split$enter)
f12.split$ivl <- as.factor(f12.split$ivl)
fit12.pn <- glm(event ~ offset(offs) + 
                age + year + ses + ivl, 
                family = "poisson", data = f12.split)
drop1(fit12.pn, test = "Chisq")
``` 

The piecewise cutting (ivl) is very statistically significant, but
some caution with the interpretation is recommended: The cut generated 13
parameters, and there may 
be too few events in some intervals. This easily checked with the function
**tapply**\index{Functions!tapply}, which is very handy for doing calculations on
subsets of the data frame. In this case we want to sum the number ofevents in each interval:

```{r sumivl6}
tapply(f12.split$event, f12.split$ivl, sum)
``` 

A reasonable interpretation of this is to restrict attention to the 10
first years; only one birth occurs after a longer waiting time. Then it
would be reasonable to collapse the intervals in $(7, 10]$ to one
interval. Thus

```{r collapsebir6}
fc <- age.window(f12.split, c(0, 11), 
                 surv = c("enter", "next.ivl", "event"))
levels(fc$ivl) <- c(0:6, rep("7-11", 7))
tapply(fc$event, fc$ivl, sum)
``` 

Note two things here. First the use of the function 
**age.window**\index{Functions!\fun{age.window}}, second the **levels** function. The
first makes an "age cut" in the Lexis diagram, i.e., all spells are
censored at (exact) age 11. The second is more intricate; factors can be
tricky to handle. The problem here is that after age.window, the new
data frame contains only individuals with values $(0, 1, \ldots, 11)$ on
the factor variable ***ivl**, but it is still *defined* as a factor
with 14 levels. The call to **levels** collapses the last seven to "7-11".

Now we rerun the analysis with the new data frame.

```{r rerunpch6}
fit <- glm(event ~  offset(offs) + age + year + ses + ivl, 
           family = "poisson", data = fc)
drop1(fit, test = "Chisq")
``` 

This change in categorization does not change the general conclusions about
statistical significance at all.

Now let us include interactions with ivl in the model.

```{r rerunpch62}
fit.ia <- glm(event ~  offset(offs) + (age + year + ses) * ivl, 
              family = "poisson", data = fc)
drop1(fit.ia, test = "Chisq")
``` 

There is an apparent interaction with age, which is not very
surprising; younger mothers will have a longer fertility period ahead than
old mothers. The other interactions do not seem to be very important, so we
remove them and rerun the model once again.

```{r rerun2pch6}
fit.ia <- glm(event ~  offset(offs) + year + ses + age * ivl, 
              family = "poisson", data = fc)
drop1(fit.ia, test = "Chisq")
``` 

Let us look at the parameter estimates.
```{r rer6}
summary(fit.ia)
``` 

We can see (admittedly not clearly) a tendency of decreasing intensity in
the later intervals with higher age.

This can actually also be done with **phreg** and the *Weibull* model and
fixed **shape** at one (i.e., an *exponential* model).

```{r rerexp6}
fit.exp <- phreg(Surv(enter, next.ivl, event) ~  year + ses + 
                 age * ivl, dist = "weibull", shape = 1, 
                 data = fc)
drop1(fit.exp, test = "Chisq")
``` 

The parameter estimates are now presented like this.

```{r rerexp6pres}
fit.exp
``` 

A lot of information, but the same content as in the output from the
Poisson regression. However, here we get the exponentiated parameter
estimates, in the column *Exp(Coef)*. These numbers are *risk
    ratios* (or relative risks), and easier to interpret.

What should be done with the interactions? The simplest way to go to get an
easy-to-interpret result is to categorize age and make separate
analyzes, one for each category 
of age. Let us do that; first we have to decide on how to
categorize: there should not be too many categories, and not too few
mothers (or births) in any category. As a starting point, take three
categories by cutting age at (exact) ages 25 and 30:

```{r cutage6}
fc$agegrp <- cut(fc$age, c(0, 25, 30, 100), 
                 labels = c("< 25", "25-30", ">= 30"))
table(fc$agegrp)
``` 

Note the use of the function **cut**. It "cuts" a continuous variable
into pieces, defined by the second argument. Note that we must give lower
and upper bounds; I chose 0 and 100, respectively. They are of course not
near real ages, but I am sure that no value falls outside the interval $(0,
100)$, and that is the important thing here. Then I can (optionally) give
names to each category with the argument **labels**.

The tabulation shows that the oldest category contains relatively few
mothers. Adding to that, the oldest category will probably also have fewer
births. We can check that with the aid of the function **tapply**.

```{r tappcut6}
tapply(fc$event, fc$agegrp, sum)
``` 

Now, rerun the last analysis for each **agegrp** separately. For
illustration only, we show how to do it for the first age group:

```{r grpbir16}
fit.ses1 <- phreg(Surv(enter, next.ivl, event) ~  year + ses + ivl,
dist = "weibull", shape = 1, data = fc[fc$agegrp == "< 25", ])
fit.ses1
``` 

You may try to repeat this for all age groups and check if the regression
parameters for **year** and **ses** are very different. Hint: They are not.

## Choosing the best parametric model


For modeling survival data with parametric proportional hazards models,
the distributions of the function **phreg** in the package **eha** are
available. How to select a suitable parametric model is shown by a couple
of examples.

### Old age mortality

The data set **oldmort** in the **R** package **eha** contains life
histories for people aged 60 and above in the years 1860--1880, i.e., 21
years. The data come from the *Demographic Data Base* at Umeå
University, Umeå, Sweden, and cover the sawmill district of Sundsvall in
the middle of Sweden. This was one of the largest sawmill districts in
Europe in the late 19th century. The town Sundsvall is located in the
district, which also contains a rural area, where farming was the main
occupation.  

```{r oldmort6}
require(eha)
data(oldmort)
summary(oldmort)
``` 

A short explanation of the variables: **id** is an identification
number. It connects records (follow-up intervals) that belong to one
person. **m.id, f.id** are mother's and father's identification numbers,
respectively. **enter, exit** and **event** are the components of the
*survival object*, i.e., start of interval, end of interval, and an
indicator of whether the interval ends with a death, respectively.

Note that by design, individuals are followed from the day they are aged
60. In order to calculate the follow-up times, we should subtract 60 from
the two columns enter and exit. Otherwise, when specifying a
parametric survivor distribution, it would in fact correspond to a
left-truncated (at 60) distribution. However, for a Cox
regression\index{Cox regression}, this 
makes no difference.

The covariate (factor) **ses.50** is the socio-economic status at
(approximately) age 50. The largest category is **unknown**. We have
chosen to include it as a level of its own, rather than treating it as
missing. One reason is that we cannot assume *missing at random*
here. On the contrary, a missing value strongly indicates that the person
belongs to a lower class, or was born early.

```{r oldmort6.reg}
om <- oldmort
om$Y <- Surv(om$enter - 60, om$exit - 60, om$event)    
fit.w <- phreg(Y ~ sex + civ + birthplace, data = om)
fit.w
``` 

Note how we introduced the new variable $Y$; the sole purpose of this is to
get more compact formulas in the modeling.


Here we applied a *Weibull* baseline distribution (the *default*
distribution in **phreg**; by specifying nothing, the Weibull is chosen).
Now let us repeat this with all the distributions in the phreg package.

```{r allreg6}
ln <- phreg(Y ~ sex + civ + birthplace, data = om, 
            dist = "lognormal")
ll <- phreg(Y ~ sex + civ + birthplace, data = om, 
            dist = "loglogistic")
g <- phreg(Y ~ sex + civ + birthplace, data = om, 
           dist = "gompertz")
ev <- phreg(Y ~ sex + civ + birthplace, data = om, 
            dist = "ev")
``` 

Then we compare the maximized log-likelihoods and choose the distribution with the
largest value.

```{r compare6}
xx <- c(fit.w$loglik[2], ln$loglik[2], ll$loglik[2], 
        g$loglik[2], ev$loglik[2])
names(xx) <- c("w", "ln", "ll", "g", "ev")
xx
``` 

The *Gompertz* (g) distribution gives the largest value of the
maximized log likelihood. Let us graphically inspect the fit, see
Figure \@ref(fig:graphi6). 

```{r graphi6,fig.cap="Graphical fit: Gompertz baseline versus the nonparametric (Cox regression, dashed)."}
fit.c <- coxreg(Y ~ sex + civ + birthplace, data = om)
check.dist(fit.c, g)
``` 

The fit is obviously great during the first 30 years (ages 60--90), but
fails somewhat thereafter. One explanation to "the failure" is that after age
90, not so 
many persons are still at risk (alive); another is that maybe the mortality
levels off at very high ages (on a very high level, of course).

The Gompertz hazard function is shown in Figure \@ref(fig:gomphaz6).

```{r gomphaz6,fig.cap="Estimated Gompertz hazard function for old age mortality data."}
plot(g, fn = "haz")
``` 

It is an exponentially increasing function of time; the Gompertz model
usually fits old age mortality very well.

The $p$-values measure the significance of a group compared to the
reference category, but we would also like to have an overall $p$-value for
the covariates (factors) as such, i.e., answer the question "does
birthplace matter?". This can be achieved by running an analysis without
birthplace:

```{r without6}
fit.w1 <- phreg(Y ~ sex + civ, data = om) 
fit.w1
``` 

Then we compare the *max log likelihoods*:
`r round(fit.w$loglik[2], digits = 3)` and
`r round(fit.w1$loglik[2], digits = 3)`, respectively. The test
statistic is 2 times the difference, 
`r round(2 * (fit.w$loglik[2] - fit.w1$loglik[2]), digits = 3)`. 
Under the null hypothesis of no **birthplace** effect on mortality, 
this test statistic has an approximate
$\chi^2$ distribution with 2 degrees of freedom. The degrees of freedom is
the number of omitted parameters in the reduced model, two in this
case. This because the factor **birthplace** has three levels.

There is a much simpler way of doing this, and that is to use the function
 `drop1`. As its name may suggest, it drops one variable at a time and
reruns the fit, calculating the max log likelihoods and differences as above.

```{r drop6}
drop1(fit.w, test = "Chisq")
``` 

As you can see, we do not only recover the test statistic for 
  birthplace, we get the corresponding tests for all the involved
covariates. Thus, it is obvious that both sex and civ have a
statistically very significant  effect on old age mortality, while 
  birthplace does not mean much. Note that these effects are measured in
the presence of the other two variables.

We can plot the baseline distribution by
```{r plotno6}
plot(fit.w)
``` 

with the result shown in Figure \@ref(fig:6weibas).

```{r 6weibas,echo=FALSE,fig.cap="Baseline distribution of remaining life at 60 for an average  person. Weibull model."}
plot(fit.w)
``` 

We can see one advantage with parametric models here: They make it
possible to estimate the hazard and density functions. This is much
trickier with a semi-parametric model like the Cox proportional hazards
model. Another question is, of course, how well the Weibull model fits the
data. One way to graphically check this is to fit a Cox
regression\index{Cox regression} model
and compare the two cumulative hazards plots. This is done by using the
function **check.dist**:\index{cumulative hazard function} 

```{r chekwei6, echo=FALSE}
fit <- coxreg(Y ~ sex + civ + birthplace, data = om)
##check.dist(fit, fit.w)
``` 

The result is shown in Figure \@ref(fig:6chekwei).

```{r 6chekwei,echo=FALSE,fig.cap="Baseline distribution of remaining life at 60 for an average person. Weibull model."}
check.dist(fit, fit.w)
``` 

Obviously, the fit is not good; the Weibull model cannot capture the fast
rise of the hazard by age. An exponentially increasing hazard function may
be needed, so let us 
try the **Gompertz** distribution:

```{r gomp6}
fit.g <- phreg(Y ~ sex + civ + birthplace, data = om, 
               dist = "gompertz")
fit.g
``` 

One sign of a much better fit is the larger value of the maximized log
likelihood, `r round(fit.g$loglik[2], digits = 3)` versus 
`r round(fit.w$loglik[2], digits = 3)` for the Weibull fit. 
  The comparison to the Cox regression\index{Cox regression} fit is given by

```{r chekgom6}
check.dist(fit, fit.g)
``` 

with the result shown in Figure~\ref{fig:6chekgom}.


```{r 6chekgom,echo=FALSE,fig.cap="Check of a Gompertz fit. The solid line is the cumulative hazards function from the Gompertz fit, and the dashed line is the fit from a Cox regression."}
check.dist(fit, fit.g)
``` 

This is a much better fit. The deviation that starts above 30
(corresponding to the age 90) is no big issue; in that range few people are
still alive and the random variation takes over. Generally, it seems as if
the Gompertz distribution fits old age mortality well.

The plots of the baseline Gompertz distribution is shown in Figure \@ref(fig:6gombas).

```{r 6gombas,echo=FALSE,fig.cap="Baseline distribution of remaining life at 60 for an average person. Gompertz model.", height=6}
plot(fit.g)
``` 

Compare to the corresponding fit to the Weibull model, Figure \@ref(fig:6weibas).


## Accelerated Failure Time Models

The accelerated failure time (AFT) model is best described through relations
between survivor functions. For instance, 
comparing two groups:

   * **Group 0:** $P(T \ge t) = S_0(t)$  (control group)
   * **Group 1:** $P(T \ge t) = S_0(\phi t)$ (treatment group)
  
The model says that treatment *accelerates} failure time by the factor $\phi$.
If $\phi < 1$, treatment is good (prolongs life), otherwise bad.
Another interpretation is that the *median* life length is
  *multiplied* by $1/\phi$ by treatment.

In Figure \@ref(fig:aftph6) the difference between the accelerated failure
time  and the 
proportional hazards models concerning the hazard functions is illustrated.

```{r aftph6,fig.cap="Proportional hazards (left) and accelerated failure time model (right). The baseline distribution is Loglogistic with shape 5 (dashed).",echo=FALSE}
x <- seq(0, 3, length = 1000)
par(mfrow = c(1, 2))
plot(x, 2 * hllogis(x, shape = 5), type = "l", ylab = "", main = "PH", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
plot(x, 2 * hllogis(2 * x, shape = 5), type = "l", ylab = "", main = "AFT", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
```

The AFT hazard is not only multiplied by 2, it is also shifted to the left;
the process is accelerated. Note how the hazards in the AFT case converges
as time increases. This is usually a sign of the suitability of an AFT model. 

### The AFT regression model
\index{model!parametric!AFT|(}
If $T$ has survivor function $S(t)$ and $T_c = T/c$,  then $T_c$ has
survivor function $S(ct)$.
Then, if $Y = \log(T)$ and $Y_c = \log(T_c)$, the
following relation holds:

\begin{equation*}
Y_c = Y - log(c).
\end{equation*}

With $Y = \epsilon$, $Y_c = Y$, and $\log(c) = -\boldsymbol{\beta} \mathbf{x}$ this can be written in
familiar form:

\begin{equation*}
Y = \boldsymbol{\beta} \mathbf{x} + \epsilon,
\end{equation*}

i.e., an ordinary linear regression model for the log survival times. In
the absence of right censoring and left truncation, this model can be
estimated by least squares. However, the presence of these forms of
incomplete data makes it necessary to rely on maximum likelihood
methods. In **R**, there is the functions `aftreg` in the package 
  **eha** and the function `survreg` in the package **survival** that
perform the task of fitting AFT models.

Besides differing parametrizations, the main difference between
  `aftreg` and `survreg` is that the latter does not allow for left
truncated data. One reason for this is that left truncation is a much
harder problem to deal with in AFT models than in proportional hazards models.

Here we describe the implementation in `aftreg`. A detailed description
of it can be found in Appendix \@ref(app:B). The model is built around 
*scale-shape* families of distributions:

\begin{equation}
S^\star\bigl\{t, (\lambda, p)\bigr\} = S_0\biggl\{\biggl(\frac{t}{\lambda}\biggr)^p\biggr\}, \quad t > 0; \;
\lambda, p > 0,
(\#eq:6aftnull)
\end{equation}

where $S_0$ is a fixed distribution. For instance, the exponential with
parameter 1:

\begin{equation*}
S_0(t) = e^{-t}, \quad t > 0,
\end{equation*}

generates, through \@ref(eq:6aftnull), the *Weibull* family
\@ref(eq:6weibull) of distributions.

%The allowance for left
%truncation and time-varying covariates forces the consideration of the
%extended model

\newcommand{\tl}{\biggl(\frac{t\exp(\boldsymbol{\beta}\mathbf{x})}{\lambda}\biggr)^p}

With time-constant covariate vector $\mathbf{x}$, the AFT model is 

\begin{equation}
    S(t; (\lambda, p, \boldsymbol{\beta}), \mathbf{x}) = S^\star\bigl\{t\exp(\boldsymbol{\beta}\mathbf{x}),
    (\lambda, p)\bigr\} =
    S_0\biggl\{\biggl(\frac{t\exp(\boldsymbol{\beta}\mathbf{x})}{\lambda}\biggr)^p\biggr\},
  \quad t > 0,
\end{equation}

and this leads to the following description of the hazard function:

\begin{multline*}
    h(t; (\lambda, p, \boldsymbol{\beta}), \mathbf{x}) = \exp(\boldsymbol{\beta}\mathbf{x})
    h^\star\bigl(t\exp(\boldsymbol{\beta}\mathbf{x})\bigr) = \\
    \exp(\boldsymbol{\beta}\mathbf{x})\frac{p}{\lambda} 
    \biggl(\frac{t\exp(\boldsymbol{\beta}\mathbf{x})}{\lambda}\biggr)^{p-1}h_0\biggl\{\tl\biggr\},
    \quad t > 0, 
\end{multline*}

where $\mathbf{x} = (x_1, \ldots, x_p)$ is a vector of covariates, and
$\boldsymbol{\beta} = (\beta_1, \ldots, \beta_p)$ is the corresponding
        vector of regression coefficients.
\index{model!parametric!AFT|)}
        
\subsection{Different parametrizations}
\index{Functions!\fun{aftreg}|(}
In **R**, there are two functions that can estimate AFT regression models, the
function `aftreg` in the package **eha**, and the function 
`survreg` in the package **survival**. In the case of no time-varying
covariates and no left truncation, they fit the same models (given a common
baseline distribution), but use different parametrizations, which will be
explained here.

### AFT models in **R**

 We repeat the examples from the proportional hazards section, but with AFT
 models instead. 
 
```{example, name = "Old age mortality", echo = TRUE}
```
 
 For a description of this data set, see above. Here we fit an AFT model
 with the *Weibull* distribution. This should be compared to the proportional
 hazards model with the *Weibull* distribution, see earlier in this chapter.
 
```{r oldmort6.aft}
fit.w1 <- aftreg(Y ~ sex + civ + birthplace, data = om)
fit.w1
```

Note that the "Max. log. likelihood" is exactly the same, but the
regression parameter estimates differ. The explanation to this is that (i)
for the *Weibull* distribution, the AFT and the PH models are the same, and
(ii) the only problem is that different parametrizations are used. The
$p$-values and the signs of the parameter estimates should be the same.
$\Box$
\index{Functions!\fun{aftreg}|)}


## Proportional hazards or AFT model?

The problem of choosing between a proportional hazards and an accelerated
failure time model (everything else equal) can be solved by comparing the
AIC\index{AIC} of the models. Since the numbers of parameters are equal in
the two 
cases, this amounts to comparing the maximized likelihoods. For instance,
in the case with *old age mortality*:

Let us see what happens with the *Gompertz* AFT model:
Exactly the same procedure as with the *Weibull* distribution, but we have to
specify the Gompertz distribution in the call (remember, the *Weibull*
distribution is the default choice, both for `phreg` and `aftreg`).\

```{r oldln6.aft}
fit.g1 <- aftreg(Y ~ sex + civ + birthplace, data = om, dist = "gompertz")
fit.g1
```

Comparing the corresponding result for the proportional hazards and the AFT
models with the Gompertz distribution,
we find that the maximized log likelihood in the former case is
`r round(fit.g1$loglik[2], 3)`, compared to
`r round(fit.g$loglik[2], 3)` for the latter. This indicates that the
proportional hazards model
  fit is better. Note however that we cannot formally test the proportional
  hazards hypothesis; the two models are not nested.

## Discrete time models

There are two ways of looking at discrete duration data; either time is truly
discrete, for instance the number of trials until an event occurs, or an
approximation due to rounding of continuous time data. In a sense all data
are discrete, because it is impossible to measure anything on a continuous
scale with infinite precision, but from a practical point of view it is
reasonable to say that data is discrete when tied events occur
embarrassingly often. 

When working with register data, time is often measured in years which
makes it necessary and convenient to work with discrete models. A typical
data format is the so-called *wide* format, where there is one record
(row) per individual, and measurements for many years. We have so far only
worked with the *long* format. The data sets created by `survSplit`
are in long format; there is one record per individual and age category. 
The R work horse in switching back and forth between the long and wide
formats is the function `reshape`. It may look confusing at first, but
if data follow some simple rules, it is quite easy to use `reshape`.

The function `reshape` is typically used with *longitudinal data*,
where there are several measurements at different time points for each
individual. If the data for one individual is registered within one record
(row), we say that data are in wide format, and if there is one record (row)
per time (several records per individual), data are in long format. Using
wide format, the rule is that time-varying variable names must end in a
numeric value indicating at which time the measurement was taken. For
instance, if the variable `civ` (civil status) is noted at times 1, 2,
and 3, there must be variables named `civ.1, civ.2, civ.3`,
respectively. It is optional to use any *separator* between the base
name (`civ`) and the time, but it should be one character or empty. The
"." is what `reshape` expects by default, so using that form
simplifies coding somewhat.

We start by creating an example data set as an illustration. This is
accomplished by starting off with the data set `oldmort` in `eha`
and "trimming" it.
```{r trimort6}
data(oldmort)
om <- oldmort[oldmort$enter == 60, ]
om <- age.window(om, c(60, 70))
om$m.id <- om$f.id <- om$imr.birth <- om$birthplace <- NULL
om$birthdate <- om$ses.50 <- NULL
om1 <- survival::survSplit(om, cut = 61:69, start = "enter", end = "exit", 
                 event = "event", episode = "agegrp")
om1$agegrp <- factor(om1$agegrp, labels = 60:69)
om1 <- om1[order(om1$id, om1$enter), ]
head(om1)
```
We may change the row numbers and recode the `id` so they are easier to read.
```{r recode6}
rownames(om1) <- 1:NROW(om1)
om1$id <- as.numeric(as.factor(om1$id))
head(om1)
``` 
This is the long format, each individual has as many records as "presence
ages". For instance, person No. 1 has four records, for the ages 60--63.
The maximum possible No. of records for one individual is 10. We can check
the distribution of No. of records per person by using the function 
  `tapply`: 
```{r recspp6}
recs <- tapply(om1$id, om1$id, length)
table(recs)
``` 
It is easier to get to grips with the distribution with a graph, in this
case a *barplot*\index{Functions!\fun{barplot}},see Figure~\ref{fig:barp6}.

```{r barp6, fig.cap = "Barplot of the number of records per person."}
barplot(table(recs))
``` 

Now, let us turn `om1` into a data frame in *wide* format. This is
done with the function `reshape`. First we remove the redundant
variables `enter` and `exit`. 
```{r wideform6}
om1$exit <- om1$enter <- NULL
om2 <- reshape(om1, v.names = c("event", "civ", "region"), 
               idvar = "id", direction = "wide", 
               timevar = "agegrp")
names(om2)
``` 
Here there are two time-fixed variables, `id` and `sex`, and three
time-varying variables, `event`, `civ`, and `region`. The
time-varying variables have suffix of the type `.xx`, where `xx` varies
from 60 to 69. 
  
This is how data in wide format usually show up; the suffix may start with
something else than `.`, but it must be a single character, or nothing. 
The real problem is how to switch from wide format to long, because our
survival analysis tools want it that way. The solution is to use
`reshape` again, but other specifications.

```{r reshap26}
om3 <- reshape(om2, direction = "long", idvar = "id", 
               varying = 3:32)
head(om3)
``` 
There is a new variable `time` created, which goes from 60 to 69, one
step for each of the ages. We would
like to have the file sorted primarily by `id` and secondary by time.
```{r sortin6}
om3 <- om3[order(om3$id, om3$time), ]
om3[1:11, ]
``` 
Note that all individuals got 10 records here, even those who only are
observed for fewer years. Individual No. 1 is only observed for the ages
60--63, and the next six records are redundant; they will not be used in an
analysis if kept, so it is from a practical point of view a good idea to
remove them.
```{r remove6}
NROW(om3)
om3 <- om3[!is.na(om3$event), ]
NROW(om3)
``` 
The data frame shrunk to almost half of what it was originally. First, let
us summarize data.
```{r summm6}
summary(om3)
``` 
The key variables in the discrete time analysis are `event` and 
  `time`. For the baseline hazard we need one parameter per value of 
  `time`, so it is practical to transform the continuous variable `time`
to a factor.
```{r turn6}
om3$time <- as.factor(om3$time)
summary(om3)
```
The summary now produces a frequency table for `time`.

For a given time point and a given individual, the response is whether an
event has occurred or not, that is, it is modeled as a 
\emph{Bernoulli}\index{Distributions!Bernoulli} outcome, which is a special
case of the \emph{binomial} distribution\index{Distributions!binomial}.
The discrete time analysis may now be performed in several ways. Most
straightforward is to run a 
logistic regression\index{logistic regression} with `event` as
response through the basic `glm` function with 
`family = binomial(link=cloglog)`. The so-called 
\emph{cloglog} link\index{cloglog link} is
used in order to preserve the proportional hazards property.
```{r glmreg6}
fit.glm <- glm(event ~ sex + civ + region + time, 
               family = binomial(link = cloglog), data = om3)
summary(fit.glm)
``` 
This output is not so pleasant, but we can anyway see that females (as
usual) have lower mortality than males, that married are better off than
unmarried, and that regional differences maybe are not so large. To get a
better understanding of the statistical significance of the findings we run
`drop1`\index{Functions!\fun{drop1}} on the fit.
```{r glmdrop6}
drop1(fit.glm, test = "Chisq")
``` 
Mildly surprisingly, `civil status` is not that statistically
significant, but `region` (and the other variables) is. The strong
significance of the time variable is of course expected; mortality is
expected to increase with increasing age.

An equivalent way, with a nicer printed output, is to use the function
\fun{glmmboot}\index{Functions!\fun{glmmboot}} in the package `glmmML`. 
```{r glmmboot6}
fit.boot <- glmmML::glmmboot(event ~ sex + civ + region, cluster = time, 
                     family = binomial(link = cloglog), 
                     data = om3)
fit.boot
``` 
The parameter estimates corresponding to `time` are contained in the
variable $fit\$frail$. They need to be transformed to get the "baseline
hazards". 
```{r calchaz6}
haz <- plogis(fit.boot$frail)
haz
``` 
A plot of the hazard function is shown in Figure \@ref(fig:plothaz6).

```{r plothaz6, fig.cap = "Baseline hazards, old age mortality."}
barplot(haz)
``` 


By some data manipulation we can also use `eha` for the analysis. For
that to succeed we need intervals as responses, and the way of doing that
is to add two variables, `exit` and `enter`. The latter must be
*slightly* smaller than the former: 

```{r mlreg6}
om3$exit <- as.numeric(as.character(om3$time))
om3$enter <- om3$exit - 0.5
fit.ML <- coxreg(Surv(enter, exit, event) ~ sex + civ + region, 
                 method = "ml", data = om3)
fit.ML
``` 
\index{Functions!\fun{as.character}}\index{Functions!\fun{as.numeric}}
Plots of the cumulative hazards and the survival function are easily
achieved, see Figures \@ref{fig:cumML6} and \@ref(fig:surML6).

```{r cumML6, fig.cap = "The cumulative hazards, from the coxreg fit."}
plot(fit.ML, fn = "cum", xlim = c(60, 70))
``` 


```{r surML6, fig.cap = "The survival function, from the coxreg fit."}
plot(fit.ML, fn = "surv", xlim = c(60, 70))
``` 

Finally, the proportional hazards assumption can be tested in the discrete
time framework by creating an interaction between `time` and the
covariates in question. It is only possible by using `glm`.
```{r testph6}
fit2.glm <- glm(event ~ (sex + civ + region) * time, 
                family = binomial(link = cloglog), 
                data = om3)
drop1(fit2.glm, test = "Chisq")
``` 
There is no sign of non-proportionality.
