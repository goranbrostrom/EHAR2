# Proportional Hazards and Cox Regression

```{r setup3,include=FALSE}
par(las = 1) # Doesn't work? Later?
knitr::opts_chunk$set(comment = "", message = TRUE, echo = TRUE, cache = FALSE, fig.height = 4)
options(show.signif.stars = FALSE, warn = 2)
```

*Proportional hazards* is a property of survival models that is fundamental for the
development of non-parametric regression models. After defining this property,
we move slowly from the two-sample case, via the $k$-sample case, to the
general regression model. On this route, we  start with the *log-rank test*
and end up with *Cox regression* [@cox72]. 

## Proportional Hazards {#sec:ph3}
\index{proportional hazards}

The property of *proportional hazards* is fundamental in Cox
regression. It is in fact the essence of Cox's simple yet ingenious
idea. The definition is as follows:

```{definition, name="Proportional hazards"}
If $h_1(t)$ and $h_0(t)$ are hazard functions from two separate
distributions, we say that they are *proportional* if

\begin{equation}
h_1(t)  =  \psi h_0(t), \quad \text{for all } t \ge 0,
(\#eq:prophaz)
\end{equation}
for some positive constant $\psi$ and *all* $t \ge 0$. Further, if
\eqref{eq:prophaz} holds, then the same 
property holds for the corresponding 
*cumulative hazard functions* $H_1(t)$ and $H_0(t)$.
\index{cumulative hazard function}

\begin{equation}
H_1(t)  =  \psi H_0(t), \quad \text{for all } t \ge 0,
(\#eq:propcumhaz)
\end{equation}
with the same proportionality constant $\psi$ as in \@ref(eq:prophaz).$\Box$
```

<!--
\begin{definition} Proportional hazards. \end{definition}
If $h_1(t)$ and $h_0(t)$ are hazard functions from two separate
distributions, we say that they are *proportional* if

\begin{equation}\label{eq:prophaz}
h_1(t)  =  \psi h_0(t), \quad \text{for all } t \ge 0,
\end{equation}
for some positive constant $\psi$ and *all* $t \ge 0$. Further, if
\eqref{eq:prophaz} holds, then the same 
property holds for the corresponding 
*cumulative hazard functions* $H_1(t)$ and $H_0(t)$.
\index{cumulative hazard function}

\begin{equation}\label{eq:propcumhaz}
H_1(t)  =  \psi H_0(t), \quad \text{for all } t \ge 0,
\end{equation}
with the same proportionality constant $\psi$ as in \ref{eq:prophaz}.$\Box$
-->


Strictly speaking, the second part of this definition follows easily from
the first (and vice versa), so more correct would be to state one part as a
definition and the other as a corollary.
The important part of this definition is "$\textit{for all }t \ge 0$",
and that 
the constant $\psi$ *does not depend on $t$*. 

Think of the hazard
functions as age-specific mortality for two groups, e.g., women and
men. It is "well known" that women have lower mortality than men in all
ages. It would therefore be reasonable to assume proportional
hazards in that case. It would mean that the female *relative*
advantage is equally large in all ages. See Example \@ref(exm:sw192) for an
investigation of this statement. 

```{example, sw192, name = "Male and female mortality, Sweden 2001--2019", echo = TRUE}
```
This is a continuation of Example \@ref(exm:sw19), and here we involve the male 
mortality alongside with the female, see Figure \@ref(fig:twosex2). We also 
expand the calendar time period to the years 2001--2019.
The reason for this is that random fluctuations in low ages, caused by the 
extremely low mortality rates, blur the picture.

```{r twosex2, echo = FALSE, fig.cap = "Female and male mortality by age, Sweden 2001-2019. The right hand panel shows the ratio between the male and female mortality by age.", fig.height = 3}
library(eha)
females <- swepop[swepop$sex == "women" & swepop$year %in% 2001:2019, 
                  c("age", "pop", "year")]
females$deaths <- swedeaths[swedeaths$sex == "women" & 
                                swedeaths$year %in% 2001:2019, "deaths"]
females <- aggregate(females[c("pop", "deaths")], by = females["age"], FUN = sum)
females$mort <- females$deaths / females$pop
##
males <- swepop[swepop$sex == "men" & swepop$year %in% 2001:2019, 
                  c("age", "pop")]
males$deaths <- swedeaths[swedeaths$sex == "men" & 
                                swedeaths$year %in% 2001:2019, "deaths"]
males <- aggregate(males[c("pop", "deaths")], by = males["age"], FUN = sum)
males$mort <- males$deaths / males$pop
oldpar <- par(mfrow = c(1, 2), las = 1)
plot(males$age, males$mort, type = "l", lty = 2, xlab = "Age", ylab = "Hazards")
lines(females$age, females$mort, type = "l", lty = 1)
legend("topleft", legend = c("Men", "Women"), lty = c(2, 1))
abline(h = 0)
plot(males$age[1:101], males$mort[1:101] / females$mort[1:101], type = "l",
     ylim = c(0.9, 3), xlab = "Age", ylab = "Hazard ratio")
abline(h = 1, lty = 2)
par(oldpar)
```

As you can see, the "common knowledge" of proportional hazards is disproved by the 
right hand panel in Figure \@ref(fig:twosex2). On the other hand, the left hand 
panel gives a rather different impression, and this illustrates the need to choose good 
approaches to graphical presentation, depending on what you want to show.


It must be emphasized that the proportional hazards assumption is an assumption 
that always must be
carefully checked. In many  situations, it would not be reasonable to assume
proportional hazards. If in doubt, check data by plotting the Nelson-Aalen
estimates\index{Nelson-Aalen estimator} for each group in the same plot. The left hand
panel of Figure \@ref(fig:twosex2) would suit this purpose better if drawn on a
log scale, see Figure \@ref(fig:logtwosex2).

```{r logtwosex2, echo = FALSE, fig.cap = "Female and male mortality by age, Sweden 2001-2019. Log scale.", fig.height = 3}
oldpar <- par(las = 1, cex.axis = 0.8)
plot(males$age, males$mort, type = "l", lty = 2, xlab = "Age", 
     log = "y", ylab = "Log hazards")
lines(females$age, females$mort, type = "l", lty = 1)
legend("topleft", legend = c("Men", "Women"), lty = c(2, 1))
par(oldpar)
```

The advantage of the log scale is twofold: (i) Small numbers are magnified so you 
can see them, and (ii) on the log scale, proportional hazards implies a constant
vertical distance between the curves, which is easier to see for the human eye.
$\Box$

For an example of a perfect fit to the proportional hazards model, see
Figure \@ref(fig:prophazfig3) (two *Weibull* hazard
functions with the proportionality constant $\psi = 2$).

```{r prophazfig3,fig=TRUE,echo=FALSE, fig.cap = "Two hazard functions that are proportional. The proportionality constant is 2.",  fig.scap="Proportional hazard functions.", fig.height = 3}
library(eha)
oldpar <- par(las = 1, mfrow = c(1, 2), cex.axis = 0.8)
x <- seq(0,1, length = 500)
#y <- dweibull(x, scale = 1, shape = 3) / (1 - pweibull(x, scale = 1, shape = 3))
y <- hweibull(x, scale = 1, shape = 3)
z <- 2 * y
plot(x, z, type = "l", main = "Normal scale", xlab = "Time", ylab = "Hazards")
lines(x, y, lty = 2)
abline(h = 0)

###
x <- seq(0.01,1, length = 500)
#y <- dweibull(x, scale = 1, shape = 3) / (1 - pweibull(x, scale = 1, shape = 3))
y <- hweibull(x, scale = 1, shape = 3)
z <- 2 * y
plot(x, z, type = "l", main = "Log-log scale", xlab = "Time", ylab = "Hazards", log = "xy")
lines(x, y, lty = 2)
abline(h = 0)
par(oldpar)
```

In the right hand panel of Figure \@ref(fig:prophazfig3), note that both 
dimensions are on a log scale. This type of plot,
constructed from empirical data, is called a *Weibull plot*\index{Weibull plot} 
in reliability applications: If the lines are
straight lines, then data are well fitted by a Weibull
distribution. Additionally, if the the slope of the line is 1 (45 degrees),
then an exponential model fits well.

To summarize Figure \@ref(fig:prophazfig3): (i) The hazard functions are
proportional because on the log-log scale, the vertical distance is
constant, (ii) Both hazard functions represent a Weibull distribution,
because both lines are straight lines, and (iii) neither represents an
exponential distribution, because the slopes are *not* one. This
latter fact may be difficult to see because of the different scales on the
axes (the *aspect ratio* is not one). 

Figure \@ref(fig:prophazfig2) shows the relationships between the cumulative
hazards functions, the density functions, and the survival functions when
the hazard functions are proportional. Note that the cumulative hazards
functions are proportional by implication, with the same proportionality
constant ($\psi = 2$ in this case). On the other hand, for the density and
survival functions, proportionality does not hold; it is in fact
theoretically impossible except in the trivial case that the
proportionality constant is unity.


```{r prophazfig2,fig=TRUE,echo=FALSE,height=5, fig.cap="The effect of proportional hazards on the density and survival functions.",fig.scap="Effect of proportional hazards.", fig.height = 5}
oldpar <- par(mfrow = c(2, 2), las = 1)
x <- seq(0,2.5, length = 500)
library(eha)
y <- hweibull(x, scale = 1, shape = 3)
z <- hweibull(x, scale = 2^(1/3), shape = 3)
plot(x, y, type = "l", main = "Hazards", xlab = "Time", ylab = "")
lines(x, z, lty = 2)
abline(h = 0)
y <- Hweibull(x, scale = 1, shape = 3)
z <- Hweibull(x, scale = 2^(1/3), shape = 3)
plot(x, y, type = "l", main = "Cumulative hazards", xlab = "Time", ylab = "")
lines(x, z, lty = 2)
abline(h = 0)
y <- dweibull(x, scale = 1, shape = 3)
z <- dweibull(x, scale = 2^(1/3), shape = 3)
plot(x, y, type = "l", main = "Density functions", xlab = "Time", ylab = "")
lines(x, z, lty = 2)
abline(h = 0)
y <- pweibull(x, scale = 1, shape = 3, lower.tail = FALSE)
z <- pweibull(x, scale = 2^(1/3), shape = 3, lower.tail = FALSE)
plot(x, y, type = "l", main = "Survival functions", xlab = "Time", ylab = "", ylim = c(0, 1))
lines(x, z, lty = 2)
abline(h = 0)
##
par(oldpar)
```

## The Log-Rank Test

The *log-rank test* \index{Tests!log-rank} is a $k$-sample test
of equality of survival functions. It is a powerful test against proportional hazards 
alternatives, but may be very weak otherwise. 
We first look at the two-sample case,
that is, $k = 2$.

\subsection{Two samples}

Suppose that we have the small data set illustrated in
Figure \@ref(fig:twosample). There are two samples, the `letters` (A, B,
C, D, E) and the `numbers` (1, 2, 3, 4, 5).

(ref:twosamplecap) Two-sample data, the `letters` (dashed) and the `numbers` (solid). Circles denote censored observations, plusses events.

(ref:twosamplescap) Two-sample data.

```{r twosample, fig.cap = "(ref:twosamplecap)", fig.scap = "(ref:twosamplescap)", echo=FALSE, fig.height = 4}
two <- data.frame(group = c(rep("numbers", 5), rep("letters", 5)),
                  id = as.character(c(1:5, LETTERS[1:5])),
                  exit = c(4, 2, 6, 1, 3.5, 5, 3, 6, 1, 2.5),
                  event = c(1,0,1,1,0,1,1,0,1,0),
                  stringsAsFactors = FALSE)
plot(c(0, two$exit[1]), c(1, 1), type = "l", main = "", xlab="Duration", ylab = "Person No.",
     axes = FALSE,  ylim = c(0, 11), xlim = c(0, 8))
for (i in 2:5){
  lines(c(0, two$exit[i]), c(i, i), lwd = 1.5)
}
for (i in 6:10){
  lines(c(0, two$exit[i]), c(i, i), lty = 2, lwd = 1.5)
}
axis(1, at = c(0, two$exit[two$event == 1]))
axis(2, at = 1:10, labels = two$id, las = 1)
box()
abline(v = 0)
deaths <- two$exit[two$event == 1]
abline(v = deaths, lty = 3)
y <- which(two$event == 1)
nd <- length(deaths)
for (i in 1:10){
  if (two$event[i] == 1){
    text(two$exit[i] + 0.2, i, "+", cex = 1.5)
  }else{
    text(two$exit[i], i, "o", cex = 1.5)
  }
}
```

The data in Figure \@ref(fig:twosample) can be presented i tabular form, see Table \@ref(tab:exlr3).
```{r exlr3, echo = FALSE}
library(survival)
ex.data <- data.frame(group = c(rep("numbers", 5), rep("letters", 5)),
                      time = c(4,2,6,1,3.5,5,3,6,1,2.5),
                      event = c(TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, 
                                TRUE, FALSE, TRUE, FALSE))
knitr::kable(ex.data, booktabs = TRUE, caption = "Example data for the log-rank test..")
```
We are interested in investigating whether `letters` and `numbers`
have the same survival chances or not. Therefore, the hypothesis

\begin{equation*}
H_0: \text{No difference in survival between numbers and letters}
\end{equation*}
is formulated. In order to test $H_0$, we make five tables, one
for each observed *event time*\index{event time}, see
Table \@ref(tab:lr1), where the the first table, relating to failure time $t_{(1)} = 1$, is shown.


```{r lr1, echo = FALSE}
xx <- matrix(c(1,1,2,4,4,8,5,5,10), ncol = 3)
rownames(xx) <- c("numbers", "letters", "Total")
colnames(xx) <- c("Deaths", "Survivals", "Total")
knitr::kable(xx, caption = "The 2x2 table for the first event time.", booktabs = TRUE)
```


Let us look at the table at failure time $t_{(1)} = 1$, Table \@ref(tab:lr1), 
from the viewpoint of the `numbers`.

*    The observed number of deaths among `numbers`: $1$.
*    The expected number of deaths among `numbers`: $2 \times 5 / 10 = 1$.

The *expected* number is calculated under $H_0$, i.e., as if there is
no difference between `letters` and `numbers` regarding mortality. It is further
assumed that the two margins (Total) are given (fixed) and that the number of 
deaths follows a hypergeometric distribution, which gives the variance. 

Then, given two deaths in
total and five out of ten observations are from the group `numbers`,
the expected number of deaths is calculated as above.

This procedure is repeated for each of the five tables, and the results are
summarized in Table \@ref(tab:sumup).

```{r sumup, echo = FALSE}
x <- c(1, 0, 1, 0, 1, 3,
       1.0, 0.5, 0.5, 0.3, 0.5, 2.8,
       0.0, -0.5, 0.5, -0.3, 0.5, 0.2,
       0.44, 0.25, 0.25, 0.22, 0.25, 1.41)
x <- matrix(x, ncol = 4)
colnames(x) <- c("Observed", "Expected", "Difference", "Variance")
rownames(x) <- c("t(1)", "t(2)", "t(3)", "t(4)", "t(5)", "Sum")
knitr::kable(x, booktabs = TRUE, caption = "Observed and expected number of deaths for numbers at event times.")
```


<!--
\begin{table}[ht!]
\tabletitle[Logrank test: Summary table]{Observed and expected number of deaths for {\tt numbers} at the
five observed event times.}
\label{tab:sumup}
\begin{tabular}{r|rr|r|r}
Time  & Observed & Expected & Difference & Variance\\ \hline
$t_{(1)}$ & 1 & 1.0 & 0.0 & 0.44\\
$t_{(2)}$ & 0 & 0.5 & -0.5 & 0.25 \\
$t_{(3)}$ & 1 & 0.5 & 0.5 & 0.25 \\
$t_{(4)}$ & 0 & 0.3 & -0.3 & 0.22 \\
$t_{(5)}$ & 1 & 0.5 & 0.5 & 0.25 \\ \hline
Sum & 3 & 2.8 & 0.2 & 1.41 \\ \hline
\end{tabular}
\end{table}
-->

<!--
Table: (#\tab:sumup)
-->

Finally, the observed test statistic $T$ is calculated as

\begin{equation*}
T = \frac{0.2^2}{1.41} \approx 0.028
\end{equation*}
Under the null hypothesis, this is an observed value from a 
$\chi^2(1)$\index{Distributions!chi-square@$\chi^2$}
distribution, and $H_0$ should be rejected for *large* values of
$T$. Using a *level of significance* of 5%, the cutting point for the
value of $T$ is 3.84, far from our observed value of 0.028. The
conclusion is
therefore that there is no (statistically significant) difference in
survival chances between `letters` and `numbers`.
Note, however, that this result depends on 
asymptotic\index{asymptotic theory} (large sample) 
properties, and 
in this toy example, these properties are not valid.

For more detail about the underlying theory, see Appendix A.

<!--
In **R**, the log-rank test is performed by the 
`coxph`\index{Functions!`coxph`} function in the
package survival (there are other options). 
-->

Let us now look at a real data example, the old age mortality data set
`oldmort` in `eha`. See Table \@ref(tab:lrreal3dat) for a sample of five records with selected columns. 


```{r lrreal3dat,echo = FALSE}
options(show.signif.stars = FALSE)
library(eha)
om <- age.window(oldmort, c(60, 85))
knitr::kable(head(om[-(1:1000), c("id", "enter", "exit", "event", "sex", "civ")], 5), booktabs=TRUE, caption = "Old age mortality.", row.names = FALSE)
```

We are interested in comparing male and female mortality in the ages 60--85 with a logrank test, and for that purpose we use the `logrank` function:

```{r noevallr, echo = TRUE}
library(eha)
fit <- logrank(Surv(enter, exit, event), group = sex, data = om)
fit
```

 The result of the logrank test is given by the $p$-value
`r round(100 * fit$p.value, 4)` per cent, a very small number. Thus, there is a very significant 
difference in mortality between men and women.
But *how large* is the difference? The log-rank test has no answer to this question.

Remember that this result depends on the proportional hazards
assumption. We can graphically check it, see Figure \@ref(fig:logrank3).

```{r logrank3, fig.cap = "Cumulative hazards for women and men in ages 60-85, 19th century Sweden.", echo = FALSE, fig.height = 3}
par(mfrow = c(1, 2), las = 1)
plot(fit, main = "Normal scale", xlab = "Age")
plot(fit, fn = "log", main = "Log scale", xlab = "Age", printLegend = "bottomright")
```


The proportionality assumption appears to be a good description looking at the left hand
panel, but it looks more doubtful on the log scale. It is an often 
observed phenomenon that differences in mortality between human groups, 
not only women and men, tend to vanish in the extremely high ages.
The validity of our test result, however, is not affected by this deviation from
proportionality, since the result was *rejection*. We can be confident in the fact 
that there is a significant difference, since violation of the proportionality assumption
*in itself* implies a difference.

### Several samples

The result for the two-sample case is easily extended to the $k$-sample
case. Instead of one $2 \times 2$ table per observed event time we get one
$k\times 2$ table per observed event time and we have to calculate expected
and observed numbers of events for $(k-1)$ groups at each failure time. The
resulting test statistic will have $(k-1)$ degrees of freedom and still be
approximately $\chi^2$ distributed. This is illustrated with the same data
set, `oldmort`, as above, but with the covariate `civ`,
which is a `factor` with three levels (`unmarried`, `married`, `widow`), instead of `sex`.
Furthermore, the investigation is limited to *male mortality*.

```{r lrplace3, echo = TRUE, comment = NA}
fit <- logrank(Surv(enter, exit, event), group = civ,  
                     data = om[om$sex == "male", ])
fit
```
The degrees of freedom for the *score test*\index{Tests!score}
is now 2, equal to the number
of levels in `civ` 
minus one. Figure \@ref(fig:grbp3) shows the cumulative hazards for each group.

```{r grbp3, fig.cap = "Old age male mortality by civil status, cumulative hazards.", echo=FALSE, fig.height = 4}
par(las = 1)
plot(fit)
```

<!--
\begin{figure}[ht!]
\includegraphics{{$HOME}/Documents/ehaBook/Sweave/figs/ch3-grbp3}
\caption{Old age mortality by birthplace, cumulative hazards.}
\label{fig:grbp3}
\end{figure}
-->

There is obviously nothing much that indicates non-proportionality in this case
either. Furthermore, the ordering is what one would expect.

We do not go deeper into this matter here, mainly because the logrank test
is a special case of *Cox regression*.

<!--
There are however two special cases of the
logrank test, the *weighted* and *stratified* versions, that may
be of interest on 
their own. The interested reader may consult the text books by
\citet[pp.\ 22-23]{kp02} or \citet[pp.\ 49--53]{collett}.
-->



## Cox Regression Models
\index{proportional hazards!Cox regression}

Starting with the definition of proportional hazards in Section \@ref(sec:ph3), the
concept of Cox regression is introduced in steps.

### Two Groups

The definition of proportionality, given in equation \@ref(eq:prophaz), can
equivalently be written as

\begin{equation}
h_x(t) = \psi^x h_0(t), \quad t > 0, \; x = 0, 1, \; \psi > 0.
(\#eq:ph2)
\end{equation}

It is easy to see that this "trick" is equivalent to \@ref(eq:prophaz):
When $x = 0$ it simply says that $h_0(t) = h_0(t)$, and when $x = 1$ it
says that $h_1(t) = \psi h_0(t)$. Since $\psi > 0$, we can calculate $\beta =
\log(\psi)$, and rewrite \@ref(eq:ph2) as

\begin{equation*}
h_x(t) = e^{\beta x} h_0(t), \quad t > 0; \; x = 0, 1; \; -\infty < \beta < \infty,
\end{equation*}
or with a slight change in notation,

\begin{equation}
h(t; x) = e^{\beta x} h_0(t), \quad t > 0; \; x = 0, 1; \; -\infty < \beta
< \infty.
(\#eq:phsimple)
\end{equation}

The sole idea by this rewriting is to pave the way for the introduction of
*Cox's regression model* [@cox72], which in its elementary form is a
proportional hazards model. In fact, we can already interpret
equation \@ref(eq:phsimple) as a Cox regression model with an explanatory
variable $x$ with corresponding regression coefficient $\beta$ (to be estimated
from data). The covariate $x$ is still only a dichotomous variate,
but we will now show how it is possible to generalize this to a situation
with explanatory variables of any form. The first step is to go from the
two-sample situation to a $k$-sample one.

### Many Groups

Can we generalize to $(k + 1)$ groups, $k \ge 2$? Yes, by expanding the
procedure in the previous subsection:

\begin{eqnarray*}
h_0(t) &\sim& \mbox{group 0} \\
h_1(t) &\sim& \mbox{group 1} \\
\cdots & & \cdots \\
h_k(t) &\sim& \mbox{group k}
\end{eqnarray*}

The underlying model is: $h_j(t) = \psi_j h_0(t), \quad t \ge 0$, $j = 1, 2,
\ldots, k$. That is, with $(k+1)$ groups, we need $k$ proportionality
constants $\psi_1, \ldots, \psi_k$ in order to define proportional
hazards. Note also that in this formulation (there are others), one group
is "marked" as a *reference group*, that is, to this group there is no
proportionality constant attached. All relations are relative to the
reference group. Note also that it doesn't matter which group
is chosen as the reference. This choice does not change the model itself,
only its representation.

With $(k + 1)$ groups, we need $k$ indicators. Let

\begin{equation*}
\mathbf{x} = (x_{1}, x_{2}, \ldots, x_{k}).
\end{equation*}
Then

\begin{eqnarray*}
\mathbf{x} = (0, 0, \ldots, 0) & \Rightarrow & \mbox{group 0} \\
\mathbf{x} = (1, 0, \ldots, 0) & \Rightarrow & \mbox{group 1} \\
\mathbf{x} = (0, 1, \ldots, 0) & \Rightarrow & \mbox{group 2} \\
\cdots & & \cdots \\
\mathbf{x} = (0, 0, \ldots, 1) & \Rightarrow & \mbox{group k}
\end{eqnarray*}

and

\begin{equation*}
h(t; \mathbf{x}) = h_0(t) \prod_{\ell = 1}^k \psi_{\ell}^{x_{\ell}} =
\left\{ \begin{array}{ll}
  h_0(t), & \mathbf{x} = (0, 0, \ldots, 0) \\
  h_0(t) \psi_j, & x_{j} = 1, \quad j = 1, \ldots k
  \end{array}\right.
\end{equation*}
With $\psi_j = e^{\beta_j}, \; j = 1, \ldots, k$, we get

\begin{equation}
h(t; \mathbf{x}) = h_0(t) e^{x_{1}\beta_1 + x_{2} \beta_2 + \cdots +
x_{k} \beta_k} = h_0(t) e^{\mathbf{x} \boldsymbol{\beta}},
(\#eq:kgroups)
\end{equation}

where $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_k)$.

### The General Cox Regression Model

We may now generalize \@ref(eq:kgroups) by letting the components of
$\mathbf{x}_i$ take *any value*. Let data and model take
the following form:

Data: 

\begin{equation}
(t_{i0}, t_i, d_i, \mathbf{x}_i), \; i = 1, \ldots, n,
\end{equation}

where $t_{i0}$ is the *left truncation time point* (if $y_{i0} = 0$ for all $i$, 
then this variable may be omitted),
$t_i$ is the *end time point*,
$d_i$ is the *"event indicator"* ($1$ or *TRUE* if event, else $0$ or *FALSE*), and
$\mathbf{x}_i$ is a vector of *explanatory variables*.

Model:

\begin{equation}
h(t; \mathbf{x}_i) = h_0(t) e^{\mathbf{x}_i \boldsymbol{\beta}}, \quad t > 0.
(\#eq:pl)
\end{equation}

This is a *regression model* where the *response variable* is $(t_0, t, d)$ (we will call it a *survival object*)
and the *explanatory variable* is $\mathbf{x}$, possibly (often) vector valued.

In \@ref(eq:pl) there are two components to estimate, the
regression coefficients $\boldsymbol{\beta}$, and the 
*baseline hazard function*\index{baseline hazard function} $h_0(t),
\; t > 0$. For the former task, the *partial likelihood* [@cox75]
is used. See Appendix A for a brief summary.

## Estimation of the Baseline Cumulative Hazard Function
\index{baseline hazard function!estimation of|(}

The usual estimator (continuous time) of the baseline cumulative hazard
function is 

\begin{equation}
\hat{H}_0(t) = \sum_{j:t_j \le t} \frac{d_j}{\sum_{m \in R_j}
e^{\mathbf{x}_m \hat{\boldsymbol{\beta}}}},
(\#eq:bashaz)
\end{equation}

where $d_j$ is the number of events at $t_j$ and $\hat{\boldsymbol{\beta}}$ 
is the result of maximizing the partial likelihood. Note that if
$\hat{\boldsymbol{\beta}} = 0$, this 
reduces to 

\begin{equation}
\hat{H}_0(t) = \sum_{j:t_j \le t} \frac{d_j}{n_j},
(\#eq:km3)
\end{equation}

the Nelson-Aalen\index{Nelson-Aalen estimator} estimator. In
\@ref(eq:km3), $n_j$ is the size of $R_j$. 

In the **R** package  **eha**, the baseline hazard is estimated at the
value zero of the covariates (but at the reference category for a factor covariate).
This is different from practice in other packages, where some average value of covariate values 
are chosen as reference. However, this is generally a bad habit, because it will frequently lead to 
a variation in the reference value as subsets of the original data set is taken.
Instead, the practitioner must exercise judgment in choosing relevant reference values 
for (continuous) covariates. There are two instances when these choices make a 
difference: (i) When interactions are present, main effect estimates will vary 
with choice (more about this later), and (ii) estimation of baseline hazards and 
survival functions.

In order to calculate the cumulative hazards function for an individual
with a specific covariate vector $\mathbf{x}$, use the formula

\begin{equation*}
\hat{H}(t; \mathbf{x}) = \hat{H}_0(t) e^{\mathbf{x}\hat{\boldsymbol{\beta}}}.
\end{equation*}

The corresponding survival functions may be estimated by the relation

\begin{equation*}
\hat{S}(t; \mathbf{x}) = \exp\bigl(-\hat{H}(t; \mathbf{x})\bigr)
\end{equation*}

It is also possible to use the terms in the sum \@ref(eq:bashaz) to build
an estimator analogous to the Kaplan-Meier estimator \@ref(eq:km2). In
practice, there is no big difference between the two methods. For more on 
interpretation of parameter estimates and model selection, see the appropriate chapter. 
\index{baseline hazard function!estimation of|)}



## Proportional hazards in discrete time
\index{proportional hazards!discrete time}

In discrete time, the hazard function is, as we saw earlier, a set of
conditional 
probabilities\index{conditional probability}, and so its range is restricted
to the interval $(0, 1)$. Therefore, the definition of proportional hazards
used for continuous time is unpractical; the multiplication of a
probability by a constant may result in a quantity larger than one.

One way of motivating proportional hazards in discrete time is to regard
the discreteness as a result of grouping true continuous time data, for
which the proportional hazards assumption hold. For
instance, in a follow-up study of human mortality, we may only have data
recorded once a year, and so life length can only be measured in
years. Thus, we assume that there is a true exact life length $T$, but we
can only observe that it falls in an interval $(t_i, t_{i+1})$.

Assume *continuous* proportional hazards, and a *partition* of
time: 

$$
0 = t_0 < t_1 < t_2 < \cdots < t_k = \infty.
$$ 
Then
\begin{multline}\label{eq:dischaz}
P(t_{j-1} \le T < t_j \mid T \ge t_{j-1}; \; \mathbf{x}) =
\frac{S(t_{j-1}\mid \mathbf{x}) - S(t_j\mid \mathbf{x})}{S(t_{j-1} \mid \mathbf{x})} \\
 =  1 - \frac{S(t_j \mid \mathbf{x})}{S(t_{j-1} \mid \mathbf{x})}
 =  1 -
\left(\frac{S_0(t_j)}{S_0(t_{j-1})}\right)^{\exp(\boldsymbol{\beta}\mathbf{x})} \\
= 1 - (1 - h_i)^{\exp(\boldsymbol{\beta}\mathbf{x})}
\end{multline}
with $h_i = P(t_{j-1} \le T < t_j \mid T \ge t_{j-1}; \; \mathbf{x} =
\mathbf{0})$, $j = 1, \ldots, k$. We take \eqref{eq:dischaz} as the
\emph{definition} of proportional hazards in discrete time.

### Logistic regression

It turns out that a proportional hazards model in discrete time, according
to definition \eqref{eq:dischaz}, is nothing else than a *logistic
  regression*\index{logistic regression}
model with the *cloglog link*\index{cloglog link function} (cloglog is
short for "complementary log-log" or $\boldsymbol{\beta}\mathbf{x} =\log(-\log(p))$). In order
to see that, let 

\begin{equation}
  (1 - h_j) = \exp(-\exp(\alpha_j)), \; j = 1, \ldots, k,
\end{equation}
and

\begin{equation*}
X_j = \left\{ \begin{array}{ll}
          1, & t_{j-1} \le T < t_j \\
          0, & \text{otherwise}%t_1 \le T < t_2 \\
         \end{array} \right., \quad j = 1, \ldots, k.
\end{equation*}
Then 

\begin{equation*}
  \begin{split}
  P(X_1 = 1; \; \mathbf{x}) &=  1 - \exp(-\exp\big(\alpha_1 +
  \boldsymbol{\beta}\mathbf{x})\big)  \\
  P(X_j = 1 \mid X_1 = \cdots = X_{j-1} = 0; \; \mathbf{x}) &=  1 - \exp(-\exp\big(\alpha_j
+ \boldsymbol{\beta}\mathbf{x})\big), \\
\quad j = 2, \ldots, k.
\end{split}
\end{equation*}
This is logistic regression with a cloglog link. Note that extra parameters
$\alpha_1, \ldots, \alpha_k$ are introduced, one for each potential event
time (interval). They correspond to the baseline hazard function in continuous time,
but are be estimated simultaneously with the regression parameters.


## Doing it in **R**
\index{Functions!coxreg|(}

We utilize the *child mortality* data, SkellefteÃ¥ 1850--1884, to
illustrate tsome aspects of an ordinary Cox regression. Newborn in the parish
are sampled between 1820 and 1840 and followed to death or reaching the age
of 15, when they are right censored. So in effect child mortality in the
ages 20--40 are studied. The data set is named `child` and available in `eha`.

There are two **R** functions that are handy for a quick look at a data
frame, `str`\index{str} and `head`\index{head}. The function `head` prints the 
first few lines (observations) of a data
frame  (there is also a corresponding `tail` function that prints a few
of the *last* rows). 

```{r head3}
library(eha) #Loads also the data frame 'mort'.
ch <- child[, c("birthdate", "sex", "socBranch", 
                "enter", "exit", "event")] # Select interesting columns.
head(ch)
```

The function `str` gives a
summary description of the *structure* of an **R** object, often a
*data frame* \index{data frame}

```{r str3, echo=TRUE}
str(ch)
```

```{r getd3,echo=FALSE}
n.obj <- NROW(ch)
```
First, there is information about the data frame: It *is* a 
`data.frame`, with six variables measured on `r n.obj` objects. Then
each 
variable is individually described: name, type, and a few of the first values.
The values are usually rounded to a few digits. The *Factor* lines are
worth noticing: They describe of course `factor` covariates and the 
levels. Internally, the levels are coded 1, 2, ...,
respectively.

Also note the variable `birthdate`: It is of the `Date` type, and it has some 
quirks when used in regression models.


```{r mortreg}
res <- coxreg(Surv(exit, event) ~ sex + socBranch + birthdate, 
              data = ch)
print(summary(res), digits = 4)
```

Note that the coefficient for `birthdate` is equal to 0.0000 (with four decimals),
yet it is significantly different from zero. The explanation is that the time unit 
behind Dates is *day*, and it would be more reasonable to use *year* as time unit.
It can be accomplished by creating a new covariate, call it `cohort`, which is 
*birth year*. It is done with the aid of the functions `toTime` (in eha) and `floor`:

```{r tocoh3}
ch$cohort <- floor(toTime(ch$birthdate))
fit <- coxreg(Surv(exit, event) ~ sex + socBranch + cohort, data = ch)
print(summary(fit), digits = 4, short = TRUE)
```

Note the argument `short = TRUE` in the print statement. It suppresses the general 
statistics that were printed in the previous output below the table of coefficients. 

### The estimated baseline cumulative hazard function
\index{cumulative hazard function|(} 

The estimated baseline cumulative function (see Equation \@ref(eq:bashaz)) is best
plotted by the `plot` command, see Figure \@ref(fig:basl3).

```{r basl3,fig=TRUE,echo=FALSE, fig.cap="Estimated cumulative baseline hazard function.", fig.height = 3}
par(las = 1)
plot(fit, xlab = "Age")
```

The figure shows the baseline cumulative hazard function for an
individual with the value zero on all covariates. What does it mean here? There are two
covariates, the first is `ses` with reference value `lower`, and the second is `birthdate`,
with reference value 0! So Figure \@ref(fig:basl3) shows the the estimated cumulative hazards
function for a man born on January 1 in the year 0 (which does not exist, by the way)
and who belongs to a lower social class. This is not a reasonable extrapolation, and we
need to get a new reference value for `birthdate`. It should be within the range of the 
observed values, which is given by the `range` function:

```{r range3, echo = TRUE}
range(ch$cohort)
```
So a reasonable value to subtract is 1860 (or any meaningful value in the range):

```{r bd3}
ch$cohort <- ch$cohort - 1860
res3 <- coxreg(Surv(exit, event) ~ sex + socBranch + cohort, data = ch)
print(summary(res3), digits = 4, short = TRUE)
```
As you can see, nothing changes regarding the parameter estimates (because no 
interactions are present). What about the baseline cumulative hazards graph?

```{r basl33,fig=TRUE,echo=FALSE, fig.cap="Estimated cumulative baseline hazard function, centered birthdate at 1810."}
par(las = 1)
plot(res3, xlab = "Age")
```

The two Figures \@ref(fig:basl3) and \@ref(fig:basl33) are identical, *except* for
the the numbers on the $y$ axis. This would *not* hold for the corresponding plots
of the baseline *survival* function (try it!).

\index{cumulative hazard function|)} 
