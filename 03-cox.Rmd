# Cox Regression

```{r setup3,include=FALSE}
par(las = 1) # Doesn't work? Later?
options(show.signif.stars = FALSE)
```

\fun{eha} is good

We move slowly from the two-sample case, via the $k$-sample case, to the
general regression model. On this route, we  start with the *log-rank test*
and end up with *Cox regression* and the *proportional hazards model* [@cox72]. 

## Proportional Hazards {#sec:ph3}
\index{proportional hazards}

The property of *proportional hazards* is fundamental in Cox
regression. It is in fact the essence of Cox's simple yet ingenious
idea. The definition is as follows:

```{definition, name="Proportional hazards"}
If $h_1(t)$ and $h_0(t)$ are hazard functions from two separate
distributions, we say that they are *proportional* if

\begin{equation}
h_1(t)  =  \psi h_0(t), \quad \text{for all } t \ge 0,
(\#eq:prophaz)
\end{equation}
for some positive constant $\psi$ and *all* $t \ge 0$. Further, if
\eqref{eq:prophaz} holds, then the same 
property holds for the corresponding 
*cumulative hazard functions* $H_1(t)$ and $H_0(t)$.
\index{cumulative hazard function}

\begin{equation}
H_1(t)  =  \psi H_0(t), \quad \text{for all } t \ge 0,
(\#eq:propcumhaz)
\end{equation}
with the same proportionality constant $\psi$ as in \@ref(eq:prophaz).$\Box$
```

<!--
\begin{definition} Proportional hazards. \end{definition}
If $h_1(t)$ and $h_0(t)$ are hazard functions from two separate
distributions, we say that they are *proportional* if

\begin{equation}\label{eq:prophaz}
h_1(t)  =  \psi h_0(t), \quad \text{for all } t \ge 0,
\end{equation}
for some positive constant $\psi$ and *all* $t \ge 0$. Further, if
\eqref{eq:prophaz} holds, then the same 
property holds for the corresponding 
*cumulative hazard functions* $H_1(t)$ and $H_0(t)$.
\index{cumulative hazard function}

\begin{equation}\label{eq:propcumhaz}
H_1(t)  =  \psi H_0(t), \quad \text{for all } t \ge 0,
\end{equation}
with the same proportionality constant $\psi$ as in \ref{eq:prophaz}.$\Box$
-->


Strictly speaking, the second part of this definition follows easily from
the first (and vice versa), so more correct would be to state one part as a
definition and the other as a corollary.
The important part of this definition is "$\textit{for all }t \ge 0$",
and that 
the constant $\psi$ *does not depend on $t$*. Think of the hazard
functions as age-specific mortality for two groups, e.g., women and
men. It is "well known" that women have lower mortality than men in all
ages. It would therefore be reasonable to assume proportional
hazards in that case. It would mean that the female *relative*
advantage is equally large in all ages. See Example \@ref(exm:sw16) for a
demonstration of this example of *proportional
  hazards*\index{proportional hazards}. 

It must be emphasized that this is an assumption that always must be
carefully checked. In many other situations, it would not reasonable to assume
proportional hazards. If in doubt, check data by plotting the Nelson-Aalen
estimates\index{Nelson-Aalen estimator} for each group in the same plot.

See
Figure \@ref(fig:prophazfig) for an example with two *Weibull* hazard
functions and the proportionality constant $\psi = 2$.

```{r prophazfig,fig=TRUE,echo=FALSE, fig.cap = "Two hazard functions that are proportional. The proportionality constant is 2.",  fig.scap="Proportional hazard functions.", fig.height = 4}
library(eha)
par(las = 1)
x <- seq(0,1, length = 500)
#y <- dweibull(x, scale = 1, shape = 3) / (1 - pweibull(x, scale = 1, shape = 3))
y <- hweibull(x, scale = 1, shape = 3)
z <- 2 * y
plot(x, z, type = "l", main = "", xlab = "Time", ylab = "Hazards")
lines(x, y, lty = 2)
abline(h = 0)
```

The proportionality is often difficult to judge by eye, so in order make it
easier to see, the plot can be made on a *log scale*, see
Figure \@ref(fig:prophazlog3).


```{r prophazlog3,fig=TRUE,echo=FALSE,fig.cap="Two hazard functions that are proportional are on a constant distance from each other on a log-log scale. The proportionality constant is 2, corresponding to a vertical distance of log(2) =  0.693.", fig.scap = "Proportional hazard functions, log scale", fig.height = 4}
par(las=1)
x <- seq(0.01,1, length = 500)
#y <- dweibull(x, scale = 1, shape = 3) / (1 - pweibull(x, scale = 1, shape = 3))
y <- hweibull(x, scale = 1, shape = 3)
z <- 2 * y
plot(log(x), log(z), type = "l", main = "", xlab = "log(Time)", ylab = "log(Hazards)")
lines(log(x), log(y), lty = 2)
abline(h = 0)
```



Note that both dimensions are on a log scale. This type of plot,
constructed from empirical data, is called a *Weibull
  plot*\index{Weibull plot} in reliability applications: If the lines are
straight lines, then data are well fitted by a Weibull
distribution. Additionally, if the the slope of the line is 1 (45 degrees),
then an exponential model fits well.

To summarize Figure \@ref(fig:prophazlog3): (i) The hazard functions are
proportional because on the log-log scale, the vertical distance is
constant, (ii) Both hazard functions represent a Weibull distribution,
because both lines are straight lines, and (iii) neither represents an
exponential distribution, because the slopes are *not* one. This
latter fact may be difficult to see because of the different scales on the
axes (the *aspect ratio* is not one). 

## The Log-Rank Test

The *log-rank test* \index{Tests!log-rank} is a $k$-sample test
of equality of survival functions. We first look at the two-sample case,
that is, $k = 2$.

\subsection{Two samples}

Suppose that we have the small data set illustrated in
Figure \@ref(fig:twosample). There are two samples, the `letters` (A, B,
C, D, E) and the `numbers` (1, 2, 3, 4, 5).

(ref:twosamplecap) Two-sample data, the `letters` (dashed) and the `numbers` (solid). Circles denote censored observations, plusses events.

(ref:twosamplescap) Two-sample data.

```{r twosample, fig.cap = "(ref:twosamplecap)", fig.scap = "(ref:twosamplescap)", echo=FALSE }
two <- data.frame(group = c(rep("numbers", 5), rep("letters", 5)),
                  id = as.character(c(1:5, LETTERS[1:5])),
                  exit = c(4, 2, 6, 1, 3.5, 5, 3, 6, 1, 2.5),
                  event = c(1,0,1,1,0,1,1,0,1,0),
                  stringsAsFactors = FALSE)
plot(c(0, two$exit[1]), c(1, 1), type = "l", main = "", xlab="Duration", ylab = "Person No.",
     axes = FALSE,  ylim = c(0, 11), xlim = c(0, 8))
for (i in 2:5){
  lines(c(0, two$exit[i]), c(i, i), lwd = 1.5)
}
for (i in 6:10){
  lines(c(0, two$exit[i]), c(i, i), lty = 2, lwd = 1.5)
}
axis(1, at = c(0, two$exit[two$event == 1]))
axis(2, at = 1:10, labels = two$id, las = 1)
box()
abline(v = 0)
deaths <- two$exit[two$event == 1]
abline(v = deaths, lty = 3)
y <- which(two$event == 1)
nd <- length(deaths)
for (i in 1:10){
  if (two$event[i] == 1){
    text(two$exit[i] + 0.2, i, "+", cex = 1.5)
  }else{
    text(two$exit[i], i, "o", cex = 1.5)
  }
}
```

The data in Figure \@ref(fig:twosample) can be presented i tabular form, see Table \@ref(tab:exlr).

```{r exlr, echo = FALSE}
library(survival)
ex.data <- data.frame(group = c(rep("numbers", 5), rep("letters", 5)),
                      time = c(4,2,6,1,3.5,5,3,6,1,2.5),
                      event = c(TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, 
                                TRUE, FALSE, TRUE, FALSE))
knitr::kable(ex.data, booktabs = TRUE, caption = "Example data for the log-rank test..")
```

<!--
\begin{figure}[ht!]
\begin{center}
\psset{yunit=0.45cm}
\pspicture(-1, -1.5)(10,12)% Data
\psaxes[ticks=none,labels=none,linewidth=1pt]{->}(9,11)
\psset{linecolor=black}
\uput[90](9,-1.5\baselineskip){t}
\uput[90](0.5,11){Person No.}
%1
\uput[0](-0.7,1){{\bf  1}}
\psline(0,1)(4,1)
\uput[0](4,1){+}
%\pscircle(4,1){4pt}
\psline[linestyle=dotted](4,0)(4,10.2)
%\uput[90](4,-2\baselineskip){$t_{(2)}$}%\psline(0,1)(4,1)
\uput[90](4,-1.5\baselineskip){$t_{(3)}$}%\psline(0,1)(4,1)
%2
\uput[0](-0.7,2){{\bf  2}}
\psline(0,2)(2,2)
%\uput[0](4.2,1){+}
\pscircle(2,2){4pt}
%\psline[linestyle=dashed](2,0)(2,5)
%\uput[90](4,-2\baselineskip){$t_{(1)}$}%\psline(0,1)(4,1)
%3
\uput[0](-0.7,3){{\bf  3}}
\psline(0,3)(6,3)
\uput[0](6,3){+}
%\pscircle(4,1){4pt}
\psline[linestyle=dotted,linecolor=black](6,0)(6,10.2)
\uput[90](6,-1.5\baselineskip){$t_{(5)}$}
%4
\uput[0](-0.7,4){{\bf  4}}
\psline(0,4)(1,4)
\uput[0](1,4){+}
%\pscircle(4,1){4pt}
\psline[linestyle=dotted,linecolor=black](1,0)(1,10.2)
%\uput[90](1,-2\baselineskip){$t_{(1)}$}%\psline(0,1)(4,1)
%\uput[90](1,-1.5\baselineskip){ 1}%\psline(0,1)(4,1)
%5
\uput[0](-0.7,5){{\bf  5}}
\psline(0,5)(3.5,5)
%\uput[0](4.2,1){+}
\pscircle(3.5,5){4pt}
%\psline[linestyle=dashed](4,0)(4,5)
%\uput[90](4,-2\baselineskip){$t_{(1)}$}\psline(0,1)(4,1)
%\endpspicture
%\psset{yunit=0.6cm}

%\pspicture(-1, -1.5)(10,6)% Data
%\psaxes[ticks=none,labels=none,linewidth=1pt]{->}(9,5.5)
%\uput[90](9,-1.5\baselineskip){duration}
%\uput[90](0.5,5.5){Person No.}
%1
\psset{linecolor=black}
\uput[0](-0.7,6){{\bf  A}}
\psline[linestyle=dashed](0,6)(5,6)
\uput[0](5,6){+}
%\pscircle(4.5,6){4pt}
\psline[linecolor=black,linestyle=dotted](5,0)(5,10.2)
\uput[90](5,-1.5\baselineskip){$t_{(4)}$}
%\uput[90](5,-1.5\baselineskip){5}%\psline(0,1)(4,1)
%2
\uput[0](-0.7,7){{\bf  B}}
\psline[linestyle=dashed](0,7)(3,7)
\uput[0](3,7){+}
%\pscircle(2,7){4pt}
\psline[linestyle=dotted,linecolor=black](3,0)(3,10.2)
\uput[90](3,-1.5\baselineskip){$t_{(2)}$}
%3
\uput[0](-0.7,8){{\bf  C}}
\psline[linestyle=dashed](0,8)(6,8)
%\uput[0](6.2,8){+}
\pscircle(6,8){4pt}
%\psline[linestyle=dashed,linecolor=black](6,0)(6,10.2)
%\uput[90](6,-2\baselineskip){$t_{(3)}$}%\psline(0,1)(4,1)
%\uput[90](6,-1.5\baselineskip){ 6}%\psline(0,1)(4,1)
%4
\uput[0](-0.7,9){{\bf  D}}
\psline[linestyle=dashed](0,9)(1,9)
\uput[0](1,9){+}
%\pscircle(4,1){4pt}
%\psline[linestyle=dashed,linecolor=black](1,0)(1,5)
%\uput[90](1,-2\baselineskip){$t_{(1)}$}%\psline(0,1)(4,1)
\uput[90](1,-1.5\baselineskip){$t_{(1)}$}
%5
\uput[0](-0.7,10){{\bf  E}}
\psline[linestyle=dashed](0,10)(2.5,10)
%\uput[0](4.2,1){+}
\pscircle(2.5,10){4pt}
%\psline[linestyle=dashed](4,0)(4,5)
%\uput[90](4,-2\baselineskip){$t_{(1)}$}\psline(0,1)(4,1)
\endpspicture
\caption[Two-sample data]{Two-sample data, the {\tt letters} (dashed) and the {\tt numbers}
  (solid). Circles denote censored observations, plusses uncensored.}
\label{fig:twosample}
\end{center}
\end{figure}
-->

We are interested in investigating whether `letters` and `numbers`
have the same survival chances or not. Therefore, the hypothesis

\begin{equation*}
H_0: \text{No difference in survival between numbers and letters}
\end{equation*}
is formulated. In order to test $H_0$, we make five tables, one
for each observed *event time*\index{event time}, see
Table \@ref(tab:lr1), where the the first table, relating to failure time $t_{(1)} = 1$, is shown.

<!--
\begin{table}[ht!]
\tabletitle[Log rank test summary]{Five $2\times 2$ tables. Each table
  reports the situation at one
  observed failure time point, i.e., at $t_{(1)}, \ldots, t_{(5)}$.}
\label{tab:lr1}
\begin{tabular}{|c|l|cc|r|}
Time & Group & Deaths & Survivals & Total \\ \hline
$t_{(1)}$ & {\tt numbers}  &   1    &    4      & 5 \\
& {\tt letters}   &   1    &    4      & 5 \\ \hline
& Total &   2    &    8      & 10 \\ \hline \hline
$t_{(2)}$ &  {\tt numbers}  &   0    &    3      & 3 \\
& {\tt letters}   &   1    &    2      & 3 \\ \hline
& Total &   1    &    5      & 6 \\ \hline \hline
$t_{(3)}$ &  {\tt numbers}  &   1    &    1      & 2 \\
& {\tt letters}   &   0    &    2      & 2 \\ \hline
& Total &   1    &    3      & 4 \\ \hline \hline
$t_{(4)}$ &  {\tt numbers}  &   0    &    1      & 1 \\
& {\tt letters}   &   1    &    1      & 2 \\ \hline
& Total &   1    &    2      & 3 \\ \hline \hline
$t_{(5)}$ &  {\tt numbers}  &   1    &    0      & 1 \\
& {\tt letters}   &   0    &    1      & 1 \\ \hline
& Total &   1    &    1      & 2 \\ \hline \hline
\end{tabular}
\end{table}


Table: (\#tab:lr1) The $2 \times 2$ table for the first event time.

| Group   | Deaths | Survivals | **Total** |
|---------|--------|-----------|-------|
| numbers |  1     |  4        |  5    |
| letters |  1     |  4        |  5    |
| **Total**   |  2     |  8        | 10    |  

-->

```{r lr1, echo = FALSE}
xx <- matrix(c(1,1,2,4,4,8,5,5,10), ncol = 3)
rownames(xx) <- c("numbers", "letters", "Total")
colnames(xx) <- c("Deaths", "Survivals", "Total")
knitr::kable(xx, caption = "The 2x2 table for the first event time.", booktabs = TRUE)
```


Let us look at the table at failure time $t_{(1)} = 1$, i.e., Table \@ref(tab:lr1), 
from the viewpoint of the `numbers`.

*    The observed number of deaths among `numbers`: $1$.
*    The expected number of deaths among `numbers`: $2 \times 5 / 10 = 1$.

The *expected* number is calculated under $H_0$, i.e., as if there is
no difference between `letters` and `numbers` regarding mortality. It is further
assumed that the two margins (Total) are given (fixed). 

Then, given two deaths in
total and five out of ten observations are from the group `numbers`,
the expected number of deaths is calculated as above.

This procedure is repeated for each of the five tables, and the results are
summarized in Table \@ref(tab:sumup).

```{r sumup, echo = FALSE}
x <- c(1, 0, 1, 0, 1, 3,
       1.0, 0.5, 0.5, 0.3, 0.5, 2.8,
       0.0, -0.5, 0.5, -0.3, 0.5, 0.2,
       0.44, 0.25, 0.25, 0.22, 0.25, 1.41)
x <- matrix(x, ncol = 4)
colnames(x) <- c("Observed", "Expected", "Difference", "Variance")
rownames(x) <- c("t(1)", "t(2)", "t(3)", "t(4)", "t(5)", "Sum")
knitr::kable(x, booktabs = TRUE, caption = "Observed and expected number of deaths at event times.")
```


<!--
\begin{table}[ht!]
\tabletitle[Logrank test: Summary table]{Observed and expected number of deaths for {\tt numbers} at the
five observed event times.}
\label{tab:sumup}
\begin{tabular}{r|rr|r|r}
Time  & Observed & Expected & Difference & Variance\\ \hline
$t_{(1)}$ & 1 & 1.0 & 0.0 & 0.44\\
$t_{(2)}$ & 0 & 0.5 & -0.5 & 0.25 \\
$t_{(3)}$ & 1 & 0.5 & 0.5 & 0.25 \\
$t_{(4)}$ & 0 & 0.3 & -0.3 & 0.22 \\
$t_{(5)}$ & 1 & 0.5 & 0.5 & 0.25 \\ \hline
Sum & 3 & 2.8 & 0.2 & 1.41 \\ \hline
\end{tabular}
\end{table}
-->

<!--
Table: (#\tab:sumup)
-->

Finally, the observed test statistic $T$ is calculated as

\begin{equation*}
T = \frac{0.2^2}{1.41} \approx 0.028
\end{equation*}
Under the null hypothesis, this is an observed value from a 
$\chi^2(1)$\index{Distributions!chi-square@$\chi^2$}
distribution, and $H_0$ should be rejected for *large* values of
$T$. Using a *level of significance* of 5%, the cutting point for the
value of $T$ is 3.84, far from our observed value of 1.41. The
conclusion is
therefore that there is no (statistically significant) difference in
survival chances between `letters` and `numbers`.
Note, however, that this result depends on 
asymptotic\index{asymptotic theory} (large sample) 
properties, and 
in this toy example, these properties are not valid.

For more detail about the underlying theory, see Appendix A.

In **R**, the log-rank test is performed by the 
`coxph`\index{Functions!`coxph`} function in the
package survival (there are other options). 

Let us now look at a real data example, the old age mortality data set
`oldmort` in `eha`. See Table \@ref(tab:lrreal3dat) for a sample of five records with selected columns. 


```{r lrreal3dat,echo = FALSE}
options(show.signif.stars = FALSE)
library(eha) # Loads 'survival' as well.
om <- age.window(oldmort, c(60, 85))
knitr::kable(head(om[-(1:1000), c("id", "enter", "exit", "event", "sex", "civ")], 5), booktabs=TRUE, caption = "Old age mortality.", row.names = FALSE)
```

We are interested in comparing male and female mortality in the ages 60--85 with a logrank test, and for that purpose we run a *Cox regression* analysis:

```{r noevallr, eval = FALSE}
fit <- coxph(Surv(enter, exit, event) ~ sex, data = om)
```

The result is given by `summary(fit)`:

```{r lrreal3,comment = NA, echo = FALSE}
fit <- coxph(Surv(enter, exit, event) ~ sex, data = om)
summary(fit)
```

Obviously, we got a lot of information here, more than we actually need. We have in fact performed
a Cox regression slightly ahead of schedule! The result of the
logrank test is displayed on the last line of output. The $p$-value is
$1.179 \times 10^{-5}$, a very small number. Thus, there is a very significant difference in mortality between men and women.
But *how large* is the difference? The answer is found at `exp(coef) = 0.8135`, which tells us that the female risk of dying is about 81% of the male risk, *at each age between 60 and 85*.

Remember that this result depends on the proportional hazards
assumption. We can graphically check it as follows.

```
sf <- survfit(Surv(enter, exit, event) ~ strata(sex), 
              data = om, start.time = 60)
plot(sf, xlab = "Age", fun = "cumhaz")
```

Note that the grouping factor (`sex`) is given through the
function `strata` in the formula. The result is shown in Figure \@ref(fig:phazom3).

```{r phazom3, fig.cap = "Old age mortality, women vs. men, cumulative hazards.", echo = FALSE, fig.height = 4}
par(las = 1)
##plot(survfit(fit))
sf <- survfit(Surv(enter, exit, event) ~ strata(sex), 
              data = om, start.time = 60)
plot(sf, xlab = "Age", main = "", fun = "cumhaz", lty = 1:2)
legend("topleft", legend = c("Men", "Women"), lty = 1:2)
```

<!--
First note that the out-commented  version works equally well, but it
is more tedious to type, and also harder to read. The `with`
\index{Functions!`with`}} function is very handy and will be used often
throughout the book.
-->


<!--
\begin{figure}[ht!]
\includegraphics{{$HOME}/Documents/ehaBook/Sweave/figs/ch3-grch}
\caption{Old age mortality, women vs.\ men, cumulative hazards.}
\label{fig:phazom3}
\end{figure}
-->

The proportionality assumption seems to be a good description from 60 to
85--90 years of age, but it seems more doubtful in the very high ages. One
reason for this may be that the high-age estimates are based on few
observations (most of the individuals in the sample died earlier), so
random fluctuations have a large impact in the high ages.

### Several samples

The result for the two-sample case is easily extended to the $k$-sample
case. Instead of one $2 \times 2$ table per observed event time we get one
$k\times 2$ table per observed event time and we have to calculate expected
and observed numbers of events for $(k-1)$ groups at each failure time. The
resulting test statistic will have $(k-1)$ degrees of freedom and still be
approximately $\chi^2$ distributed. This is illustrated with the same data
set, `oldmort`, as above, but with the covariate `civ`,
which is a `factor` with three levels (`unmarried`, `married`, `widow`), instead of `sex`.
Furthermore, the investigation is limited to *male mortality*.

```{r lrplace3, echo = FALSE, comment = NA}
summary(fit <- coxph(Surv(enter, exit, event) ~ civ,  
                     data = om[om$sex == "male", ]))
```
The degrees of freedom for the *score test*\index{Tests!score}
is now 2, equal to the number
of levels in `civ` 
minus one. Being `unmarried` seem to have great impact on
old age mortality. It is however recommended to check the proportionality
assumption graphically, see Figure \@ref(fig:grbp3).

```{r grbp3, fig.cap = "Old age male mortality by civil status, cumulative hazards.", echo=FALSE}
par(las = 1)
sfm <- survfit(Surv(enter, exit, event) ~ strata(civ), 
               data = om[om$sex == "male", ], start.time = 60)
plot(sf, xlab = "Age", fun = "cumhaz", lty = 1:3, ylab = "")
##with(om[om$sex == "male", ], plot(Surv(enter, exit, event), strata = civ, main = "", xlab = "Age", ylab = "", las))
```

<!--
\begin{figure}[ht!]
\includegraphics{{$HOME}/Documents/ehaBook/Sweave/figs/ch3-grbp3}
\caption{Old age mortality by birthplace, cumulative hazards.}
\label{fig:grbp3}
\end{figure}
-->

There is obviously nothing that indicates non-proportionality in this case
either. Furthermore, the `unmarried` have have significantly higher mortality than both `married`
and `widows`.

We do not go deeper into this matter here, mainly because the logrank test
generally is a special case of Cox regression, which will be described in
detail later in this chapter.

<!--
There are however two special cases of the
logrank test, the *weighted* and *stratified* versions, that may
be of interest on 
their own. The interested reader may consult the text books by
\citet[pp.\ 22-23]{kp02} or \citet[pp.\ 49--53]{collett}.
-->



## Proportional Hazards Regression Models
\index{proportional hazards!regression}

The definition in Section \@ref(sec:ph3) is valid for models in continuous
time. 
Figure \@ref(fig:prophazfig2) shows the relationships between the cumulative
hazards functions, the density functions, and the survival functions when
the hazard functions are proportional. Note that the cumulative hazards
functions are proportional by implication, with the same proportionality
constant ($\psi = 2$ in this case). On the other hand, for the density and
survival functions, proportionality does not hold; it is in fact
theoretically impossible except in the trivial case that the
proportionality constant is unity.


```{r prophazfig2,fig=TRUE,echo=FALSE,height=5, fig.cap="The effect of proportional hazards on the density and survival functions.",fig.scap="Effect of proportional hazards."}
oldpar <- par(mfrow = c(2, 2))
x <- seq(0,2.5, length = 500)
library(eha)
y <- hweibull(x, scale = 1, shape = 3)
z <- hweibull(x, scale = 2^(1/3), shape = 3)
plot(x, y, type = "l", main = "Hazards", xlab = "Time", ylab = "Hazards")
lines(x, z, lty = 2)
abline(h = 0)
y <- Hweibull(x, scale = 1, shape = 3)
z <- Hweibull(x, scale = 2^(1/3), shape = 3)
plot(x, y, type = "l", main = "Cumulative hazards", xlab = "Time", ylab = "Cum. hazards")
lines(x, z, lty = 2)
abline(h = 0)
y <- dweibull(x, scale = 1, shape = 3)
z <- dweibull(x, scale = 2^(1/3), shape = 3)
plot(x, y, type = "l", main = "Density functions", xlab = "Time", ylab = "Density")
lines(x, z, lty = 2)
abline(h = 0)
y <- pweibull(x, scale = 1, shape = 3, lower.tail = FALSE)
z <- pweibull(x, scale = 2^(1/3), shape = 3, lower.tail = FALSE)
plot(x, y, type = "l", main = "Survival functions", xlab = "Time", ylab = "Survival", ylim = c(0, 1))
lines(x, z, lty = 2)
abline(h = 0)
##
par(oldpar)
```

### Two Groups

The definition of proportionality, given in equation \@ref(eq:prophaz), can
equivalently be written as

\begin{equation}
h_x(t) = \psi^x h_0(t), \quad t > 0, \; x = 0, 1, \; \psi > 0.
(\#eq:ph2)
\end{equation}

It is easy to see that this "trick" is equivalent to \@ref(eq:prophaz):
When $x = 0$ it simply says that $h_0(t) = h_0(t)$, and when $x = 1$ it
says that $h_1(t) = \psi h_0(t)$. Since $\psi > 0$, we can calculate $\beta =
\log(\psi)$, and rewrite \@ref(eq:ph2) as

\begin{equation*}
h_x(t) = e^{\beta x} h_0(t), \quad t > 0; \; x = 0, 1; \; -\infty < \beta < \infty,
\end{equation*}
or with a slight change in notation,

\begin{equation}
h(t; x) = e^{\beta x} h_0(t), \quad t > 0; \; x = 0, 1; \; -\infty < \beta
< \infty.
(\#eq:phsimple)
\end{equation}

The sole idea by this rewriting is to pave the way for the introduction of
*Cox's regression model* [@cox72], which in its elementary form is a
proportional hazards model. In fact, we can already interpret
equation \@ref(eq:phsimple) as a Cox regression model with an explanatory
variable $x$ with corresponding regression coefficient (to be estimated
from data) $\beta$. The covariate $x$ is still only a dichotomous variate,
but we will now show how it is possible to generalize this to a situation
with explanatory variables of any form. The first step is to go from the
two-sample situation to a $k$-sample one.

### Many Groups

Can we generalize to $(k + 1)$ groups, $k \ge 2$? Yes, by expanding the
procedure in the previous subsection:

\begin{eqnarray*}
h_0(t) &\sim& \mbox{group 0} \\
h_1(t) &\sim& \mbox{group 1} \\
\cdots & & \cdots \\
h_k(t) &\sim& \mbox{group k}
\end{eqnarray*}

The underlying model is: $h_j(t) = \psi_j h_0(t), \quad t \ge 0$, $j = 1, 2,
\ldots, k$. That is, with $(k+1)$ groups, we need $k$ proportionality
constants $\psi_1, \ldots, \psi_k$ in order to define proportional
hazards. Note also that in this formulation (there are others), one group
is "marked" as a *reference group*, that is, to this group there is no
proportionality constant attached. All relations are relative to the
reference group. Note also that it essentially doesn't matter which group
is chosen as the reference. This choice does not change the model itself,
only its representation.

With $(k + 1)$ groups, we need $k$ indicators. Let

\begin{equation*}
\mathbf{x} = (x_{1}, x_{2}, \ldots, x_{k}).
\end{equation*}
Then

\begin{eqnarray*}
\mathbf{x} = (0, 0, \ldots, 0) & \Rightarrow & \mbox{group 0} \\
\mathbf{x} = (1, 0, \ldots, 0) & \Rightarrow & \mbox{group 1} \\
\mathbf{x} = (0, 1, \ldots, 0) & \Rightarrow & \mbox{group 2} \\
\cdots & & \cdots \\
\mathbf{x} = (0, 0, \ldots, 1) & \Rightarrow & \mbox{group k}
\end{eqnarray*}

and

\begin{equation*}
h(t; \mathbf{x}) = h_0(t) \prod_{\ell = 1}^k \psi_{\ell}^{x_{\ell}} =
\left\{ \begin{array}{ll}
  h_0(t), & \mathbf{x} = (0, 0, \ldots, 0) \\
  h_0(t) \psi_j, & x_{j} = 1, \quad j = 1, \ldots k
  \end{array}\right.
\end{equation*}
With $\psi_j = e^{\beta_j}, \; j = 1, \ldots, k$, we get

\begin{equation}
h(t; \mathbf{x}) = h_0(t) e^{x_{1}\beta_1 + x_{2} \beta_2 + \cdots +
x_{k} \beta_k} = h_0(t) e^{\mathbf{x} \boldsymbol{\beta}},
(\#eq:kgroups)
\end{equation}

where $\boldsymbol{\beta} = (\beta_1, \beta_2, \ldots, \beta_k)$.

### The General Proportional Hazards Regression Model

We may now generalize \@ref(eq:kgroups) by letting the components of
$\mathbf{x}_i$ take *any value*. Let data and model take
the following form:

Data: 

\begin{equation}
(t_{i0}, t_i, d_i, \mathbf{x}_i), \; i = 1, \ldots, n,
\end{equation}

where $t_{i0}$ is the *left truncation time point* (if $y_{i0} = 0$ for all $i$, then this variable may be omitted,
$t_i$ is the *end time point*,
$d_i$ is the *"event indicator"* ($1$ or *TRUE* if event, else $0$ or *FALSE*), and
$\mathbf{x}_i$ is a vector of *explanatory variables*.

Model:

\begin{equation}
h(t; \mathbf{x}_i) = h_0(t) e^{\mathbf{x}_i \boldsymbol{\beta}}, \quad t > 0.
(\#eq:pl)
\end{equation}

This is a *regression model* where the *response variable* is $(t_0, t, d)$ (we will call it a *survival object*)
and the *explanatory variable* is $\mathbf{x}$, possibly (often) vector valued.

In \@ref(eq:pl) there are two components to estimate, the
regression coefficients $\boldsymbol{\beta}$, and the 
*baseline hazard function*\index{baseline hazard function} $h_0(t),
\; t > 0$. For the former task, the *partial likelihood* [@cox75]
is used. See Appendix A for a brief summary.

## Estimation of the Baseline Hazard
\index{baseline hazard function!estimation of|(}

The usual estimator (continuous time) of the baseline cumulative hazard
function is 

\begin{equation}
\hat{H}_0(t) = \sum_{j:t_j \le t} \frac{d_j}{\sum_{m \in R_j}
e^{\mathbf{x}_m \hat{\boldsymbol{\beta}}}},
(\#eq:bashaz)
\end{equation}

where $d_j$ is the number of events at $t_j$. Note that if
$\hat{\boldsymbol{\beta}} = 0$, this 
reduces to 

\begin{equation}
\hat{H}_0(t) = \sum_{j:t_j \le t} \frac{d_j}{n_j},
(\#eq:km3)
\end{equation}

the Nelson-Aalen\index{Nelson-Aalen estimator} estimator. In
\@ref(eq:km3), $n_j$ is the size of $R_j$. 

In the **R** package  **eha**, the baseline hazard is estimated at the
means of the covariates (or, more precisely, at the means of the columns of
the design matrix; this makes a difference for factors).

\begin{equation}
\hat{H}_0(t) = \sum_{j:t_j \le t} \frac{d_j}{\sum_{m \in R_j}
e^{(\mathbf{x}_m - \bar{\mbox{$\mathbf{x}$}})\hat{\boldsymbol{\beta}}}},
(\#eq:bashazeha)
\end{equation}

In order to calculate the cumulative hazards function for an individual
with a specific covariate vector $\mathbf{x}$, use the formula

\begin{equation*}
\hat{H}(t; \mathbf{x}) = \hat{H}_0(t) e^{(\mathbf{x} - \bar{\mbox{$\mathbf{x}$}})\hat{\boldsymbol{\beta}}}.
\end{equation*}

The corresponding survival functions may be estimated by the relation

\begin{equation*}
\hat{S}(t; \mathbf{x}) = \exp\bigl(-\hat{H}(t; \mathbf{x})\bigr)
\end{equation*}

It is also possible to use the terms in the sum \@ref(eq:bashaz) to build
an estimator analogous to the Kaplan-Meier estimator \@ref(eq:km2). In
practice, there is no big difference between the two methods. 
\index{baseline hazard function!estimation of|)}


## Explanatory Variables

Explanatory variables, or *covariates*\index{covariate}, may be of
essentially two 
different types *continuous* and *discrete*. The discrete type
usually takes only a finite number of distinct values and is often called a
`factor`, e.g., in **R**. A special case of a factor is one that takes
only two distinct values, say 0 and 1. Such a factor is called an
*indicator*, because we can let the value 1 indicate the presence of a
certain property and 0 denote its absence. To summarize, there is 

*    **Covariate**: taking values in an *interval* (e.g., *age*, *blood pressure*).
*    **Factor**: taking a *finite* number of values (e.g., *civil status*, *occupation*).\index{factor}
*    **Indicator**: a factor taking *two* values (e.g., *gender*).


### Continuous Covariates

We use the qualifier *continuous* to stress that factors are
excluded, because often the term *covariate* is used as a synonym for
*explanatory variable*. 

Values taken by a continuous covariate are *ordered*. The *effect* on the response is by model definition ordered in
the *same* or *reverse* order.
On the other hand, values taken by a factor are 
*unordered*\index{factor!unordered} (but may be defined as 
  ordered\index{factor!ordered} in **R**). 

### Factor Covariates

An explanatory variable that can take only a finite (usually small) number
of distinct values is called a 
*categorical variable*\index{categorical variable}. In **R**
language, it is called a 
*factor*\index{factor}. Examples of such variables are
  *gender*, *socio-economic status*, *birth place*. Students of
statistics have long been taught to create *dummy variables* in such
situations, in the following way: 

*    Given a categorical variable $F$ with $(k+1)$ levels $(f_0, f_1, f_2, \ldots f_k)$ ($k+1$ levels),
*    Create $k$ *indicator* (``dummy'') variables $(I_1, I_2, \ldots I_k)$.

<!--
\begin{tabular}{c|cccccc|l}
$F$   & $I_1$ & $I_2$ & $I_3$ & $\cdots$ & $I_{k-1}$ & $I_k$ & Comment\\ \hline
$f_0$ &  0    &  0    &  0    & $\cdots$ & 0         & 0  & *reference}   \\
$f_1$ &  1    &  0    &  0    & $\cdots$ & 0         & 0     \\
$f_2$ &  0    &  1    &  0    & $\cdots$ & 0         & 0     \\
$f_3$ &  0    &  0    &  1    & $\cdots$ & 0         & 0     \\
$\cdots$ &  $\cdots$  &  $\cdots$  &  $\cdots$  & $\cdots$ & $\cdots$
& $\cdots$     \\
$f_{k-1}$ &  0    &  0    &  0    & $\cdots$ & 1         & 0 \\
$f_k$ &  0    &  0    &  0    & $\cdots$ & 0         & 1
\end{tabular}

-->
The level $f_0$ is the reference category, characterized by that all indicator
variables are zero for an individual with this value. Generally, for
the level, $f_i, \; i = 1,\ldots, k$, the indicator variable $I_i$ is one,
the rest are zero. In other words, for a single individual, at most one
indicator is one, and the rest are zero.

In **R**, there is no need to explicitly create dummy variables, it is done
behind the scenes by the functions 
`factor`\index{Functions!factor} and 
`as.factor`\index{Functions!as.factor}.

Note that a factor with *two* levels, i.e., an indicator
variable\index{indicator variable}, can always be treated as a continuous
covariate, if coded 
numerically (e.g., 0 and 1).

```{example, name="Infant mortality and age of mother"}
Consider a demographic example, the influence of mother's age (a continuous covariate) on infant mortality.
It is considered well-known that a *young* mother means high risk for
the infant, and
also that *old* mother means high risk, compared to
*"in-between-aged"* mothers. So the risk order is not the same (or
reverse) as the age order. 
```
One solution (not necessarily the best) to this problem is to *factorize*: Let, for instance,

\begin{equation*}
\mbox{mother's age} = \left\{\begin{array}{ll}
                           \mbox{low}, & 15 < \mbox{age} \le 25 \\
                           \mbox{middle}, & 25 < \mbox{age} \le 35 \\
                           \mbox{high}, & 35 < \mbox{age}
                            \end{array} \right.
\end{equation*}

In this layout, there will be two parameters measuring the deviation from
the reference category, which will be the first category by default.

In **R**, this is easily achieved with the aid of the 
`cut`\index{Functions!cut} function. It
works like this:

```{r cut}
age <- rnorm(100, 30, 6)
age.group <- cut(age, c(15, 25, 35, 48))
summary(age.group)      
```

Note that the created intervals by default are closed to the right and open
to the left. This has consequences for how observations exactly on a
boundary are treated; they belong to the lower-valued interval. The
argument `right` in the call to `cut` can be used switch this
behaviour the other way around. 

Note further that values falling below the smallest value (15 in our
example) or above the largest value (48) are reported as *missing values*\index{missing values}
(`NA` in **R** terminology, `Not Available`).

For further information about the use of the `cut` function, see the
help page. $\Box$


## Interactions

The meaning of *interaction* between two explanatory variables is
described by an example, walking through the three possible combinations of covariate types.
We return to the `oldmort` data set, slightly modified, see Table \@ref(tab:ommod), where the first six of a total to 6495 rows are shown. 

```{r ommod, echo = FALSE}
om <- age.window(oldmort, c(60, 85))
om$farmer <- om$ses.50 == "farmer"
om$farmer <- factor(om$farmer, labels = c("no", "yes"))
om <- om[, c("birthdate", "enter", "exit", "event", "sex", "farmer", "imr.birth")]
knitr::kable(head(om), booktabs = TRUE, caption = "Old age mortality, Sundsvall 1860-1880", row.names = FALSE)
```

The sampling frame is a rectangle in the Lexis diagram, see Figure \@ref(fig:lexmort). It can be described as all persons born between January 1, 1775 and January 1, 1881, and present in the solid rectangle at some moment.

```{r lexmort, fig.cap = "Old age mortality sampling frame, Lexis diagram.", echo = FALSE}
plot(c(1860, 1880), c(60, 60), type = "l", ylim = c(0, 100), xlim = c(1770, 1890), axes = FALSE, lwd = 1.5, xlab = "Year", ylab = "Age")
axis(1, at = c(1775, 1821, 1860, 1881))
axis(2, at = c(0, 60, 85, 100), las = 1)
box()
lines(c(1860, 1881), c(85, 85), lwd = 1.5)
lines(c(1860, 1860), c(60, 85), lwd = 1.5)
lines(c(1881, 1881), c(60, 85), lwd = 1.5)
abline(h = c(60, 85), lty = 3)
abline(v = c(1860, 1881), lty = 3)
lines(c(1775, 1860), c(0, 85), lty = 2)
lines(c(1821, 1881), c(0, 60), lty = 2)
abline(h = 0)
```

```{example, name = "Old age mortality"}
There are four possible explanatory variables behind survival after age 60 in the example. 
```
\begin{eqnarray*}
\mbox{farmer} & = & \left\{ \begin{array}{ll}
             \mbox{no} & \\
             \mbox{yes} & \left(e^{\alpha}\right)
            \end{array} \right. \\
%\end{equation*}
%\begin{equation*}
\mbox{sex} & = & \left\{ \begin{array}{ll}
             \mbox{male} & \\
             \mbox{female} & \left(e^{\beta}\right)
            \end{array} \right. \\
\mbox{birthdate } (x_1) & & \mspace{90mu} \left(e^{\gamma_1 x_1}\right) \\
\mbox{IMR at birth } (x_2) & & \mspace{90mu} \left(e^{\gamma_2 x_2}\right) \\
\end{eqnarray*}

The two first are factors, and the last two are continuous covariates. We
go through the three distinct combinations and illustrate the meaning of
interaction in each. $\Box$


### Two factors

Assume that we, in the oldmort example, only have the factors `sex` and `farmer` at our 
disposal as explanatory variables. We may fit a Cox regression model like this:

```{r coxtwofacfit, comment = NA}
fit <- coxph(Surv(enter, exit, event) ~ sex + farmer, data = om)
```


The output from `coxph` (the *model fit*) is saved in the object `fit`, which is investigated,
first by the the function `drop1`, which performs successive $\chi^2$ tests of the significance of 
the estimated regression coefficients (the null hypothesis in each case is that the true coefficient 
is zero). 

```{r coxtwofacdrop, comment = NA}
drop1(fit, test = "Chisq")
```

The $p$-value corresponding to the coefficient for `sex` is *very* small, meaning that there 
is a strongly significant difference in mortality between women and men. The difference in mortality 
between farmers and non-farmers is also statistically significant, but with a larger $p$-value.

Next, we want to see the *size* of the difference in mortality between women and men, and between farmers 
and non-farmers, so the estimates are printed (rounded to three decimal places).

```{r coxtwofaccoef, comment = NA}
round(summary(fit)$coefficients, 3)
```

This result tells us that female mortality is 79.3 per cent of the male mortality, and that the farmer
mortality is 88 per cent of the non-farmer mortality (from the column "exp(coef)"). Furthermore, this model 
is *multiplicative* (additive on the log scale), so we can conclude that the mortality of a female farmer is 
$0.793 \times 0.880 \times 100$ = `r round(0.793 * 0.880 * 100, 1)` per cent of that of a male non-farmer.

We can also illustrate the sex difference graphically, see Figure \@ref(fig:coxtwofaccoefgr).

```{r coxtwofaccoefgr, fig.cap = "Cumulative hazards for men and women."}
fit2 <- coxph(Surv(enter, exit, event) ~ strata(sex) + farmer, 
              data = om)
plot(survfit(fit2), fun = "cumhaz", xlim = c(60, 85), 
     lty = 1:2, xlab = "Age")
abline(h = 0)
legend("topleft", legend = c("Men", "Women"), lty = 1:2)
```

Obviously the proportionality assumption is well met: It can also be formally tested as follows:

```{r testprop}
fit3 <- coxph(Surv(enter, exit, event) ~ sex + farmer, 
              data = om)
(pczp <- cox.zph(fit3))
```

First look at the $p$-value corresponding to *GLOBAL*: it gives an overall verdict of the proportionality
assumption. Values below 0.05 (say) are suspicious: It tells us that some variable is bad, and it is time 
to look at the individual $p$-values. There are no problems here.

As always, you should interpret these tests with care. With large data sets, you will frequently get small $p$-values, and still the graph shows a reasonable agreement with proportionality. Then do not bother about the proportionality.

An obvious question to ask: Is the sex difference in mortality the same among farmers as among non-farmers? In other words: Is there an *interaction* between sex and farmer/non-farmer regarding the effect on mortality?

In order to test for an interaction in **R**, the plus ($+$) sign in the formula is changed to a multiplication sign ($*$):

```{r multsign}
fit4 <- coxph(Surv(enter, exit, event) ~ sex * farmer, 
              data = om)
(x <- drop1(fit4, test = "Chisq"))
```

The small $p$-value (`r round(x[[4]][2], 4) * 100` per cent) indicates that there is a *statistically* significant (at the 5 per cent level) interaction effect. The *size* of the interaction effect is seen in the table of estimated coefficients (rounded to three decimal places):

```{r tablecoef}
round(summary(fit4)$coefficients[, 1:2], 3)
```

Interpretation: The mortality of female farmers is  $100 \times 0.743 = 74.3$ per cent of the mortality of male farmers. Hovever, moving to the non-farmer group, this relation has to be multiplied by $1.338$, the interaction effect. The result is that among non-farmers, the female mortality is `r round(74.3 * 1.338, 1)` per cent of the male mortality.

In cases like this, it is often recommended to perform separate analyses for women and men, or, alternatively, for farmers and non-farmers. Do that as an exercise and interpret your results.


### One factor and one continuous covariate

Now the covariates `sex` (factor) and `birthdate` (continuous) are in focus. First the model without interaction is fitted.

```{r facconmul}
fit5 <- coxph(Surv(enter, exit, event) ~ sex + birthdate, data = om)
drop1(fit5, test = "Chisq")
```

Obviously `sex` is statistically significant, while `imr.birth` is not. We go directly to the estimated coefficients after adding an interaction term to the model:

```{r facconmulint}
fit6 <- coxph(Surv(enter, exit, event) ~ sex * birthdate, data = om)
round(summary(fit6)$coefficients[, 1:2], 5)
```

What is going on here? The main effect of `sex` is huge! The answer lies in the interpretation of the coefficients: 
(i) To evaluate the effect on mortality for an 
individual with given values of the covariates, you must involve the interaction coefficient. For instance, a female born on January 2, 1801 (`birthdate` = `r round(toTime("1801-01-02"), 3)`)
will have the relative risk 

$$\exp(9.12285 - 0.00327 \times 1801.003 - 0.00519 \times 1801.003) = 0.00185$$

compared to a man with `birthdate`= 0, the reference values! Obviously, it is ridiculous to extrapolate the model 1800 years back in time, but this is nevertheless the correct way to interpret the coefficients. So, at `birthdate` = 0, women's mortality is 9162 times higher than that of men!
(ii) With interactions involved, it is strongly recommended to *center* continuous covariates. In our example, we could do that by subtracting 1810 (say) from `birthdate`. That makes `sex = "male", birthdate = 1800` the new *reference values*. The result is

```{r centering, echo = FALSE}
om$birthdate <- om$birthdate - 1810
fit7 <- coxph(Surv(enter, exit, event) ~ sex * birthdate, data = om)
(res <- round(summary(fit7)$coefficients[, 1:2], 5))
```

```{r echo=FALSE}
int0 <- 0
int1 <- res[1, 1]
sl0 <- res[2, 1]
sl1 <- sl0 + res[3, 1]
```

This is more reasonable-looking: At the beginning of the year 1810, woman had a mortality of 76.8 per cent of that of men.

To summarize: The effects can be illustrated by two curves, one for men and one for women. The coefficients for `sex`, 0 and `r int1`, are the respective intercepts, and `r sl0` and 
`r sl1` are the slopes. See Figure \@ref(fig:figgraphill3) for a graphical illustration.

```{r figgraphill3, fig.cap="The effect of birthdate on relative mortality for men and women. Reference: Men, birthdate = 1810.", echo=FALSE, fig.height = 4}
x <- seq(min(om$birthdate), max(om$birthdate), length = 1000)
y0 <- exp(int0 + x * sl0)
y1 <- exp(int1 + x * sl1)
plot(x, y0, type = "l", ylim = c(min(y0, y1), max(y0, y1)), lty = 4, xlab = "Birthdate.", ylab = "Relative risk", axes = FALSE)
att <- c(-30, -20, -10, 0, 10)
axis(1, at = att, labels = att + 1810)
axis(2, las = 1)
box()
lines(x, y1, lty = 2)
points(0, 1)
abline(h = 1)
abline(v = 0)
text(-10, 1.06, "Men")
text(-10, 0.89, "Women")
```


### Two continuous covariates

In our example data, the covariates `birthdate` and `imr.birth` are continuous, and we get

```{r botcont}
fit8 <- coxph(Surv(enter, exit, event) ~ birthdate * imr.birth, data = om)
(res <- round(summary(fit8)$coefficients[, 1:2], 5))
```

The interpretation of the coefficients are: The reference point is `birthdate = 0` (1810) and `imr.birth = 0`, and if `birthdate = x` and `imr.birth = y`, then the relative risk is  
$$ \exp(-0.03024 x + 0.02004 y + 0.00158 x y),$$
analogous to the calculation in the case with one continuous and one factor covariate.

## Interpretation of parameter estimates

In the proportional hazards model the parameter estimates are logarithms of
risk ratios relative to the baseline hazard. The precise interpretations of
the coefficients for the two types of covariates are discussed. 
The conclusion is that $e^\beta$ has a more direct and intuitive
interpretation than $\beta$ itself.

### Continuous covariate 

If $x$ is a continuous covariate, and
$h(t; x) = h_0(t)e^{\beta x}$,
then

\begin{equation*}
\frac{h(t; x+1)}{h(t; x)} = \frac{h_0(t)e^{\beta (x+1)}}
{h_0(t)e^{\beta x}} = e^\beta.
\end{equation*}
so the risk increases with a factor $e^\beta$, when $x$ is
increased by one unit. In other words,
$e^\beta$ is a *relative risk*\index{relative risk} (or a *hazard
  ratio*\index{hazard ratio}, which often is a preferred term in certain professions).

### Factor

For a factor covariate, in the usual coding with a reference category,
$e^\beta$ is the relative risk compared to that reference
category.

## Proportional hazards in discrete time
\index{proportional hazards!discrete time}

In discrete time, the hazard function is, as we saw earlier, a set of
conditional 
probabilities\index{conditional probability}, and so its range is restricted
to the interval $(0, 1)$. Therefore, the definition of proportional hazards
used for continuous time is unpractical; the multiplication of a
probability by a constant may result in a quantity larger than one.

One way of introducing proportional hazards in discrete time is to regard
the discreteness as a result of grouping true continuous time data, for
which the proportional hazards assumption hold. For
instance, in a follow-up study of human mortality, we may only have data
recorded once a year, and so life length can only be measured in
years. Thus, we assume that there is a true exact life length $T$, but we
can only observe that it falls in an interval $(t_i, t_{i+1})$.

Assume *continuous* proportional hazards, and a *partition* of
time: 

$$
0 = t_0 < t_1 < t_2 < \cdots < t_k = \infty.
$$ 
Then
\begin{multline}\label{eq:dischaz}
P(t_{j-1} \le T < t_j \mid T \ge t_{j-1}; \; \mathbf{x}) =
\frac{S(t_{j-1}\mid \mathbf{x}) - S(t_j\mid \mathbf{x})}{S(t_{j-1} \mid \mathbf{x})} \\
 =  1 - \frac{S(t_j \mid \mathbf{x})}{S(t_{j-1} \mid \mathbf{x})}
 =  1 -
\left(\frac{S_0(t_j)}{S_0(t_{j-1})}\right)^{\exp(\boldsymbol{\beta}\mathbf{x})} \\
= 1 - (1 - h_i)^{\exp(\boldsymbol{\beta}\mathbf{x})}
\end{multline}
with $h_i = P(t_{j-1} \le T < t_j \mid T \ge t_{j-1}; \; \mathbf{x} =
\mathbf{0})$, $j = 1, \ldots, k$. We take \eqref{eq:dischaz} as the
\emph{definition} of proportional hazards in discrete time.

### Logistic regression

It turns out that a proportional hazards model in discrete time, according
to definition \eqref{eq:dischaz}, is nothing else than a *logistic
  regression*\index{logistic regression}
model with the *cloglog link*\index{cloglog link function} (cloglog is
short for "complementary log-log" or $\boldsymbol{\beta}\mathbf{x} =\log(-\log(p))$). In order
to see that, let 

\begin{equation}
  (1 - h_j) = \exp(-\exp(\alpha_j)), \; j = 1, \ldots, k
\end{equation}
and

\begin{equation*}
X_j = \left\{ \begin{array}{ll}
          1, & t_{j-1} \le T < t_j \\
          0, & \text{otherwise}%t_1 \le T < t_2 \\
         \end{array} \right., \quad j = 1, \ldots, k
\end{equation*}
Then 

\begin{equation*}
  \begin{split}
  P(X_1 = 1; \; \mathbf{x}) &=  1 - \exp(-\exp\big(\alpha_1 +
  \boldsymbol{\beta}\mathbf{x})\big)  \\
  P(X_j = 1 \mid X_1 = \cdots = X_{j-1} = 0; \; \mathbf{x}) &=  1 - \exp(-\exp\big(\alpha_j
+ \boldsymbol{\beta}\mathbf{x})\big), \\
\quad j = 2, \ldots, k.
\end{split}
\end{equation*}
This is logistic regression with a cloglog link. Note that extra parameters
$\alpha_1, \ldots, \alpha_k$ are introduced, one for each potential event
age. They correspond to the baseline hazard function in continuous time,
but are be estimated simultaneously with the regression parameters.

## Model selection

In regression modeling, there is often several competing models for
describing data. In general, there are no strict rules for ``correct
selection''. However, for *nested* models\index{nested model}, there
are some formal guidelines. For a precise definition of this concept, see
Appendix A. 


### Model selection in general

Some general advise regarding model selection is given here. 

*    Remember, there are no \emph{true} models, only some *useful*
  ones. This statement is attributed to G.E.P.\ Box.
*    More than one model may be useful.
*    Keep \emph{important} covariates in the model.
*    Avoid automatic stepwise procedures!
*    If interaction effects are present, the corresponding main
effects *must* be there.


## Doing it in **R**
\index{Functions!coxreg|(}

We utilize the male mortality data, SkellefteÃ¥ 1800--1820, to
illustrate the aspects of an ordinary Cox regression. Males aged 20 (exact)
are sampled between 1820 and 1840 and followed to death or reaching the age
of 40, when they are right censored. So in effect male mortality in the
ages 20--40 are studied. The survival time starts counting at zero for each
individual at his 21st birthdate (that is, at exact age 20 years).

There are two **R** functions that are handy for a quick look at a data
frame, `str`\index{str} and `head`\index{head}. The first give a
summary description of the *structure* of an **R** object, often a
*data frame* \index{data frame}

```{r str,echo=TRUE}
library(eha) #Loads also the data frame 'mort'
str(mort)
```

```{r getd,echo=FALSE}
n.obj <- NROW(mort)
```
First, there is information about the data frame: It *is* a 
`data.frame`, with six variables measured on `r n.obj` objects. Then
each 
variable is individually described: name, type, and a few of the first values.
The values are usually rounded to a few digits. The *Factor* line is
worth noticing: It is of course a `factor` covariate, it takes two
levels, `lower` and `upper`. Internally, the levels are coded 1, 2,
respectively. Then (by default), `lower` (with internal code equal to
1) is the `reference category`. If desired, this can be changed with the
`relevel` function.

```{r relevel}
mort$ses2 <- relevel(mort$ses, ref = "upper")
str(mort)
```
Comparing the information about the variables `ses` and `ses2`, you
find that the codings, and also the reference categories, are
reversed. Otherwise the variables are identical, and any analysis involving
any of
them as an explanatory variable would lead to identical *conclusions*,
but not identical parameter estimates, see below.

The function `head` prints the first few lines (observations) of a data
frame  (there is also a corresponding `tail` function that prints a few
of the *last* rows). 

```{r str3}
head(mort)
```
Apparently, the variables `ses` and `ses2` are identical, but we know
that behind the scenes they are formally different; different coding and
(therefore) different reference category. This shows up in analyzes
involving the two variables, one at a time.

First the analysis is run with `ses`.

```{r mortreg}
res <- coxreg(Surv(enter, exit, event) ~ ses + birthdate, 
              data = mort)
res
```
Then the variable `ses2` is inserted instead.

```{r mortreg2}
res2 <- coxreg(Surv(enter, exit, event) ~ ses2 + birthdate, 
               data = mort)
res2
```
Notice the difference; it is only formal. The substantial results are
completely equivalent.

### Likelihood Ratio Test

If we judge the result by the Wald $p$-values, it seems as if both
variables are highly significant. To be sure it is advisable to apply the
likelihood ratio test. In **R**, this is easily achieved by applying the 
  `drop1` function on the result, `res`,  of the Cox regression.

```{r drop}
drop1(res, test = "Chisq")
```
In fact, we have performed *two* likelihood ratio tests here. First,
the effect of removing `ses` from the full model. The $p$-value for
this test is very small, $p = 0.00005998$. The scientific notation,
5.998e-05 means that the decimal point should be moved five steps to the
left (that's the minus sign). The reason for this notation is essentially
space-saving. Then, the second LR test concerns the absence or presence of
`birthdate` in the full model. This also gives a small $p$-value, $p =
0.006972$. 

The earlier conclusion is not altered; both variables are highly significant.

### The estimated baseline cumulative hazard function
\index{cumulative hazard function|(} 

The estimated baseline cumulative function (see Equation \@ref(eq:bashaz)) is best
plotted by the `plot` command, see Figure \@ref(fig:basl).

```{r basl,fig=TRUE,echo=FALSE, fig.cap="Estimated cumulative baseline hazard function."}
plot(res)
```

The figure shows the baseline cumulative hazard function for an
*average* individual (i.e., with the value zero on all covariates when
all covariates are *centered* around their weighted means. 
\index{cumulative hazard function|)} 
