# Register-Based Survival Data Models
\index{register data}
```{r settings7, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(comment = "", message = FALSE, echo = TRUE, cache = FALSE)
options(width = 70, digits = 7)
```

During the last couple of decades, huge data bases with poulation data have 
popped up around the world. They are great sources of information in demographic 
and statistical research, but their sizes creates challenges for the statistical 
tools we traditionally use in the demographic analyses.

We distinguish between data sources with individual content and sources with 
tabular data. In both cases, however, we will end up analyzing tables, so we 
start by describing methods for analyzing tables.

Finally, in this chapter, we show how to combine the technique with 
communal covariates\index{communal covariates} 
with tabulating data. The problem we solve is that the creation of an individual-based
data set with communal covariates usually leads to uncomfortably large data sets.
Tabulation is often a good solution.  

## Tabular data
\index{tabular data}
National statistical agencies produce statistics describing the population year 
by year, in terms of population size by age and sex, number of births by mother's
age and sex of the newborn, number of deaths by age and sex, and so on. In the
package **eha**, there are two data sets taken from [Statistics Sweden](https://scb.se),
`swepop` and `swedeaths`.

The content of `swepop` is shown in Table \@ref(tab:swepop7) (first five rows out
of 10504 rows in total).

\tiny
```{r swepop7, echo = FALSE, message = FALSE}
library(eha)
library(kableExtra)
sw <- swepop
sw$id <- NULL
rownames(sw) <- 1:NROW(sw)
kbl(head(sw, 5), booktabs = TRUE, caption = "First rows of 'swepop'.",
                label = "swepop7")
```
\normalsize
It contains the average Swedish population size by year, age, and sex, where age 
is grouped into one-year classes, but the highest age class, labelled 100, is in fact 
"ages 100 and above". The original table gives the population sizes at the end of each year,
but here we have calculated an average size over the year in question by taking the mean
of the actual value and the corresponding value from the previous year. The population size 
can in this way be used as "total time at risk" or *exposure*. 

The content of `swedeaths` (first five rows) is shown in Table \@ref(tab:swedeaths7).

```{r swedeaths7, echo = FALSE}
deaths <- swedeaths
deaths$id <- NULL
rownames(deaths) <- 1:NROW(deaths)
kbl(head(deaths, 5), booktabs = TRUE, caption ="First rows of 'swedeaths'.")
```

The `(age, sex, year)` columns in the two data frames are identical, so we can 
create a suitable data frame for analysis simply by putting `swedeaths$deaths`
into the data frame `swepop` and rename it to `sw`. The first rows of `sw` are
(Table \@ref(tab:sw7)).

```{r sw7, echo = FALSE}
sw$deaths <- deaths$deaths
kbl(head(sw, 5), booktabs = TRUE, caption = "First rows of combined data frame.")
```

The response variable in our analysis is two-dimensional: `(deaths, pop)`, 
which is on an "occurrence/exposure" form, suitable for an analysis with the 
function `tpchreg`.\index{Functions!\fun{tpchreg}} The result is a *proportional hazards* analysis with 
the assumption of a piecewise constant baseline hazard function:

```{r tpchtab7}
system.time(fit <- tpchreg(oe(deaths, pop) ~ sex + I(year - 1995), 
               time = age, last = 101, data = sw))
sf <- summary(fit)
sf
```

Note several things here:

1.   The fitting procedure is wrapped in a call to the function `system.time`. 
This results in the first report headed "user  system  elapsed", gives the time
in seconds for the whole operation ("elapsed"), split up by "user" (the time spent
with our call) and "system" (time spent by the operating system, for instance 
reading, writing and organizing the user operation). The interesting value for us 
is that the total elapsed time was around six tenth of a second.


2.   The response is `oe(deaths, pop)` where `oe` \index{Functions!\fun{oe}} 
is short for "occurrence/exposure".

3.   All covariates have a *reference value*. For factors with the default coding,
the reference category is given, but for continuous covariates we need to specify
it, if we are not satisfied with the default value *zero*. In the case with the 
covariate `year`, we are certainly *not* satisfied with zero as reference point, 
so we choose a suitable value in the range of the observed values. This is done in
practice by *subtracting* the reference value from the corresponding variable, 
forming `year - 1995` in our case. In the output above, this will only affect the value
of *Restricted mean survival*, which is calculated for a "reference individual",
in this case *a woman living her life during the year 1995*. There are not many such 
women, of course, what we are doing here is comparing periods, not cohorts.

4.   Note the argument `last = 101`. Since we are fitting a survival distribution
that is unbounded in time, we should specify the range for our data. The lower 
limit is given by `min(age)` (the argument `time`), but there is no natural upper limit.
The choice of the value for `last` do not affect parameter estimates and general 
conclusions, it only affects plots in the extreme of the right tail (slightly). 

5. Note the "Restricted mean survival": It gives the expected remaining life 
*at birth* for a *woman* in the *year 1995* (the reference values).

The graph of the baseline hazard function ("age-specific mortality") is shown in 
Figure \@ref(fig:base7).

```{r base7, fig.cap = "Baseline hazard function, women 1995. Model based.", echo = FALSE}
par(las = 1)
plot(fit, fn = "haz", main = "", xlab = "Age", ylab = "Mortality", col = "red")
abline(h = 0)
abline(v = sf$rmean, lty = 3)
text(78, 0.4, "Expected life at birth (81.1)", srt = 90, cex = 0.8)
```

Apart from the first year of life, mortality seems to be practically zero the first
forty years of life, and then exponentially increasing with age.

This is simple enough, but some questions remain unanswered. First, the model 
assumes proportional hazards, implying for instance that male mortality is around 
50 percent higher than female in all ages. It is fairly simple to organize a 
formal test of the proportionality assumption, but it is close to meaningless, 
because of the huge amount of data (almost five million deaths, "all" null
hypotheses will be rejected). A better 
alternative is to do it graphically by *stratifying* on `sex`.

```{r base27, fig.cap = "Baseline hazard function, women and men 1995.", echo = FALSE}
fit2 <- tpchreg(oe(deaths, pop) ~ strata(sex) + I(year - 1995), 
               time = age, last = 101, data = sw)
sf2 <- summary(fit2)
plot(fit2, fn = "haz", main = "", xlab = "Age", ylab = "Mortality", 
     col = c("red", "blue"))
abline(h = 0)
abline(v = sf2$rmean, lty = 3, col = c("red", "blue"))
```

Due to the extremely low mortality in early ages, Figure \@ref(fig:base27) is 
quite non-informative. We try the same plot on a log scale, see Figure \@ref(fig:base37).

```{r base37, fig.cap = "Baseline hazard function, women and men 1995. Log scale.", echo = FALSE}
plot(fit2, fn = "haz", main = "", xlab = "Age", ylab = "Log mortality", 
     col = c("red", "blue"), log = "y")
```

The female advantage is largest in early adulthood, after which it slowly decreases.
An even clearier picture is shown in Figure \@ref(fig:base47), where the ratio of 
male vs. female mortality is plotted.

```{r base47, fig.cap = "Baseline hazard functions, women and men 1995. Log scale.", echo = FALSE}
x <- seq(0, 100)
y <- fit2$hazards[2, ] / fit2$hazards[1, ]
plot(x, y, main = "Men vs. women", xlab = "Age", ylab = "Hazard ratio", 
     col = c("blue"), type = "l", ylim = c(1, 3))
abline(h = 1)
```

This graph gives the clearest picture of the relative differences in mortality 
over the life span. Here we see, somewhat unexpectedly, that the drop in the ratio 
is broken around  age 50, and the continues after age 70. This is not clearly 
visible in the earlier figures. A possible explanation is that the age interval
20-50 approximately coincides with women's fertility period, that is, during
this period, the mortality advantage decreases more than pure age motivates. 

This pattern may change over the time period 1969--2020. We split the 
data by year into quarters in order to check this.

```{r quarters7, fig.cap = "Baseline hazard functions, men and women by time period.", echo = FALSE}
oldpar <- par(mfrow = c(2, 2))
start <- 1969
for (i in 1:4){
    slut <- start + 12
    swx <- sw[sw$year %in% start:slut, ]
    fit2 <- tpchreg(oe(deaths, pop) ~ strata(sex) + I(year - start + 7), 
               time = age, last = 101, data = swx)
    y <- fit2$hazards[2, ] / fit2$hazards[1, ]
    plot(x, y, main = paste("Men vs. women", start, "-", slut), xlab = "Age", ylab = "Hazard ratio", 
         col = c("blue"), type = "l", ylim = c(1, 3))
    abline(h = 1)
    start <- slut + 1
}
par(oldpar)
```

The rise after age 50 seems to vanish with time, according to Figure \@ref(fig:quarters7), 
the period 2007--2019.

Finally, we investigate the development of expected life ($e_0$) over time, see Figure
\@ref(fig:e07).

```{r e07, fig.cap = "Expected life at birth by sex and year (period data).", echo = FALSE}
fem <- numeric(52)
mal <- numeric(52)
i <- 0
for (year in 1969:2020){
    i <- i + 1
    fit <- tpchreg(oe(deaths, pop) ~ strata(sex), 
                   data = sw[sw$year == year, ],
                   time = age, last = 101)
    x <- summary(fit)
    fem[i] <- x$rmean[1]
    mal[i] <- x$rmean[2]
}
att <- c(1969, 1981, 1994, 2007, 2020)
plot(1969:2020, fem, type = "b", col = "red", ylim = c(70, 85), 
     ylab = "Age", xlab = "Year", axes = FALSE)
axis(1, at = att)
axis(2, las = 1)
box()
lines(1969:2020, mal, type = "b", col = "blue", pch = "+")
text(1990, 82, "Women", col = "red")
text(1990, 73, "Men", col = "blue")
abline(h = c(75, 80, 85), lty = 3)
abline(v = att, lty = 3)
```

The difference in longevity between women and men is slowly decreasing over the 
time period. Also note the values in the last two years: Mortality in Sweden was
unusually low in the year 2019, resulting in high values of expected life, but in 
2020, the pandemic virus *covid-19*\index{covid-19} struck, and the expected life dropped by
around five months for women and almost nine months for men, compared to 2019.

## Individual data

The analysis of individual survival data is what the rest of this book is all 
about, so what is new here? The answer lies in the *size* of the data files, 
that is, the number of individuals is *huge*. 

As an example, we have mortality data consisting of all Swedish citizens aged
30 and above for the years 2001--2013. It may be described as yearly censuses, where 
all Swedish persons alive at December 31 the specific year, are registered together 
with vital information. It starts with register data of the form shown in 
Table \@ref(tab:vitalform7).

```{r vitalform7, echo = FALSE}
rl <- readRDS("rawlis.rds")
##rl$region <- rl$municipality
##rl$municipality <- NULL
rownames(rl) <- 1:4
kbl(rl, booktabs = TRUE, caption = "Register data, Sweden December 31, 2001.") %>%
     kable_styling(font_size = 10)   
```

The variables are

*   **id** A unique id number, which allows us to link information from different
sources (and years) to one individual.
*   **year** The present year, 2001, $\ldots$, 2013.
*   **sex**  Female or male.
*   **birthdate** A constructed variable. Due to integrity reasons, birth time information
is only given in the form of year and quarter of the year. So the *birthdate* is created
by year and the first quarter is represented by the decimal places ".125" (equal roughly
to February 15), the second quarter is ".375", third ".625", and finally the fourth 
quarter is represented by ".875"). The birthdate will be off by plus or minus one 
month and a half, which is deemed to be a good enough precision concerning adult
mortality.
*   **deathdate** Is given by day in the sources. The missing value symbol (*NA*) 
is given to those who are not registered as dead before January 1, 2014.
*   **civst, ses, region** Information about *civil status*, 
*socioeconomic status*, and *geographic area*, respectively.

From this yearly information, survival data is created by following each individual 
one year ahead, until the next "census". Age at start, `enter`, is calculated by 
subtracting `birthdate` from the date of presence, `year + 1`. For an individual 
with a deathdate less than or equal to `enter + 1 = year + 2`, the variable `exit` is 
calculated as `deathdate - birthdate` and the variable `event` is set equal to one, otherwise 
`exit = enter + 1` and `event = 0`, a censored observation.

The result of this is shown for the same individuals as above in Table \@ref(tab:workedform7).

```{r workedform7, echo = FALSE}
wl <- readRDS("worklis.rds")
##wl$region <- wl$municipality # Change name.
##wl$municipality <- NULL
rownames(wl) <- 1:4
kbl(wl[, c("id", "year", "enter", "exit", "event")], booktabs = TRUE,caption = "Constructed data, Sweden December 31, 2001.") %>%
     kable_styling(font_size = 10)   
```

This file from the year 2001 (covering the year 2002!) contains *5.8 million* individuals
and 92 thousand deaths. To perform a Cox regression on such massive data sets 
(and there are thirteen of them) is time, software, 
and hardware consuming, but there is a simple remedy: Make tables and fit piecewise 
constant proportional hazards models.

This is done with the aid of the `eha` function `toTpch` as follows:

```
lisa2001$cohort <- floor(lisa2001$birthdate) 
listab2001 <- toTpch(Surv(enter, exit, event) ~ sex + cohort + year + 
                                                civst + region + ses,
                     data = lisa2001, 
                     cuts = c(seq(30, 100, by = 5), 105))
```

```{r maketableread7, echo = FALSE, comment=""}
lisa2001 <- readRDS("mytables/listab2001.rds")
lisa2001$region <- lisa2001$municipality
lisa2001$municipality <- NULL
kableExtra::kbl(head(lisa2001, 4), booktabs = TRUE, digits = 4,
                caption = "Mortality in ages above 30, Sweden 2002 ('year' = 2001).", 
                label = "maketableread7") %>%
    kable_styling(font_size = 10)
```

Note the creation of the variable `cohort`: The function `floor` simply removes 
the decimals from a positive number. The result is shown in 
Table \@ref(tab:maketableread7) (four first rows) 
which is a table with  `r nrow(lisa2001)` rows. That is, one row for each unique 
combination of the six covariates, empty combinations excluded. Note the two created
variables, `event` which is the total number of events for each combination of covariates, 
and `exposure`, which is total time at risk for each corresponding combination.


In the survival analysis, the pair `(event, exposure)` is the *response*, and 
behind the scene, *Poisson regression* is performed with `event` as response and 
`log(exposure)` as *offset*.

Finally, thirteen similar tables are created (one for each year 2001--2013) and 
merged, using the **R** command `rbind`.



```{r sumres7, echo = FALSE}
##listab <- readRDS("~/Forskning/Blog/mytables/listab2001.rds")
##for (y in 2002:2013){
##    fnam <- paste("~/Forskning/Blog/mytables/listab", y, ".rds", sep = "")
##    xx <- readRDS(fnam)
##    listab <- rbind(listab, xx)
##}
listab <- readRDS("mytables/listab.rds")
listab$ses <- relevel(listab$ses, ref = "worker")
listab$civst <- relevel(listab$civst, ref = "married")
levels(listab$age)[15] <- "100-105"
listab$cohort <- NULL
```

A summary of the result gives

*   Number of records: `r NROW(listab)`.
*   Number of deaths: `r round(sum(listab$event) / 1000000, 2)` millions.
*   Total time at risk: `r round(sum(listab$exposure / 1000000), 2)` million years.

Now this data set is conveniently analyzed with the aid of the function `tpchreg` 
in `eha`. An example:

```{r runtab7, comment = "", echo = TRUE}
listab$year <- listab$year - 2000 # Enumerate (center)!
list2 <- aggregate(cbind(event, exposure) ~ sex + civst + ses + 
                       year + age, FUN = sum, data = listab)
fit <- tpchreg(oe(event, exposure) ~ strata(sex) + civst + ses + 
                   year, data = list2, time = age, last = 105)
xx <- summary(fit)
emen <- round(xx$rmean[1], 2)
ewomen <- round(xx$rmean[2], 2)
```

```{r runtable77, results = "asis", echo = FALSE}
if (knitr::is_latex_output()){
    fit.ltx <- tpchreg(oe(event, exposure) ~ strata(sex) + civst + ses + 
                   year, data = list2, time = age, last = 105)
    dr = drop1(fit.ltx, test = "Chisq")[-2, ] # A hack!!
    ltx(summary(fit.ltx), dr = dr, 
        caption = "Mortality in Sweden 2002-2014, ages 30 and above.", 
        label = "runtable7")
}
```

```{r runtable7, echo = FALSE}
if (knitr::is_html_output()) print(xx)
```

The result is shown in Table 7.7.

We note a couple of things here:

1.   The data frame `listab` contains `r NROW(listab)` records, and the execution time 
of a Poisson regression might take a long time. Since we did not use the covariates 
`region` (a factor with 25 levels) and `cohort` (92 levels), we could aggregate over 
these variables which resulted 
in the data frame `list2` with only `r NROW(list2)` records. This cuts down 
computing time by approximately 99 per cent, or to 0.17 seconds from 17 seconds. 
The results are not affected by this. Also note the similarity between 
the `formula` arguments of the two functions `aggregate` and `tpchreg`. 
The differences are that in `aggregate`, the function `cbind` is used instead 
of `oe`, and `strata()` is left out.

2.   All *p*-values are effectively zero, a consequence of of the huge amount of data.
It is also reasonable to claim that this constitutes a *total investigation*, and 
therefore *p*-values are meaningless.

3. The restricted mean survival 
*at age 30* for a *married worker* in the *year 2002* is `r emen` 
years for men and `r ewomen` years for women,
that is, the expected age of death is `r 30 + emen` years for 
men and `r 30 + ewomen` years for women alive at 30.

A graph of the conditional survival functions is shown in Figure \@ref(fig:wbhaz7).

```{r wbhaz7, fig.cap = "Survival above 30, males and females, married worker, Sweden 2002.", echo = FALSE, fig.height = 3.5}
par(las = 1)
plot(fit, fn = "sur", col = c("blue", "red"), lty = c(1, 2), xlab = "Age", 
     main = "", printLegend = "center")
axis(1, at = seq(30, 100, by = 10))
##axis(2, las = 1)
##box()
abline(h = 0)
```

## Communal covariates and tabulation

We got temperature data from
[SMHI](https://www.smhi.se/data/meteorologi/ladda-ner-meteorologiska-observationer#param=airtemperatureInstant,stations=all,stationid=140500) for central Umeå, daily measurements starting at 1 
November, 1858 and ending at 30 September, 1979 (this particular weather station was replaced at this last date).

Since we attempt to match these data to population data for the time period 1 January 1901 -- 31 December 1950, we
 cut
the temperature data to the same time period.


```{r gettemp}
library(eha)
temp <- read.table("~/Forskning/Data/ume_temp.csv", 
                   header = TRUE,
                   skip = 10, sep = ",")[, 1:3]
## We need only the first three columns
names(temp) <- c("date", "time", "temp")

temp$date <- as.Date(temp$date)
temp$year <- as.numeric(format(temp$date, "%Y"))
temp$month <- as.numeric(format(temp$date, "%m"))
temp <- temp[order(temp$date, temp$time), ]
temp$quarter <- quarters(temp$date)
temp <- temp[temp$year %in% 1901:1950, ]
##

temp$time <- as.factor(temp$time)
temp$quarter <- as.factor(temp$quarter)
```

```{r plottemp}
library(kableExtra)
source("~/Documents/EHAR2/R/tbl.R")
tbl(head(temp, 10), fs = 10) # fs stands for 'font size'
```

Obviously, there are three measurements each day, and the temperature range is
`r range(temp$temp)` degrees Celsius. 
The variables *year*, *month*, and *quarter* were extracted from the *date*. 

### Reading individual data

We extract population data for Umeå, 1 Jan 1901 to 31 Dec 1950, and include all 
ages between 15 and 100:

```{r popdata, echo = FALSE}
library(skum)
ume <- obs[obs$region == "ume", ]
ume$event <- (ume$sluttyp == 2)
ume <- cal.window(ume, c(1901, 1951))
ume <- age.window(ume, c(15, 101))
ume <- ume[, c("id", "sex", "birthdate", "enter", "exit", "event", "ortnmn")]
ume$urban <- ume$ortnmn == "UMEÅ"
ume$ortnmn <- NULL
noll <- ume$enter >= ume$exit
ume$enter[noll] <- ume$enter[noll] - 0.001 # Fix zero length spells
show <- head(ume, 10)
show$birthdate <- toDate(show$birthdate)
tbl(show, fs = 10)
```

There are `r NROW(ume)` records about `r length(unique(ume$id))` individuals in this selection.
In order to simplify the presentation, only two covariates, `sex` and `urban`, are included.

### First analysis

The first question is what to do with the temperature measurements. They span 
50 years, with mostly three measurements per day. In this first analysis we 
focus on the effect on mortlity of *low* temperatures, so we start by taking the 
maximum of daily temperatures, then minimum over month. We thus end
up with one measurement per month and year, in total 600 measurements.

Start by finding the maximum value each day:

```{r maxtemp}
idx <- with(temp, tapply(date, date))
maxtemp <- with(temp, tapply(temp, date, max))
temp$maxtemp <- maxtemp[idx]
## Reduce to one measurement per day:
mtemp <- temp[!duplicated(temp$date), 
              c("date", "maxtemp", "month", "quarter", "year")]
rownames(mtemp) <- 1:NROW(mtemp)
tbl(head(mtemp, 12), fs = 10)
```

The next step is to take minimum over year and month:

```{r aver}
atmp <- aggregate(list(temp = mtemp$maxtemp), 
                  by = mtemp[, c("month", "year")], 
                  FUN = min)
tbl(head(atmp, 12), fs = 10)
```

Now we can try to apply `make.communal` to our data, to split spells *by year and month*:

```{r makecom, cache = TRUE}
atmp$yearmonth <- atmp$year * 100 + atmp$month 
## A trick to get format 190101
comtime <- system.time(
    nt <- make.communal(ume, atmp["yearmonth"], 
                        start = 1901, period = 1/12))

oj <- nt$enter >= nt$exit - 0.0001 # Too short interval!
nt$enter[oj] <- nt$exit[oj] - 0.001  # Break accidental ties
saveRDS(nt, file = "mytables/nt.rds")
##nt <- readRDS("mytables/nt73.rds")
tbl(head(nt, 12), fs = 10)
```

Total time for *making communal*: 25 seconds.


The next step is to read temperature from `atmp` to `nt`:

```{r readtmp}
indx <- match(nt$yearmonth, atmp$yearmonth)
nt$temp <- atmp$temp[indx]
nt$year <- nt$yearmonth %/% 100 # Restore 'year'.
nt$month <- nt$yearmonth %% 100
tbl(head(nt, 10), fs = 10)
```

This data frame contains `r NROW(nt)` records, maybe too much for a comfortable 
Cox regression. Let's see what happens:

```{r coxph, cache = TRUE}
##library(survival)
ptm <- proc.time()
fit <- coxreg(Surv(enter, exit, event) ~ 
                  sex + temp, data = nt)
res <- summary(fit)
saveRDS(res, file = "mytables/res73.rds")
##res <- readRDS("mytables/res73.rds")
ctime <- proc.time() - ptm
tbl(regtable(res))
```

This is quite bad (the computing time,  almost 2 minutes), 
but we already have a substantial result: 
Survival chances *increase* with temperature. But remember that Umeå 
is a northern town, not much warm weather here. But it can also be the case that both
high and low temperatures are critical, which may motivate the categorization of 
temperature. Let us look at its distribution over the year.

```{r temphist, fig.cap = "Monthly minimum  temperatures, Umeå 1901-1950."}
hist(atmp$temp, xlab = "Temperature (centigrade)", main = "")
```

And the winter distribution:

```{r temphist1, fig.cap = "Monthly minimum  temperatures, Umeå 1901-1950. October-March."}
hist(atmp[atmp$month %in% c(1:3, 10:12), ]$temp, 
     xlab = "Temperature (centigrade)", main = "")
```

And in the summer:

```{r temphist2, fig.cap = "Monthly minimum  temperatures, Umeå 1901-1950. April-September."}
hist(atmp[atmp$month %in% c(4:9), ]$temp, 
     xlab = "Temperature (centigrade)", main = "")
```


Winter survival analysis results, with cut points at -20, -5 degrees Celsius:

```{r catetemp1, cache = TRUE}
nt$ctemp <- cut(nt$temp, breaks = c(-30, -20, -5, 5))
fit <- coxreg(Surv(enter, exit, event) ~ sex + ctemp, 
              data = nt[nt$month %in% c(1:3, 10:12), ])
s1fit73 <- summary(fit)
saveRDS(s1fit73, file = "mytables/s1fit73.rds")
##s1fit73 <- readRDS("mytables/s1fit73.rds")
tbl(regtable(s1fit73))
```
Obviously, mortality increases with decreasing temperature, but there is not much 
difference between the two coldest intervals: Clearly best is a temperature around the freezing point. 

Summer results with cut points 0, 10 degrees Celsius:

```{r catetemp2, cache = TRUE}
nt$ctemp <- cut(nt$temp, breaks = c(-20, 0, 10, 17))
fit <- coxreg(Surv(enter, exit, event) ~ sex + ctemp, 
              data = nt[nt$month %in% c(4:9), ])
s2fit73 <- summary(fit)
saveRDS(s2fit73, file = "mytables/s2fit73.rds")
##s1fit73 <- readRDS("mytables/s1fit73.rds")
tbl(regtable(s2fit73))
```

Also here is it clear that the months with the highest min temperature are the healthiest.

### Reducing the size of the data set

The analyses so far are very rudimentary, including all ages in one and so on. 
Here we show how extract small subsets of the full data set by some simple and 
reasonable assumptions. It builds on *categorical covariates* and an assumption
of *piecewise constant hazards*. We use the function `toTpch` in the `eha` package:

```{r tabulate}
told <- toTpch(Surv(enter, exit, event) ~ sex + urban + yearmonth, 
               data = nt, cuts = seq(15, 100, by = 5))
indx <- match(told$yearmonth, atmp$yearmonth)
told$temp <- atmp$temp[indx]
saveRDS(told, file = "mytables/told73.rds")
##told <- readRDS("mytables/told73.rds")
tbl(head(told, 6), fs = 10)
```

The tabulation reduces the data set from 2.5 million records to 19 thousands. And
computing time accordingly (reduced to a quarter of a second!), without jeopardizing the results. Let's see:

```{r}
fit <- tpchreg(oe(event, exposure) ~ sex + temp, 
               data = told, time = age)
tbl(regtable(summary(fit)))
```

Essentially the same conclusion as in the earlier attempts: Mortality decreases 
with increasing temperature.

### Conclusion

This was a very superficial analysis of the effect of temperature on mortality. 
The main purpose of the presentation was to show how to include *communal covariates*
(here: temperature) in survival analysis, and how to reduce huge data sets by tabulation.
Read more about it on [EHAR2](http://ehar.se/r/ehar2/), Chapters 6 and 7.

**Final note:** The timings above are of course heavily dependent on both hardware
and software. I used my laptop, a DELL M6800 with 32 Gb memory and running 
Ubuntu 20.10, R 4.0.4 in RStudio 1.4.1103.
