# Parametric Models

```{r settings8, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(comment = "", message = FALSE, echo = FALSE, cache = FALSE)
options(width = 70, digits = 7)
```

We have already been given an example of a parametric survival model, the 
*piecewise constant hazards model* in the previous chapter. 
Apart from that, we have so far studied nonparametric methods for survival
analysis. This is a tradition that has its roots in medical (cancer)
research. In technical applications, on the other hand, parametric models
dominate; of special interest is the *Weibull* model \citep{ww51}. The text
book by \cite{jl03} is a good source for the study of parametric survival
models. 
More technical detail about parametric distributions is found in Appendix B .

Three kinds of parametric models are considered: the
*proportional hazards*\index{model!parametric!PH} and the 
*accelerated failure  time*\index{model!parametric!AFT} in continuous time, 
and the *discrete time proportional hazards*\index{model!parametric!discrete} 
models.


## Proportional Hazards Models

A proportional hazards family of distributions is generated from one
specific continuous distribution by multiplying the hazard function of that
distribution by a strictly positive constant, and letting that constant
vary over the full positive real line. So, if $h_0$ is the hazard function
corresponding to the generating distribution, the family of distributions
can be described by saying that $h$ is a member of the family if

\begin{equation*}
  h_1(t) = c h_0(t) \quad \text{for some } c > 0, \; \text{and all } t > 0.
\end{equation*}
Note that it is possible to choose any hazard function (on the positive
real line) as the generating function. The resulting proportional hazards
class of distributions may or may not be a well recognized family of
distributions. 

In **R**, **eha** is one of the packages that can fit parametric proportional hazards
models. In the following subsections, the possibilities are examined.

The parametric distribution functions that naturally can be used as the baseline
distribution in the function `phreg` are the *Weibull*,
*Extreme value*, and the *Gompertz* distributions. 
The *Piecewise constant hazards* model is treated in the functions `pch`(individual data)
and `tpch` (tabular data). 

The *lognormal* and *loglogistic* distributions are 
also included as possible choices and allow for hazard functions that are 
first increasing to a maximum and then
decreasing, while the other distributions all have monotone hazard
functions. However, since these families are not closed under proportional 
hazards without artificially adding a third, "proportionality",  parameter,
they are not discussed here (regard these possibilities as experimental).
It is better to combine the lognormal and the 
loglogistic distributions with the accelerated failure time modeling, where they 
naturally fit in. 

See Figure \@ref(fig:6hazs) for Weibull and Gompertz hazard functions with selected
parameter values.

```{r 6hazs,fig.cap = "Selected hazard functions.", echo=FALSE,fig.height=5,message=FALSE}
library(eha)
oldpar <- par(mfrow = c(2, 2))
x <- seq(0.001, 10.001, length = 1000)
y <- hweibull(x, shape = 3, scale = 6)
plot(x, y, main = "Weibull, shape = 3", type = "l", 
     ylab = "", xlab = "Time", lwd = 2, ylim = c(0, 1.5), las = 1)
abline(h = 0, v = 0)
y <- hweibull(x, shape = 1/2, scale = 2)
plot(x, y, main = "Weibull, shape = 1/3", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 1.5), lwd = 2, las = 1)
abline(h = 0, v = 0)
y <- hgompertz(x, shape = 1/6, rate = 0.2, param = "rate")
plot(x, y, main = "Gompertz, rate = 2", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 1.5), lwd = 2, las = 1)
abline(h = 0, v = 0)
y <- hgompertz(x, shape = 1.5, rate = -0.2, param = "rate")
plot(x, y, main = "Gompertz, rate = -2", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 1.5), lwd = 2, las = 1)
abline(h = 0, v = 0)
par(oldpar)
```

We note in passing that the fourth case, the Gompertz model with negative rate parameter,
does not represent a true survival distribution, because the hazard function decreases
too fast: There will be a positive probability of eternal life.

Experience shows that the Gompertz distribution fits adult mortality very well, in the
ages 30 to 85, say. The modeling of mortality from birth to early adulthood, on the 
other hand, is demanding since the typical hazard function for all these ages is 
U-shaped with high infant mortality and relatively low child mortality. Since 
both the Weibull and the Gompertz distributions have a monotone hazard function,
neither is suitable to fit the mortality of the *full* human life span.  However, both 
distributions are suitable for fitting shorter pieces of the life span, and for longer spans 
there are two possibilities, a nonparametric model (Cox regression) and a 
piecewise constant hazard hazard model, where the former can be seen as a 
limiting case of the latter. More about that later. 

### The Weibull model

The family of *Weibull* distributions
may be defined by the family of hazard functions

\begin{equation}
h(t; p, \lambda) = \frac{p}{\lambda} \biggl(\frac{t}{\lambda}\biggr)^{p-1},
\quad t, p, \lambda > 0.
(\#eq:6weibull)
\end{equation}

If we start with

$$
h_1(t) = t^{p-1}, \quad p \ge 0, \; t > 0
$$

for a *fixed* $p$, and generate a proportional hazards family from there,

\begin{equation*}
h_c = c h_1(t), \quad c, t > 0,
\end{equation*}

we get

\begin{equation}\label{eq:6weibdist}
h_c = c t^{p-1} = \frac{p}{\lambda} \biggl(\frac{t}{\lambda}\biggr)^{p-1}
\end{equation}

by setting
$$
c = \frac{p}{\lambda^p},
$$

which shows that for each fixed $p > 0$, a proportional hazards family is
generated by varying $\lambda$ in \eqref{eq:6weibdist}. On the other hand,
if we pick two members from the family \eqref{eq:6weibdist} with
*different* values of $p$, they would not be proportional. 

To summarize, the *Weibull* family of distributions is not *one* family
of proportional hazards distributions, but a *collection* of families
of proportional hazards. The collection is indexed by $p > 0$. It is true,
though, that all the families are closed under the *Weibull* distribution. 

The proportional hazards regression model with a *Weibull* baseline
distribution as obtained by multiplying \@ref(eq:6weibull) by $\exp(\beta x)$:

\begin{equation}
  h(t; x, \lambda, p, \beta) = \frac{p}{\lambda}
  \biggl(\frac{t}{\lambda}\biggr)^{p-1} e^{\beta x}, \quad t > 0.
(\#eq:6weibreg)
\end{equation}

The function **phreg** in package **eha** fits models like
\@ref(eq:6weibreg) by default.

### The Gompertz distribution

The Gompertz families of distributions are defined in essentially two ways in the 
**R** package `eha`: The *rate* and the *canonical* representations. The reason 
for this is that the families need to be differently represented depending on 
whether proportional hazards or accelerated failure time models are under consideration.

In the *proportional hazards* case, the *rate* formulation is used, and it is
characterized by an exponentially increasing hazard function with fixed rate `r`:

\begin{equation}
h(t; p, r) = p e^{r t}, \quad p, t > 0; -\infty < r < \infty.
\end{equation}

As noted earlier, when $r < 0$, the hazard function $h$ is decreasing "too fast" 
to define a proper survival function, and $r = 0$ gives the
*exponential distribution* as a special case. And for each fixed $r$, the family 
of distributions indexed by $p > 0$ constitutes a proportional hazards family of 
distributions, and the corresponding regression model is written as

\begin{equation}
h(t; x, p, r, \beta) = p e^{r t} e^{\beta x}, \quad t > 0.
\end{equation}

### Application


```{example name = "Old age mortality."}
```
The data set **oldmort** in the **R** package **eha** contains
  life histories of people followed from their 60th birthday to their 100th, or 
  until death, born between June 28, 1765 and December 31, 1820
  in Skellefteå. The data set is described in detail in Chapter 1. The variable
  `enter` is age at start of the given interval, and `exit` contains the age at 
  the end of the interval. We need to calculate *follow-up time* since age 60, so a new 
  data frame, `olm`, is created as a copy of `oldmort`, and then 60 is subtracted from 
  `enter` and `exit`. 
  See the result in
  Table \@ref(tab:olddata8), where the most relevant variables for our purpose are shown.
  
```{r olddata8, echo = FALSE}
library(eha)
library(kableExtra)
olm <- oldmort[, c("birthdate", "sex", "region", "enter", "exit", "event")]
olm$age <- olm$enter # Age at start of follow-up
olm$enter <- olm$enter - 60 # Duration since 60th birthday.
olm$exit <- olm$exit - 60 # Ditto
olm$birthdate <- toDate(olm$birthdate)
source("R/tbl.R")
tbl(head(olm, 5), caption = "The data set 'olm', first rows.")
##kbl(head(olm, 5), booktabs = TRUE, row.names = FALSE, 
  ##  caption = "The data set 'olm', first rows.", label = "olddata8") %>%
    ##    kable_styling(font_size = 11, full_width = FALSE)  
```

The variable names are more or less self explanatory,  *enter*  and *exit* are time 
in years since the sixtieth birthday, *age* is age at start of follow-up 
(the original `enter` variable). The variable *event* is 
an indicator of death at the duration given by `exit`.

To have something to compare to, a Cox regression is performed first, see Table
\@ref(tab:coxold8). Two covariates are included in the model, `sex` and `region`.
Both are *categorical*, `region` with three categories, *town* (reference), *industry*, and *rural*,
and `sex` with *male* as reference category.

```{r coxold8, echo = FALSE, results="asis"}
fit.c <- coxreg(Surv(enter, exit, event) ~ sex + region, data = olm)
dr.c <- drop1(fit.c, test = "Chisq")
xx <- summary(fit.c)
cap <- "Old age mortality, Cox proportional hazards model."
if (knitr::is_latex_output()){
    ltx(fit.c, dr = dr.c, caption = cap, label = "tab:coxold8")
}else{
    ##knitr::kable(round(xx$coefficients, 4), booktabs = TRUE, caption = cap)
    kbl(round(xx$coefficients, 4), booktabs = TRUE, caption = cap) %>%
    kable_styling(font_size = 11, full_width = FALSE)
}
```

Then a Weibull model is fitted, see Table \@ref(tab:weibold8).

```{r preweibold8, echo = TRUE}
fit <- phreg(Surv(enter, exit, event) ~ sex + region, 
             dist = "weibull", data = olm)
xx <- summary(fit)
```

```{r weibold8, results = "asis", echo = FALSE}
dr <- drop1(fit, test = "Chisq")
cap <- "Old age mortality, Weibull proportional hazards model."
if (knitr::is_latex_output()){
    ltx(fit, dr = dr, 
        caption = cap, 
        label = "tab:weibold8")
}else{
    kbl(round(xx$coefficients, 4), booktabs = TRUE, caption = cap) %>%
    kable_styling(font_size = 11, full_width = FALSE)

    ##knitr::kable(round(xx$coefficients, 4), caption = cap, booktabs = TRUE)
}
```

A closer look at the estimates of regression coefficients shows that they are not very
close, in Table \@ref(tab:sbys8) they are put side by side for easier comparison.

```{r sbys8, echo = FALSE, results='asis'}
n.co <- length(fit.c$coefficients)
out <- cbind(fit.c$coefficients, fit$coefficients[1:n.co])
colnames(out) <- c("Cox", "Weibull")
out <- round(out, 4)
kbl(out, booktabs = TRUE,
             caption = "Coefficients with Cox and Weibull regressions, data oldmort.") %>%
  kable_styling(full_width = FALSE)
```

Let us compare the estimated cumulative baseline hazard functions, see Figure \@ref(fig:cwcolm8).

```{r cwcolm8, fig.cap = "Baseline cumulative hazards for Cox and Weibull regressions."}
##check.dist(fit.c, fit)
par(las = 1)
haz <- hazards(fit, cum = TRUE)
plot(fit.c, xlab = "Years above age 60.")
lines(haz$x, haz$y, lty = 2)
legend("topleft", legend = c("Cox regression", "Weibull regression"), lty = 1:2)
```

This is not a good fit, it seems as if the Weibull hazard cannot grow fast enough.
A better approach is to fit a *Gompertz* distribution, and check parameter 
and baseline hazards estimates, see Figure \@ref(fig:gowecof8) and 
Table \@ref(tab:gowecot8).

```{r gowecof8, fig.cap = "Baseline cumulative hazards for Cox and Gompertz regressions."}
fit.g <- phreg(Surv(enter, exit, event) ~ sex + region, 
             dist = "gompertz", param = "rate", data = olm)
plot(fit.c, xlab = "Years above age 60.")
haz.g <- hazards(fit.g, cum = TRUE)
plot(fit.c, xlab = "Years above age 60.")
lines(haz.g$x, haz.g$y, lty = 2)
legend("topleft", legend = c("Cox regression", "Gompertz regression"), lty = 1:2)
```

The Gompertz model fits the baseline hazard very well up until duration 30 (age 90),
but after that the exponential growth slows down. The early growth rate is 
`r round(fit.g$coefficients[n.co + 1], 3) * 100` per cent per year.

```{r gowecot8}
out <- cbind(fit.g$coefficients[1:n.co], out)
colnames(out) <- c("Gompertz", "Cox", "Weibull")
out <- round(out, 3)
source("R/tbl.R")
tbl(out, caption = "Coefficients with Gompertz, Cox and Weibull regressions, time scale duration since age 60.")
##kableExtra::kbl(out, booktabs = TRUE, 
  ##           caption = "Coefficients with Gompertz, Cox and Weibull regressions, time scale duration since age 60.")
```

The Gompertz and Cox models are very close, both regarding regression parameter estimates and 
baseline hazard functions.


### The parametric model with left truncation

The data set `oldmort` contains left-truncated life histories as a consequence of
using *age* as time scale. In the presentation above we chose to change the time scale 
so that the origin was age 60 (sharp). This is of no importance when fitting the
semi-parametric Cox regression, an additive change of time scale will only shift
the estimated cumulative hazards along the x-axis.

But for parametric models it matters, and we illustrate it by repeating the 
previous Weibull, Gompertz,  and Cox regression analyses with the original time scale. 
The consequence is that focus is shifted from a conditional (on survival until 60) analysis
to an unconditional, where the baseline hazard and regression coefficients are estimated for
the *full life span* (0--100 years of age).

```{r origts8}
fit.g <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort, dist = "gompertz")
fit.w <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort)
fit.c <- coxreg(Surv(enter, exit, event) ~ sex + region, data = oldmort)
yy <- round(cbind(fit.g$coefficients[1:n.co], fit.c$coefficients, fit.w$coefficients[1:n.co]), 3)
colnames(yy) <- c("Gompertz", "Cox", "Weibull")
##knitr::kable(yy, booktabs = TRUE, caption = "Coefficients with Gompertz, Cox and Weibull regressions, time scale age.")
source("R/tbl_html.R")
tbl_html(yy, caption = "Coefficients with Gompertz, Cox and Weibull regressions, time scale age." )
```

Compare these fitted coefficients with the earlier from Table \@ref(tab:gowecot8). For the Gompertz and Cox regression models, coefficients are identical, while they differ for the Weibull distribution.

Let us look at the baseline cumulative hazards, see Figure \@ref(fig:origtsfig8), where 
the graph is cut at ages 60 and 80 for clarity. Notice the value at Time = 60: The parametric models are
"extrapolating" back to time at birth, and so the estimates do not represent the cumulative hazards of the conditional
distribution, given survival to age 60. This has an impact on the estimates of the regression coefficients
in the case of the Weibull distribution, because the *conditional* is *not* Weibull even though the unconditional is.
This phenomenon does *not* apply to the Gompertz distribution, for which the conditional distribution is again Gompertz
with the same rate, but with a different level. 

```{r origtsfig8, fig.cap = "Gompertz vs Cox and Weibull vs Cox estimated cumulative hazards functions."}
op = par(mfrow = c(1, 2), las = 1)
plotHaz(fit.g, fit.c, interval = c(60, 80), ylim = c(0, 1.6))
plotHaz(fit.w, fit.c, interval = c(60, 80), ylim = c(0, 1.6))
par(op)
```

From Figure \@ref(fig:origtsfig8) it appears as if we can get the *conditional* 
cumulative hazards simply by subtracting the value at age 60, $H(60)$, from the 
whole curves, and that is in fact correct. In the Gompertz case, it would simply
recover Figure \@ref(fig:gowecof8), but the Weibull case is different: Starting 
with Figure \@ref(fig:cwcolm8) and adding the adjusted Weibull curve from 
Figure \@ref(fig:origtsfig8) we get Figure \@ref(fig:adjweib8).

```{r adjweib8, fig.cap = "Comparison of the conditional and unconditional Weibull models."}
fit0 <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort)
fit60 <- phreg(Surv(enter, exit, event) ~ sex + region, data = olm)
haz0 <- hazards(fit0)
haz60 <- hazards(fit60)
plot(fit.c, ylim = c(0, 8), xlab = "Age")
lines(haz0$x, haz0$y - haz0$y[1], lty = 2, col = 2, lwd = 1.5)
lines(haz60$x + 60, haz60$y, lty = 4, col = 4, lwd = 1.5)
legend("topleft", legend = c("Non-parametric", "Conditional Weibull", "Weibull"), 
       lty = c(1, 2, 4), col = c(1, 2, 4))
```

Obviously, the conditional Weibull distribution fits data much better than the 
unconditional one. The comparison with the Gompertz distribution is shown in 
Figure \@ref(fig:adjwg8).

```{r adjwg8, fig.cap = "Comparison of the Gompertz and conditional Weibull models."}
##fit0 <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort)
##fit60 <- phreg(Surv(enter, exit, event) ~ sex + region, data = olm)
##haz0 <- hazards(fit0)
##haz60 <- hazards(fit60)
fit.g <- phreg(Surv(enter, exit, event) ~ sex + region, dist = "gompertz", 
               param = "rate", data = olm)
haz.g <- hazards(fit.g)
plot(fit.c, ylim = c(0, 8), xlab = "Age")
lines(haz0$x, haz0$y - haz0$y[1], lty = 2, col = 2, lwd = 1.5)
lines(haz.g$x + 60, haz.g$y, lty = 4, col = 4, lwd = 1.5)
legend("topleft", legend = c("Non-parametric", "Conditional Weibull", "Gompertz"), 
       lty = c(1, 2, 4), col = c(1, 2, 4))
```

It seems as if the conditional Weibull model fits data as good as or even better 
than the Gompertz model. The latter grows too fast in the very high ages, and 
this is an observation found in many studies lately [@rozh17; @leva17; @italy18; @gb19].  

### The piecewise constant proportional hazards model

The *piecewise constant* hazard (`pch`) model can always be used in the fitting process with good results. 
It is however an uncertainty moment in the process: How should time be cut into pieces, and how
many pieces should there be? Two possible strategies, (i) choose equally-spaced cut points, and 
(ii) relatively more cut points where there are many deaths, that is, where the hazard function is expected 
to be steep.

The `oldmort` data set spans a time interval of length 40 years, and we know that mortality on the age
interval 60--100 is increasing almost exponentially, suggesting more cut points in the high ages. 
Against that is the fact that in very high ages, say 90--100, not many observations are still around, most
of them have already died.

We may start with eight intervals of equal length, 60--65, 65--70, ..., 95--100, and fit a pch model. The 
result is shown in Table \@ref(tab:pchold8).

```{r pchold8, results='asis', echo = FALSE}
fit.pch <- pchreg(Surv(enter, exit, event) ~ sex + region, data = oldmort, 
                  cuts = seq(60, 100, by = 5))
dr.pch <- drop1(fit.pch, test = "Chisq")
cap <- "Old age mortality, pch proportional hazards model."
if (knitr::is_latex_output()){
  ltx(fit.pch, dr = dr.pch, 
      caption = cap, 
      label = "tab:pchold8")
}else{
  out <- summary(fit.pch)$coefficients[, -5]
  out <- round(out, 4)
  tbl(out, caption = cap)
}
```

Then the baseline cumulative hazards are compared to the one from the Cox regression fit
in Figure \@ref(fig:pcholdfig8).

```{r pcholdfig8, fig.cap="Piecewise constant cumulative hazards, old age mortality."}
check.dist(fit.pch, fit.c, main = "")
```

As expected, a good fit. It is also obvious that the estimated regression parameters 
do not vary much between the studied models. 

The piecewise constant model works well with this data set, but its real strength is its
flexibility and speed with huge data sets. Its full potential is maximized by initially 
tabulate the data by using the `eha` function `toTpch`. An illustration with the `oldmort` 
data set.

```{r pretab0, echo = TRUE}
olmtab <- toTpch(Surv(enter, exit, event) ~ sex + region, 
                 cuts = c(seq(60, 85, by = 5), 100), data = oldmort)
```

The resulting table (first five rows) is shown in Table \@ref(tab:pretab8).

```{r pretab8, echo = FALSE}
cap = "Table version of the data frame 'oldmort'."
tbl(head(olmtab, 5), caption = cap)
```

The original data set has `r NROW(oldmort)` rows (observations), while the created
table has only `r NROW(olmtab)` rows. The latter is analyzed via the function
`tpchreg`, see Table \@ref(tab:tpchold8), which is identical to Table \@ref(tab:pchold8).

```{r tpchold08, echo = TRUE}
fit.tpch <- tpchreg(oe(event, exposure) ~ sex + region, data = olmtab, time = age)
```

```{r tpchold8, echo = FALSE, results = 'asis'}
cap <- "Proportional hazards with table version of 'oldmort'."
if (knitr::is_latex_output()){
  dr <- drop1(fit.tpch, test = "Chisq")
  ltx(fit.tpch, dr = dr, 
      caption = cap,
      label = "tab:tpchold8")
}else{
  out <- round(summary(fit.tpch)$coefficients[, -5], 4)
  tbl(out, caption = cap)
}
```


### Testing the proportional hazards assumption

The `pch` model is well suited for a formal test of the proportional hazards model, 
but some trickery is needed with the `eha` package. It is best shown by example, 
and we continue by utilizing the newly created data table `olmtab`.

By omitting the `time` argument in the call to `tpchreg`, an exponential (constant
hazards) model is fitted, and the variable `age` is free to be included as a covariate.

```{r prophaz8, echo = TRUE}
fit.tpch <- tpchreg(oe(event, exposure) ~ age * (sex + region), data = olmtab)
(dr <- drop1(fit.tpch, test = "Chisq"))
```
The age interaction with `sex` is very non-significant, while the effect of region on mortality 
seems to vary significantly with age. It can be illustrated by performing a stratified
(by region) analysis with the `time = age` variable included in the usual way, 
then plotting the hazard functions for the three strata, see Figure 
\@ref(fig:plotstraph8).

```{r straph8, echo = TRUE}
fit.str <- tpchreg(oe(event, exposure) ~ sex + strata(region), 
                   time = age, data = olmtab)
```

```{r plotstraph8, fig.cap = "Hazard functions for three regions, old age mortality, Skellefteå 1860-80."}
op <- par(mfrow = c(1, 2), cex = 0.8)
##
plot(fit.str, fn = "haz", col = 1:3, lty = c(1:2, 4), lwd = 1.5, 
     main = "Hazard functions", xlab = "Age")
abline(h = 0)
plot(fit.str, fn = "cum", col = 1:3, lty = c(1:2, 4), lwd = 1.5, 
     main = "Cumulative hazards functions", xlab = "Age")
abline(h = 0)
par(op)
```

The deviating region is `town`. It is also the smallest region, with no registered 
deaths above age 90 with only 4.7 person years. We also note that mortality in ages 85-90
is highest in the town, while in the younger ages the town region has the lowest mortality.
The zero mortality in ages above age 90 is simply an artefact depending on very few 
observed person years.

Let us perform the same exercise with a larger data set, the Swedish population 1968-2019.
(See also Example \@ref(exm:sw192) in Chapter 3.) We check the hypothesis of 
proportional hazards between women and men, the question is: Is the female advantage of 
the same relative size in all ages?

```{r sweprop8, echo = TRUE}
sp <- swepop
sp$deaths <- swedeaths$deaths
```

Then 

```{r strasw8, echo = TRUE}
fit.swr <- tpchreg(oe(deaths, pop) ~ strata(sex) + I(year - 2000), last = 101,
                   time = age, data = sp)
rr.sex <- exp(tpchreg(oe(deaths, pop) ~ sex + I(year - 2000), last = 101,
                   time = age, data = sp)$coefficients[1])
cumhaz <- hazards(fit.swr, cum = TRUE) # Cumulative hazards
haz <- hazards(fit.swr, cum = FALSE) # NOT Cumulative hazards
```

```{r plotstrasw8, fig.cap = "Mortality ratio for men vs. women, Sweden 1968-2019."}
op <- par(mfrow = c(1, 2))
plot(haz$x, haz$y[2, ] / haz$y[1, ], type = "l", ylim = c(1, 3), 
     xlab = "Age", ylab = "Hazard Ratio")
abline(h = 1)
abline(h = rr.sex, lty = 2)
text(5, 1.65, "PH")
plot(cumhaz$x, cumhaz$y[2, ] / cumhaz$y[1, ], type = "l", ylim = c(1, 3), 
     xlab = "Age", ylab = "Cumulative Hazard Ratio")
abline(h = 1)
abline(h = rr.sex, lty = 2)
text(5, 1.65, "PH")
par(op)
```

We note two things: (i) The variation around the proportional hazards estimate (PH)
is huge and (ii) the smoothing effect of accumulation is large, which we should 
keep in mind as a warning when trying to judge proportionality in graphs of cumulative 
hazards functions.




### Choosing the best parametric proportional hazards model


For modeling survival data with parametric proportional hazards models,
the distributions of the function `phreg` in the package **eha** are
available. How to select a suitable parametric model is shown by a couple
of examples using the now familiar data set `oldmort`.


Remember that by design, individuals are followed from the day they are aged
60. In order to calculate the follow-up times, we usually subtract 60 from
the two columns enter and exit. Otherwise, when specifying a
parametric survivor distribution, it would in fact correspond to a
left-truncated (at 60) distribution. However, for a Cox
regression\index{Cox regression}, this  makes no difference.

```{r oldmort6.reg,echo=TRUE}
om <- oldmort
fm <- as.formula("Surv(enter, exit, event) ~ sex + region")
fm0 <- as.formula("Surv(enter - 60, exit - 60, event) ~ sex + region")
fit.w <- phreg(fm, data = oldmort)
o.w <- extractAIC(fit.w)[2]
fit.w0 <- phreg(fm0, data = oldmort)
o.w0 <- extractAIC(fit.w0)[2]
``` 

Here we applied a *Weibull* baseline distribution (the *default*
distribution in `phreg`; by specifying nothing, the Weibull is chosen). Note also the 
way we can store a formula for future use: This is particularly useful when the same 
model will be fitted several times while changing some attribute, like baseline distribution.
Now let us repeat this with the `pch`, `gompertz` and `ev` distributions in the phreg 
package, for both the unconditional and conditional approaches. 

```{r allreg6}
pch <- pchreg(fm, data = oldmort, cuts = c(seq(60, 85, by = 5), 100))
o.pch <- extractAIC(pch)[2]
g <- phreg(fm, data = om,  dist = "gompertz", param = "rate")
o.g <- extractAIC(g)[2]
ev <- phreg(fm, data = om, 
            dist = "ev")
o.ev <- extractAIC(ev)[2]
g0 <- phreg(fm, data = oldmort, 
           dist = "gompertz")
o.g0 <- extractAIC(g0)[2]
ev0 <- phreg(fm, data = oldmort, 
            dist = "ev")
o.ev0 <- extractAIC(ev0)[2]
``` 

Then we compare the AICs and choose the distribution with the
smallest value.

```{r compare6}
source("R/tbl.R")
xx <- c(o.w, o.w0, o.g, o.g0, o.ev, o.ev0, o.pch, o.pch)
xx <- matrix(xx, ncol = 4)
colnames(xx) <- c("Weibull", "Gompertz", "EV", "PCH")
rownames(xx) <- c("Conditional", "Unconditional")
tbl(xx)
``` 

First we note that only for the Weibull distribution it makes a difference which time scale 
(age or duration) is used. The Gompertz distribution gives the best fit, but the *conditional*
Weibull (age as time scale) is very close. We also saw this graphically earlier (Figure \@ref(fig:adjwg8)).

The reason that the `pch` model fares so badly is that it is punished for the large 
number of parameters (eight) it uses to estimate the baseline hazard.

## Accelerated Failure Time Models

The accelerated failure time (AFT) model is best described through relations
between survivor functions. For instance, 
comparing two groups:

   * **Group 0:** $P(T \ge t) = S_0(t)$  (control group)
   * **Group 1:** $P(T \ge t) = S_0(\phi t)$ (treatment group)
  
The model says that treatment *accelerates} failure time by the factor $\phi$.
If $\phi < 1$, treatment is good (prolongs life), otherwise bad.
Another interpretation is that the *median* life length is
  *multiplied* by $1/\phi$ by treatment.

In Figure \@ref(fig:aftph6) the difference between the accelerated failure
time  and the 
proportional hazards models concerning the hazard functions is illustrated.

```{r aftph6,fig.cap="Proportional hazards (left) and accelerated failure time model (right). The baseline distribution is Loglogistic with shape 5 (dashed).",echo=FALSE}
x <- seq(0.001, 3.001, length = 1000)
par(mfrow = c(1, 2))
plot(x, 2 * hllogis(x, shape = 5), type = "l", ylab = "", main = "PH", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
plot(x, 2 * hllogis(2 * x, shape = 5), type = "l", ylab = "", main = "AFT", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
```

The AFT hazard is not only multiplied by 2, it is also shifted to the left;
the process time is accelerated. Note how the hazards in the AFT case converges
as time increases. This is usually a sign of the suitability of an AFT model. 



### The AFT regression model
\index{model!parametric!AFT|(}
If $T$ has survivor function $S(t)$ and $T_c = T/c$,  then $T_c$ has
survivor function $S(ct)$.
Then, if $Y = \log(T)$ and $Y_c = \log(T_c)$, the
following relation holds:

\begin{equation*}
Y_c = Y - log(c).
\end{equation*}

With $Y = \epsilon$, $Y_c = Y$, and $\log(c) = -\boldsymbol{\beta} \mathbf{x}$ this can be written in
familiar form:

\begin{equation*}
Y = \boldsymbol{\beta} \mathbf{x} + \epsilon,
\end{equation*}

i.e., an ordinary linear regression model for the log survival times. In
the absence of right censoring and left truncation, this model can be
estimated by least squares. However, the presence of these forms of
incomplete data makes it necessary to rely on maximum likelihood
methods. In **R**, the functions `aftreg` in the package 
  **eha** and the function `survreg` in the package **survival** that
perform the task of fitting AFT models. The package **flexsurv** [@flexsurv] has some useful 
functionality in this area.

Besides differing parametrizations, the main difference between
  `aftreg` and `survreg` is that the latter does not allow for left
truncated data. One reason for this is that left truncation is a much
harder problem to deal with in AFT models than in proportional hazards models.

A detailed description of the implementation  of the AFT models in `eha` is 
found in Appendix \@ref(app:B).


### AFT modeling in **R**

 We repeat the examples from the proportional hazards section, but with AFT
 models instead. 
 
```{example, label = "oldmort06", name = "Old age mortality", echo = TRUE}
```
 
 For a description of this data set, see above. Here we fit an AFT model
 with the *Weibull* distribution. This should be compared to the proportional
 hazards model with the Weibull distribution, see earlier in this chapter.
 
```{r oldmort8.aft}
source("R/fit.out.R")
fit.w1 <- aftreg(fm, data = om)
if (knitr::is_latex_output()){ # PDF
        dr <- drop1(fit.w1, test = "Chisq")
        ltx(fit.w1, dr = dr, caption = caption, label = "tab:oldmort8.aft")
}else{ # HTML
    summary(fit.w1)
}
```

Note that the "Max. log. likelihood" is exactly the same, but the
regression parameter estimates differ. The explanation to this is that (i)
for the *Weibull* distribution, the AFT and the PH models are the same, and
(ii) the only problem is that different parametrizations are used. The
$p$-values and the signs of the parameter estimates should be the same.
$\Box$
\index{Functions!\fun{aftreg}|)}


```{example name = "Length of birth intervals, Weibull model."}
```
The data set **fert** in the **R** package **eha** contains
  birth intervals for married women in 19th century Skellefteå. It is
  described in detail in Chapter 1.
  Here only the
  intervals starting with the first birth for each woman are considered.
  First the data are extracted and examined.



```{r fert6.12}
library(eha)
data(fert)
f12 <- fert[fert$parity == 1, ]
f12$Y <- Surv(f12$next.ivl, f12$event)
head(f12)
```

Some women never got a second child, for instance the first woman (**id  = 1**) above. 
Also, note the just created variable $Y$; it is printed
as a vector, with some values having a trailing``$+$''; Those are the
censored observations (**event = 0**). But $Y$ is really a matrix, in
this case with two columns. The first column equals **next.ivl** and the
second is **event**.

```{r Y12}
is.matrix(f12$Y)
dim(f12$Y)
```

Next, the proportional hazards *Weibull* regression is fit and the
likelihood ratio 
tests for an effect of each covariate is performed. 
Remember that the function *drop1*\index{Functions!*drop1*} is the
one to use for the latter task. 

```{r phweib}
fit.w <- phreg(Y ~ age + year + ses, data = f12)
drop1(fit.w, test = "Chisq")
```

Three variables are included, two are extremely significant, $age$,
mother's age at start of the interval, and $year$, the calendar year at
the start of the interval. The variable *ses*, socio-economic status,
is also statistically significant, but not to the extent of the other two. 

**Note:** When we say that a variable is "significant", we are rejecting the
hypothesis that the true parameter value attached to it is equal to zero.

Now take a look at the actual fit.

```{r fitw, echo = FALSE}
fit.w
kof <- fit.w$coef[1]
```

The estimated coefficient for **age**, 
`r round(kof, digits = 3)`, is negative, indicating lower
risk of a birth with increasing age. The effect of calendar time is
positive and indicating an increase in the intensity of giving birth of the
size approximately 5 per thousand. Regarding *socio-economic status*,
*farmer* wives have the highest fertility and the *upper* class the
lowest, when it comes to getting a second child. Finally, we see that the
estimate for "$\log(\text{shape})$" is positive ($0.299$) and significantly
different from zero. This means that the intensity is increasing with
duration, see Figure \@ref(fig:weibfert6).

```{r weibfert6, fig.cap="Estimated Weibull baseline distribution for length of interval between first and second birth.", fig.scap = "Weibull birth intervals"}
oldpar <- par(mfrow = c(1, 2))
plot(fit.w, fn = "haz")
plot(fit.w, fn = "cum")
par(oldpar)
``` 

$\Box$

### The Lognormal model

An example of a more complex family of distributions with the proportional
hazards property is to start with a
*lognormal* distribution.
The family of *lognormal* distributions is characterized by the fact that
taking the natural logarithm of a random variable from the family gives a
random variable from the family of *Normal* distributions. Both the
hazard and the survivor functions lack closed forms.
Furthermore, multiplying the hazard function by a constant not equal to 1
leads to a non-lognormal hazard function.

So, while we for the *Weibull* family of distributions found subfamilies with
proportional hazards by keeping $p$ fixed and varying $\lambda$, for the
*lognormal* distribution we cannot use the same trick. Instead, by
multiplying the hazard function by a constant we arrive at a three-parameter
family of distributions. 
The lognormal proportional hazards model is
possible to fit with the function **phreg**.

```{example, name = "Length of birth intervals, Lognormal model."}
```
  The data are already extracted and examined in the previous example.
  Next, the proportional hazards *lognormal* regression and the likelihood
  ratio tests for an effect of each covariate are given.

```{r phlognorm}
fit.lognorm <- phreg(Y ~ age + year + ses, data = f12, 
                     dist = "lognormal")
drop1(fit.lognorm, test = "Chisq")
``` 


Three variables are included, two are extremely significant. First the
variable **age**, mother's age at start of the interval, and second
  *year*, the calendar year at the start of the interval. The variable 
  *ses*, socio-economic status, is also significant, but to the extent of
the previous two. 

Now take a look at the actual fit.
```{r fitlogist}
fit.lognorm
``` 

```{r secret16,echo=FALSE}
kof <- fit.lognorm$coef[1]
``` 

The estimated coefficient for age, 
`r round(kof, digits = 3)`, is negative, indicating lower
risk of a birth with increasing age. The effect of calendar time is
positive and indicating an increase in the intensity of giving birth of the
size approximately 4 per thousand. Regarding *socio-economic status*,
farmer wives have the highest fertility and the unknown and
upper classes the 
lowest, when it comes to getting a second child. Finally, in
Figure \@ref(fig:lognormfert6) it is clear that the hazard function is first
increasing to a maximum around 3 years after the first birth and then
decreasing. This is quite a different picture compared to the Weibull fit
in Figure \@ref(fig:weibfert6)! This shows the weakness with the Weibull model; it
allows only for *monotone* (increasing or decreasing) hazard
functions.

```{r lognormfert6,fig.cap="Estimated lognormal baseline distribution for length of birth intervals.",echo=FALSE}
oldpar <- par(mfrow = c(1, 2))
plot(fit.lognorm, fn = "haz", main = "Hazard function")
plot(fit.lognorm, fn = "cum", main = "Cumulative hazard function")
par(oldpar)
``` 

It was created using
```{r crepl1,echo=TRUE,fig=FALSE}
oldpar <- par(mfrow = c(1, 2))
plot(fit.lognorm, fn = "haz", main = "Hazard function")
plot(fit.lognorm, fn = "cum", main = "Cumulative hazard function")
par(oldpar)
``` 
$\Box$


### The Loglogistic model

### The Extreme Value model

### The Gompertz model

## Comparing the Weibull and Lognormal fits

From looking at Figures \@ref(fig:weibfert6) and \@ref(fig:lognormfert6), it
seems obvious that at least one of the fits must be less good: The
*Weibull* cumulative hazards
curve is convex and the *lognormal* one is concave, and the estimates
of the two hazard
functions are very different in shape. There are two direct ways
of comparing the fits.

The first way to compare is to look at the maximized log likelihoods. For
the *Weibull* fit it is `r round(fit.w$loglik[2], digits = 2)` and for
the *lognormal* fit it is 
`r round(fit.lognorm$loglik[2], digits = 2)`. The rule is that the
largest value wins, so the *lognormal* model is the clear winner. Note that
we do not claim to perform a formal test here; a likelihood ratio test
would require that the two models we want to compare are nested, but that
is not the case here. This is (here) equivalent to using the AIC\index{AIC}
as a 
measure of comparison, because the two models have the same number of
parameters to estimate.

The second method of comparison is graphical. We can plot the cumulative
hazard functions\index{cumulative hazard function}
against the nonparametric estimate from a Cox
regression\index{Cox regression} 
fit, and judge which looks closer. It starts by fitting a Cox model with
the same covariates:\index{Functions!\fun{coxreg}}

```{r coxr6}
fit.cr <- coxreg(Y ~ age + year + ses, data = f12)
``` 

Then the function **check.dist** (from **eha*) is called. We start
with the *Weibull* fit:

```{r weibcheck6,fig.cap="Check of the Weibull model, birth intervals."}
check.dist(fit.cr, fit.w)
``` 

This fit (Figure \@ref(fig:weibcheck6)) looks very poor: One reason is that
the longest waiting time for an observed event is about 12 years, while
some women simply do not get more than one child, with a long waiting time
ending in a censoring as a result. The time span from around 12 to 25
simply has no events, and the nonparametric estimate of the hazard function
in that interval is zero. The Weibull distribution fit cannot cope with
that. 

One possibility is to try to fit a Weibull distribution only for the 12
first years of duration. Note below that after calling **age.window**,
$Y$ *must* be recreated! This is the danger with this "lazy" approach.

```{r trunc6f12cens}
f12$enter <- rep(0, NROW(f12))
f12.cens <- age.window(f12, c(0, 12), 
                       surv = c("enter", "next.ivl", "event"))
f12.cens$Y <- Surv(f12.cens$enter, f12.cens$next.ivl, 
                   f12.cens$event)
fit.wc <- phreg(Y ~ age + year + ses, data = f12.cens)
fit.c <- coxreg(Y ~ age + year + ses, data = f12.cens)
fit.wc
``` 

Compare the Weibull fit with what we had without cutting off
durations at 12. 
The comparison with the nonparametric cumulative hazards function is now
seen in Figure \@ref(fig:weibcheckcens6).

```{r weibcheckcens6, fig.cap = "Check of the Weibull model, birth intervals censored at 12.", fig.scap = "Check of the Weibull model, censored birth intervals."}
check.dist(fit.c, fit.wc)
``` 

This wasn't much better. Let's look what happens with the lognormal
distribution. 

We continue using the data censored at 12 and refit the lognormal model.

```{r trunc6f12cens.ln}
fit.lnc <- phreg(Y ~ age + year + ses, data = f12.cens, 
                 dist = "lognormal")
``` 

Then, in Figure \@ref(fig:lncheckcens6) we check the fit against the
nonparametric fit again.

```{r lncheckcens6, fig.cap="Check of the lognormal model, birth intervals censored at 12.",fig.scap="Check of the lognormal model, birth intervals"}
check.dist(fit.c, fit.lnc)
``` 

It is not very much better. In fact, this empirical distribution has
features that the ordinary parametric distributions cannot cope with: Its
hazard function starts off being zero for almost one year (birth intervals
are rarely shorter than one year), then it grows rapidly and soon decreases
towards zero again.

There is, however one parametric distribution that is flexible enough: The
piecewise constant hazard distribution.

### The piecewise constant hazard (pch) model{#pch6}

The pch distribution is flexible because you can add as many parameters as
you want. We will try it on the birth interval data. We start off with just
four intervals, and the non-censored data set.

```{r pch6fert}
fit.pch <- pchreg(Surv(next.ivl, event) ~ age + year + ses, 
                 data = f12, cuts = c(0, 4, 8, 12))
fit.c <- coxreg(Surv(next.ivl, event) ~ age + year + ses, 
                data = f12)
fit.pch
``` 

Then we compare the cumulative hazards in Figure \@ref(fig:pchcheck8).

```{r pchcheck8,fig.cap="Check of the pch model, uncensored birth intervals."}
compHaz(fit.c, fit.pch)
``` 

This is not good enough. We need shorter intervals close to zero, so

```{r pchfert2}
fit.pch <- pchreg(Surv(next.ivl, event) ~ age + year + ses, 
                 data = f12, cuts = 0:13)
fit.pch
``` 

This gives us 13 intervals with constant baseline hazard, that is, 14
parameters to estimate only for the baseline hazard function! But the fit
is good, see Figure \@ref(fig:pch13check8). In fact, it is almost perfect!

```{r pch13check8,fig.cap = "Check of the pch model with thirteen constant hazard intervals, uncensored birth intervals." }
compHaz(fit.c, fit.pch)
``` 

One of the advantages with parametric models is that it is easy to study
and plot the hazard function, and not only the cumulative hazards function,
which is the dominant tool for (graphical) analysis in the case of the
nonparametric model of Cox regression\index{Cox regression}. For the
piecewise constant model 
just fitted we get the estimated hazard function in Figure \@ref(fig:pchhaz6).

```{r pchhaz6,fig.cap="The estimated hazard function for the length of birth intervals, piecewise constant hazard distribution."}
plot(fit.pch, fn = "haz")
``` 

The peak is reached during the third year of waiting, and after that the
intensity of giving birth drops fast, and it becomes zero after 12 years.

What are the implications for the estimates of the regression parameters of
the different choices of parametric model? Let us compare the regression
parameter estimates only for the three distributional assumptions, *Weibull*,
*lognormal*, and *pch*. See Table \@ref(tab:threecomp6).

```{r threecomp6,echo=FALSE}
##require(xtable)
##fit.c$coef
##fit.w$coef
##fit.lognorm$coef
##fit.pch$coef
ta <- cbind(fit.c$coef[1:5], fit.pch$coef[1:5], fit.w$coef[1:5], fit.lognorm$coef[1:5])
colnames(ta) <- c("Cox", "Pch", "Weibull", "Lognormal")
ta <- round(exp(ta), digits = 3)
knitr::kable(ta, booktabs = TRUE, caption = "Parameter estimates under different distribution assumptions for the birth intervals data.")
``` 

We can see that the differences are not large. However, the two assumptions
closest to the "truth", the Cox model and the Pch model, are very close,
while the other two have larger deviations. So an appropriate choice of
parametric model seems to be somewhat important, contrary to "common
knowledge", which says that it is not so important to have a good fit of
the baseline hazard, if the only interest lies in the regression parameters. 


## Proportional hazards or AFT model?

The problem of choosing between a proportional hazards and an accelerated
failure time model (everything else equal) can be solved by comparing the
AIC\index{AIC} of the models. Since the numbers of parameters are equal in
the two 
cases, this amounts to comparing the maximized likelihoods. For instance,
in the case with *old age mortality*:

Let us see what happens with the *Gompertz* AFT model:
Exactly the same procedure as with the *Weibull* distribution, but we have to
specify the Gompertz distribution in the call (remember, the *Weibull*
distribution is the default choice, both for `phreg` and `aftreg`).\

```{r oldln6.aft}
fit.g1 <- aftreg(fm, data = om, dist = "gompertz")
fit.g1
```

Comparing the corresponding result for the proportional hazards and the AFT
models with the Gompertz distribution,
we find that the maximized log likelihood in the former case is
`r round(fit.g1$loglik[2], 3)`, compared to
`r round(fit.g$loglik[2], 3)` for the latter. This indicates that the
proportional hazards model
  fit is better. Note however that we cannot formally test the proportional
  hazards hypothesis; the two models are not nested.

## Discrete time models

There are two ways of looking at discrete duration data; either time is truly
discrete, for instance the number of trials until an event occurs, or an
approximation due to rounding of continuous time data. In a sense all data
are discrete, because it is impossible to measure anything on a continuous
scale with infinite precision, but from a practical point of view it is
reasonable to say that data is discrete when tied events occur
embarrassingly often. 

When working with register data, time is often measured in years which
makes it necessary and convenient to work with discrete models. A typical
data format is the so-called *wide* format, where there is one record
(row) per individual, and measurements for many years. We have so far only
worked with the *long* format. The data sets created by `survSplit`
are in long format; there is one record per individual and age category. 
The R work horse in switching back and forth between the long and wide
formats is the function `reshape`. It may look confusing at first, but
if data follow some simple rules, it is quite easy to use `reshape`.

The function `reshape` is typically used with *longitudinal data*,
where there are several measurements at different time points for each
individual. If the data for one individual is registered within one record
(row), we say that data are in wide format, and if there is one record (row)
per time (several records per individual), data are in long format. Using
wide format, the rule is that time-varying variable names must end in a
numeric value indicating at which time the measurement was taken. For
instance, if the variable `civ` (civil status) is noted at times 1, 2,
and 3, there must be variables named `civ.1, civ.2, civ.3`,
respectively. It is optional to use any *separator* between the base
name (`civ`) and the time, but it should be one character or empty. The
"." is what `reshape` expects by default, so using that form
simplifies coding somewhat.

We start by creating an example data set as an illustration. This is
accomplished by starting off with the data set `oldmort` in `eha`
and "trimming" it.
```{r trimort6}
data(oldmort)
om <- oldmort[oldmort$enter == 60, ]
om <- age.window(om, c(60, 70))
om$m.id <- om$f.id <- om$imr.birth <- om$birthplace <- NULL
om$birthdate <- om$ses.50 <- NULL
om1 <- survival::survSplit(om, cut = 61:69, start = "enter", end = "exit", 
                 event = "event", episode = "agegrp")
om1$agegrp <- factor(om1$agegrp, labels = 60:69)
om1 <- om1[order(om1$id, om1$enter), ]
head(om1)
```
We may change the row numbers and recode the `id` so they are easier to read.
```{r recode6}
rownames(om1) <- 1:NROW(om1)
om1$id <- as.numeric(as.factor(om1$id))
head(om1)
``` 
This is the long format, each individual has as many records as "presence
ages". For instance, person No. 1 has four records, for the ages 60--63.
The maximum possible No. of records for one individual is 10. We can check
the distribution of No. of records per person by using the function 
  `tapply`: 
```{r recspp6}
recs <- tapply(om1$id, om1$id, length)
table(recs)
``` 
It is easier to get to grips with the distribution with a graph, in this
case a *barplot*\index{Functions!\fun{barplot}},see Figure~\ref{fig:barp6}.

```{r barp6, fig.cap = "Barplot of the number of records per person."}
barplot(table(recs))
``` 

Now, let us turn `om1` into a data frame in *wide* format. This is
done with the function `reshape`. First we remove the redundant
variables `enter` and `exit`. 
```{r wideform6}
om1$exit <- om1$enter <- NULL
om2 <- reshape(om1, v.names = c("event", "civ", "region"), 
               idvar = "id", direction = "wide", 
               timevar = "agegrp")
names(om2)
``` 
Here there are two time-fixed variables, `id` and `sex`, and three
time-varying variables, `event`, `civ`, and `region`. The
time-varying variables have suffix of the type `.xx`, where `xx` varies
from 60 to 69. 
  
This is how data in wide format usually show up; the suffix may start with
something else than `.`, but it must be a single character, or nothing. 
The real problem is how to switch from wide format to long, because our
survival analysis tools want it that way. The solution is to use
`reshape` again, but other specifications.

```{r reshap26}
om3 <- reshape(om2, direction = "long", idvar = "id", 
               varying = 3:32)
head(om3)
``` 
There is a new variable `time` created, which goes from 60 to 69, one
step for each of the ages. We would
like to have the file sorted primarily by `id` and secondary by time.
```{r sortin6}
om3 <- om3[order(om3$id, om3$time), ]
om3[1:11, ]
``` 
Note that all individuals got 10 records here, even those who only are
observed for fewer years. Individual No. 1 is only observed for the ages
60--63, and the next six records are redundant; they will not be used in an
analysis if kept, so it is from a practical point of view a good idea to
remove them.
```{r remove6}
NROW(om3)
om3 <- om3[!is.na(om3$event), ]
NROW(om3)
``` 
The data frame shrunk to almost half of what it was originally. First, let
us summarize data.
```{r summm6}
summary(om3)
``` 
The key variables in the discrete time analysis are `event` and 
  `time`. For the baseline hazard we need one parameter per value of 
  `time`, so it is practical to transform the continuous variable `time`
to a factor.
```{r turn6}
om3$time <- as.factor(om3$time)
summary(om3)
```
The summary now produces a frequency table for `time`.

For a given time point and a given individual, the response is whether an
event has occurred or not, that is, it is modeled as a 
\emph{Bernoulli}\index{Distributions!Bernoulli} outcome, which is a special
case of the \emph{binomial} distribution\index{Distributions!binomial}.
The discrete time analysis may now be performed in several ways. Most
straightforward is to run a 
logistic regression\index{logistic regression} with `event` as
response through the basic `glm` function with 
`family = binomial(link=cloglog)`. The so-called 
\emph{cloglog} link\index{cloglog link} is
used in order to preserve the proportional hazards property.
```{r glmreg6}
fit.glm <- glm(event ~ sex + civ + region + time, 
               family = binomial(link = cloglog), data = om3)
summary(fit.glm)
``` 
This output is not so pleasant, but we can anyway see that females (as
usual) have lower mortality than males, that married are better off than
unmarried, and that regional differences maybe are not so large. To get a
better understanding of the statistical significance of the findings we run
`drop1`\index{Functions!\fun{drop1}} on the fit.
```{r glmdrop6}
drop1(fit.glm, test = "Chisq")
``` 
Mildly surprisingly, `civil status` is not that statistically
significant, but `region` (and the other variables) is. The strong
significance of the time variable is of course expected; mortality is
expected to increase with increasing age.

An equivalent way, with a nicer printed output, is to use the function
\fun{glmmboot}\index{Functions!\fun{glmmboot}} in the package `glmmML`. 
```{r glmmboot6}
fit.boot <- glmmML::glmmboot(event ~ sex + civ + region, cluster = time, 
                     family = binomial(link = cloglog), 
                     data = om3)
fit.boot
``` 
The parameter estimates corresponding to `time` are contained in the
variable $fit\$frail$. They need to be transformed to get the "baseline
hazards". 
```{r calchaz6}
haz <- plogis(fit.boot$frail)
haz
``` 
A plot of the hazard function is shown in Figure \@ref(fig:plothaz6).

```{r plothaz6, fig.cap = "Baseline hazards, old age mortality."}
barplot(haz)
``` 


By some data manipulation we can also use `eha` for the analysis. For
that to succeed we need intervals as responses, and the way of doing that
is to add two variables, `exit` and `enter`. The latter must be
*slightly* smaller than the former: 

```{r mlreg6}
om3$exit <- as.numeric(as.character(om3$time))
om3$enter <- om3$exit - 0.5
fit.ML <- coxreg(Surv(enter, exit, event) ~ sex + civ + region, 
                 method = "ml", data = om3, coxph = FALSE)
fit.ML
``` 
\index{Functions!\fun{as.character}}\index{Functions!\fun{as.numeric}}
Plots of the cumulative hazards and the survival function are easily
achieved, see Figures \@ref{fig:cumML6} and \@ref(fig:surML6).

```{r cumML6, fig.cap = "The cumulative hazards, from the coxreg fit."}
plot(fit.ML, fn = "cum", xlim = c(60, 70))
``` 


```{r surML6, fig.cap = "The survival function, from the coxreg fit."}
plot(fit.ML, fn = "surv", xlim = c(60, 70))
``` 

Finally, the proportional hazards assumption can be tested in the discrete
time framework by creating an interaction between `time` and the
covariates in question. It is only possible by using `glm`.
```{r testph6}
fit2.glm <- glm(event ~ (sex + civ + region) * time, 
                family = binomial(link = cloglog), 
                data = om3)
drop1(fit2.glm, test = "Chisq")
``` 
There is no sign of non-proportionality.
