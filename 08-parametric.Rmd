# Parametric Models {#parametric}

```{r settings8, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(comment = "", message = FALSE, echo = FALSE, cache = FALSE)
options(width = 70, digits = 7)
```

We have already been given an example of a parametric survival model, the 
*piecewise constant hazards model* in the previous chapter. 
Apart from that, we have so far studied nonparametric methods for survival
analysis. This is a tradition that has its roots in medical (cancer)
research. In technical applications, on the other hand, parametric models
dominate; of special interest is the *Weibull* model \citep{ww51}. The text
book by \cite{jl03} is a good source for the study of parametric survival
models. 
More technical detail about parametric distributions is found in Appendix B .

Three kinds of parametric models are considered here: the
*proportional hazards*\index{model!parametric!PH} and the 
*accelerated failure  time*\index{model!parametric!AFT} in continuous time, 
and the *discrete time proportional hazards*\index{model!parametric!discrete} 
models.


## Proportional Hazards Models

A proportional hazards family of distributions is generated from one
specific continuous distribution by multiplying the hazard function of that
distribution by a strictly positive constant, and letting that constant
vary over the full positive real line. So, if $h_0$ is the hazard function
corresponding to the generating distribution, the family of distributions
can be described by saying that $h$ is a member of the family if

\begin{equation*}
  h_1(t) = c h_0(t) \quad \text{for some } c > 0, \; \text{and all } t > 0.
\end{equation*}
Note that it is possible to choose any hazard function (on the positive
real line) as the generating function. The resulting proportional hazards
class of distributions may or may not be a well recognized family of
distributions. 

In **R**, **eha** is one of the packages that can fit parametric proportional hazards
models. In the following subsections, the possibilities are examined.

The parametric distribution functions that naturally can be used as the baseline
distribution in the function `phreg` are the *Weibull*,
*Extreme value*, and the *Gompertz* distributions. 
The *Piecewise constant hazards* model is treated in the functions `pch`(individual data)
and `tpch` (tabular data). 

The *lognormal* and *loglogistic* distributions are 
also included as possible choices and allow for hazard functions that are 
first increasing to a maximum and then
decreasing, while the other distributions all have monotone hazard
functions. However, since these families are not closed under proportional 
hazards without artificially adding a third, "proportionality",  parameter,
they are not discussed here (regard these possibilities as experimental).
It is better to combine the lognormal and the 
loglogistic distributions with the accelerated failure time modeling, where they 
naturally fit in. 

See Figure \@ref(fig:6hazs) for Weibull and Gompertz hazard functions with selected
parameter values.

```{r 6hazs,fig.cap = "Selected hazard functions.", echo=FALSE,fig.height=5,message=FALSE}
library(eha)
oldpar <- par(mfrow = c(2, 2))
x <- seq(0.001, 10.001, length = 1000)
y <- hweibull(x, shape = 3, scale = 6)
plot(x, y, main = "Weibull, shape = 3", type = "l", 
     ylab = "", xlab = "Time", lwd = 2, ylim = c(0, 1.5), las = 1)
abline(h = 0, v = 0)
y <- hweibull(x, shape = 1/2, scale = 2)
plot(x, y, main = "Weibull, shape = 1/3", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 1.5), lwd = 2, las = 1)
abline(h = 0, v = 0)
y <- hgompertz(x, shape = 1/6, rate = 0.2, param = "rate")
plot(x, y, main = "Gompertz, rate = 2", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 1.5), lwd = 2, las = 1)
abline(h = 0, v = 0)
y <- hgompertz(x, shape = 1.5, rate = -0.2, param = "rate")
plot(x, y, main = "Gompertz, rate = -2", type = "l",
     ylab = "", xlab = "Time", ylim = c(0, 1.5), lwd = 2, las = 1)
abline(h = 0, v = 0)
par(oldpar)
```

We note in passing that the fourth case, the Gompertz model with negative rate parameter,
does not represent a true survival distribution, because the hazard function decreases
too fast: There will be a positive probability of eternal life.

Experience shows that the Gompertz distribution fits adult mortality very well, in the
ages 30 to 85, say. The modeling of mortality from birth to early adulthood, on the 
other hand, is demanding since the typical hazard function for all these ages is 
U-shaped with high infant mortality and relatively low child mortality. Since 
both the Weibull and the Gompertz distributions have a monotone hazard function,
neither is suitable to fit the mortality of the *full* human life span.  However, both 
distributions are suitable for fitting shorter pieces of the life span, and for longer spans 
there are two possibilities, a nonparametric model (Cox regression) and a 
piecewise constant hazard hazard model, where the former can be seen as a 
limiting case of the latter. More about that later. 

### The Weibull model

The family of *Weibull* distributions
may be defined by the family of hazard functions

\begin{equation}
h(t; p, \lambda) = \frac{p}{\lambda} \biggl(\frac{t}{\lambda}\biggr)^{p-1},
\quad t, p, \lambda > 0.
(\#eq:6weibull)
\end{equation}

If we start with

$$
h_1(t) = t^{p-1}, \quad p \ge 0, \; t > 0
$$

for a *fixed* $p$, and generate a proportional hazards family from there,

\begin{equation*}
h_c(t) = c h_1(t), \quad c, t > 0,
\end{equation*}

we get

\begin{equation}\label{eq:6weibdist}
h_c(t) = c t^{p-1} = \frac{p}{\lambda} \biggl(\frac{t}{\lambda}\biggr)^{p-1}
\end{equation}

by setting
$$
c = \frac{p}{\lambda^p},
$$

which shows that for each fixed $p > 0$, a proportional hazards family is
generated by varying $\lambda$ in \eqref{eq:6weibdist}. On the other hand,
if we pick two members from the family \eqref{eq:6weibdist} with
*different* values of $p$, they would not be proportional. 

To summarize, the *Weibull* family of distributions is not *one* family
of proportional hazards distributions, but a *collection* of families
of proportional hazards. The collection is indexed by $p > 0$. It is true,
though, that all the families are closed under the Weibull distribution. 

The proportional hazards regression model with a Weibull baseline
distribution is obtained by multiplying \@ref(eq:6weibull) by $\exp(\beta x)$:

\begin{equation}
  h(t; x, \lambda, p, \beta) = \frac{p}{\lambda}
  \biggl(\frac{t}{\lambda}\biggr)^{p-1} e^{\beta x}, \quad t > 0.
(\#eq:6weibreg)
\end{equation}

The function **phreg** in package **eha** fits models like
\@ref(eq:6weibreg) by default.

### The Gompertz distribution

The Gompertz families of distributions are defined in essentially two ways in the 
**R** package `eha`: The *rate* and the *canonical* representations. The reason 
for this duality is that the families need to be differently represented depending on 
whether proportional hazards or accelerated failure time models are under consideration.

In the *proportional hazards* case, the *rate* formulation is used, and it is
characterized by an exponentially increasing hazard function with fixed rate `r`:

\begin{equation}
h(t; p, r) = p e^{r t}, \quad p, t > 0; -\infty < r < \infty.
\end{equation}

As noted earlier, when $r < 0$, the hazard function $h$ is decreasing "too fast" 
to define a proper survival function, and $r = 0$ gives the
*exponential distribution* as a special case. And for each fixed $r$, the family 
of distributions indexed by $p > 0$ constitutes a proportional hazards family of 
distributions, and the corresponding regression model is written as

\begin{equation}
h(t; x, p, r, \beta) = p e^{r t} e^{\beta x}, \quad t > 0.
\end{equation}

### Application


```{example name = "Old age mortality."}
```
The data set **oldmort** in the **R** package **eha** contains
  life histories of people followed from their 60th birthday to their 100th, or 
  until death, born between June 28, 1765 and December 31, 1820
  in Skellefte책. The data set is described in detail in Chapter 1. The variable
  `enter` is age at start of the given interval, and `exit` contains the age at 
  the end of the interval. We need to calculate *follow-up time* since age 60, so a new 
  data frame, `olm`, is created as a copy of `oldmort`, and then 60 is subtracted from 
  `enter` and `exit`. 
  See the result in
  Table \@ref(tab:olddata8), where the most relevant variables for our purpose are shown.
  
```{r olddata8, echo = FALSE}
library(eha)
library(kableExtra)
olm <- oldmort[, c("birthdate", "sex", "region", "enter", "exit", "event")]
olm$age <- olm$enter # Age at start of follow-up
olm$enter <- olm$enter - 60 # Duration since 60th birthday.
olm$exit <- olm$exit - 60 # Ditto
olm$birthdate <- toDate(olm$birthdate)
source("R/tbl.R")
tbl(head(olm, 5), caption = "The data set 'olm', first rows.")
##kbl(head(olm, 5), booktabs = TRUE, row.names = FALSE, 
  ##  caption = "The data set 'olm', first rows.", label = "olddata8") %>%
    ##    kable_styling(font_size = 11, full_width = FALSE)  
```

The variable names are more or less self explanatory,  *enter*  and *exit* are time 
in years since the sixtieth birthday, *age* is age at start of follow-up 
(the original `enter` variable). The variable *event* is 
an indicator of death at the duration given by `exit`.

To have something to compare to, a Cox regression is performed first, see Table
\@ref(tab:coxold8). Two covariates are included in the model, `sex` and `region`.
Both are *categorical*, `region` with three categories, *town* (reference), *industry*, and *rural*,
and `sex` with *male* as reference category.

```{r coxold8, echo = FALSE, results="asis"}
fit.c <- coxreg(Surv(enter, exit, event) ~ sex + region, data = olm)
dr.c <- drop1(fit.c, test = "Chisq")
xx <- summary(fit.c)
cap <- "Old age mortality, Cox proportional hazards model."
source("R/fit.out.R")
fit.out(fit.c, caption = cap, label = "coxold8")
#if (knitr::is_latex_output()){
 #   ltx(fit.c, dr = dr.c, caption = cap, label = "tab:coxold8")
##}else{
    ##knitr::kable(round(xx$coefficients, 4), booktabs = TRUE, caption = cap)
 ##   kbl(round(xx$coefficients, 4), booktabs = TRUE, caption = cap) %>%
 #   kable_styling(font_size = 11, full_width = FALSE)
#}
```

Then a Weibull model is fitted, see Table \@ref(tab:weibold8).

```{r preweibold8, echo = TRUE}
fit <- phreg(Surv(enter - 60, exit - 60, event) ~ sex + region, 
             dist = "weibull", data = oldmort)
```

```{r weibold8, results = "asis", echo = FALSE}
dr <- drop1(fit, test = "Chisq")
cap <- "Old age mortality, Weibull proportional hazards model."
fit.out(fit, caption = cap, label = "weibold8")
```

A closer look at the estimates of regression coefficients shows that they are not very
close, in Table \@ref(tab:sbys8) they are put side by side for easier comparison.

```{r sbys8, echo = FALSE, results='asis'}
n.co <- length(fit.c$coefficients)
out <- cbind(fit.c$coefficients, fit$coefficients[1:n.co])
colnames(out) <- c("Cox", "Weibull")
out <- round(out, 4)
kbl(out, booktabs = TRUE,
             caption = "Coefficients with Cox and Weibull regressions, data oldmort.") %>%
  kable_styling(full_width = FALSE)
```

Let us compare the estimated cumulative baseline hazard functions, see Figure \@ref(fig:cwcolm8).

```{r cwcolm8, fig.cap = "Baseline cumulative hazards for Cox and Weibull regressions."}
##check.dist(fit.c, fit)
par(las = 1)
haz <- hazards(fit, cum = TRUE)
plot(fit.c, xlab = "Years above age 60.")
lines(haz$x, haz$y, lty = 2)
legend("topleft", legend = c("Cox regression", "Weibull regression"), lty = 1:2)
```

This is not a good fit, it seems as if the Weibull hazard cannot grow fast enough.
A better approach is to fit a *Gompertz* distribution, and check parameter 
and baseline hazards estimates, see Figure \@ref(fig:gowecof8) and 
Table \@ref(tab:gowecot8).

```{r gowecof8, fig.cap = "Baseline cumulative hazards for Cox and Gompertz regressions."}
fit.g <- phreg(Surv(enter - 60, exit - 60, event) ~ sex + region, 
             dist = "gompertz", param = "rate", data = oldmort)
plot(fit.c, xlab = "Years above age 60.")
haz.g <- hazards(fit.g, cum = TRUE)
plot(fit.c, xlab = "Years above age 60.")
lines(haz.g$x, haz.g$y, lty = 2)
legend("topleft", legend = c("Cox regression", "Gompertz regression"), lty = 1:2)
```

The Gompertz model fits the baseline hazard very well up until duration 30 (age 90),
but after that the exponential growth slows down. The early growth rate is 
`r round(fit.g$coefficients[n.co + 1], 3) * 100` per cent per year.

The result of fitting the Gompertz model is shown in Table \@ref(tab:gocofph8).

```{r gocofph8, results = 'asis'}
source("R/fit.out.R")
cap <- "Old age mortality, Gompertz PH model."
lab <- "gocofph8"
fit.out(fit.g, caption = cap, label = lab)
```


```{r gowecot8}
out <- cbind(fit.g$coefficients[1:n.co], out)
colnames(out) <- c("Gompertz", "Cox", "Weibull")
out <- round(out, 3)
source("R/tbl_html.R")
tbl_html(out, caption = "Coefficients with Gompertz, Cox and Weibull regressions, time scale duration since age 60.")
##kableExtra::kbl(out, booktabs = TRUE, 
  ##           caption = "Coefficients with Gompertz, Cox and Weibull regressions, time scale duration since age 60.")
```

The Gompertz and Cox models are very close, both regarding regression parameter estimates and 
baseline hazard functions.


### The parametric model with left truncation

The data set `oldmort` contains left-truncated life histories as a consequence of
using *age* as time scale. In the presentation above we chose to change the time scale 
so that the origin was age 60 (sharp). This is of no importance when fitting the
semi-parametric Cox regression, an additive change of time scale will only shift
the estimated cumulative hazards along the x-axis.

But for parametric models it matters, and we illustrate it by repeating the 
previous Weibull, Gompertz,  and Cox regression analyses with the original time scale. 
The consequence is that focus is shifted from a conditional (on survival until 60) analysis
to an unconditional, where the baseline hazard and regression coefficients are estimated for
the *full life span* (0--100 years of age).

```{r origts8}
fit.g <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort, dist = "gompertz")
fit.w <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort)
fit.c <- coxreg(Surv(enter, exit, event) ~ sex + region, data = oldmort)
yy <- round(cbind(fit.g$coefficients[1:n.co], fit.c$coefficients, fit.w$coefficients[1:n.co]), 3)
colnames(yy) <- c("Gompertz", "Cox", "Weibull")
##knitr::kable(yy, booktabs = TRUE, caption = "Coefficients with Gompertz, Cox and Weibull regressions, time scale age.")
source("R/tbl_html.R")
tbl_html(yy, caption = "Coefficients with Gompertz, Cox and Weibull regressions, time scale age." )
```

Compare these fitted coefficients with the earlier from Table \@ref(tab:gowecot8). For the Gompertz and Cox regression models, coefficients are identical, while they differ for the Weibull distribution.

Let us look at the baseline cumulative hazards, see Figure \@ref(fig:origtsfig8), where 
the graph is cut at ages 60 and 80 for clarity. Notice the value at Time = 60: The parametric models are
"extrapolating" back to time at birth, and so the estimates do not represent the cumulative hazards of the conditional
distribution, given survival to age 60. This has an impact on the estimates of the regression coefficients
in the case of the Weibull distribution, because the *conditional* is *not* Weibull even though the unconditional is.
This phenomenon does *not* apply to the Gompertz distribution, for which the conditional distribution is again Gompertz
with the same rate, but with a different level. 

```{r origtsfig8, fig.cap = "Gompertz vs Cox and Weibull vs Cox estimated cumulative hazards functions."}
op = par(mfrow = c(1, 2), las = 1)
plotHaz(fit.g, fit.c, interval = c(60, 80), ylim = c(0, 1.6), leglab = c("Gompertz", "Cox"))
plotHaz(fit.w, fit.c, interval = c(60, 80), ylim = c(0, 1.6), leglab = c("Weibull", "Cox"))
par(op)
```

From Figure \@ref(fig:origtsfig8) it appears as if we can get the *conditional* 
cumulative hazards simply by subtracting the value at age 60, $H(60)$, from the 
whole curves, and that is in fact correct. In the Gompertz case, it would simply
recover Figure \@ref(fig:gowecof8), but the Weibull case is different: Starting 
with Figure \@ref(fig:cwcolm8) and adding the adjusted Weibull curve from 
Figure \@ref(fig:origtsfig8) we get Figure \@ref(fig:adjweib8).

```{r adjweib8, fig.cap = "Comparison of the conditional and unconditional Weibull models."}
fit0 <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort)
fit60 <- phreg(Surv(enter, exit, event) ~ sex + region, data = olm)
haz0 <- hazards(fit0)
haz60 <- hazards(fit60)
plot(fit.c, ylim = c(0, 8), xlab = "Age")
lines(haz0$x, haz0$y - haz0$y[1], lty = 2, col = 2, lwd = 1.5)
lines(haz60$x + 60, haz60$y, lty = 4, col = 4, lwd = 1.5)
legend("topleft", legend = c("Non-parametric", "Conditional Weibull", "Weibull"), 
       lty = c(1, 2, 4), col = c(1, 2, 4))
```

Obviously, the conditional Weibull distribution fits data much better than the 
unconditional one. The comparison with the Gompertz distribution is shown in 
Figure \@ref(fig:adjwg8).

```{r adjwg8, fig.cap = "Comparison of the Gompertz and conditional Weibull models."}
##fit0 <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort)
##fit60 <- phreg(Surv(enter, exit, event) ~ sex + region, data = olm)
##haz0 <- hazards(fit0)
##haz60 <- hazards(fit60)
fit.g <- phreg(Surv(enter, exit, event) ~ sex + region, dist = "gompertz", 
               param = "rate", data = olm)
haz.g <- hazards(fit.g)
plot(fit.c, ylim = c(0, 8), xlab = "Age")
lines(haz0$x, haz0$y - haz0$y[1], lty = 2, col = 2, lwd = 1.5)
lines(haz.g$x + 60, haz.g$y, lty = 4, col = 4, lwd = 1.5)
legend("topleft", legend = c("Non-parametric", "Conditional Weibull", "Gompertz"), 
       lty = c(1, 2, 4), col = c(1, 2, 4))
```

It seems as if the conditional Weibull model fits data as good as or even better 
than the Gompertz model. The latter grows too fast in the very high ages, and 
this is an observation found in many studies lately [@rozh17; @leva17; @italy18; @gb19].  

### The piecewise constant proportional hazards model

The *piecewise constant* hazard (`pch`) model can always be used in the fitting process with good results. 
It is however an uncertainty moment in the process: How should time be cut into pieces, and how
many pieces should there be? Two possible strategies, (i) choose equally-spaced cut points, and 
(ii) relatively more cut points where there are many deaths, that is, where the hazard function is expected 
to be steep.

The `oldmort` data set spans a time interval of length 40 years, and we know that mortality on the age
interval 60--100 is increasing almost exponentially, suggesting more cut points in the high ages. 
Against that is the fact that in very high ages, say 90--100, not many observations are still around, most
of them have already died.

We may start with eight intervals of equal length, 60--65, 65--70, ..., 95--100, and fit a pch model
with the aid of the function `pchreg`\index{Functions!\fun{pchreg}} in `eha`. The 
result is shown in Table \@ref(tab:pchold8).

```{r pchold8, results='asis', echo = FALSE}
fit.pch <- pchreg(Surv(enter, exit, event) ~ sex + region, data = oldmort, 
                  cuts = seq(60, 100, by = 5))
dr.pch <- drop1(fit.pch, test = "Chisq")
cap <- "Old age mortality, pch proportional hazards model."
if (knitr::is_latex_output()){
  ltx(fit.pch, dr = dr.pch, 
      caption = cap, 
      label = "tab:pchold8")
}else{
  out <- summary(fit.pch)$coefficients[, -5]
  out <- round(out, 4)
  tbl(out, caption = cap)
}
```

Then the baseline cumulative hazards are compared to the one from the Cox regression fit
in Figure \@ref(fig:pcholdfig8).

```{r pcholdfig8, fig.cap="Piecewise constant cumulative hazards, old age mortality."}
check.dist(fit.pch, fit.c, main = "")
```

As expected, a good fit. It is also obvious that the estimated regression parameters 
do not vary much between the studied models. 

The piecewise constant model works well with this data set, but its real strength is its
flexibility and speed with huge data sets. Its full potential is maximized by initially 
tabulate the data by using the `eha` function `toTpch`. An illustration with the `oldmort` 
data set.

```{r pretab0, echo = TRUE}
olmtab <- toTpch(Surv(enter, exit, event) ~ sex + region, 
                 cuts = c(seq(60, 85, by = 5), 100), data = oldmort)
```

The resulting table (first five rows) is shown in Table \@ref(tab:pretab8).

```{r pretab8, echo = FALSE}
cap = "Table version of the data frame 'oldmort'."
tbl(head(olmtab, 5), caption = cap)
```

The original data set has `r NROW(oldmort)` rows (observations), while the created
table has only `r NROW(olmtab)` rows. The latter is analyzed via the function
`tpchreg`, see Table \@ref(tab:tpchold8), which is identical to Table \@ref(tab:pchold8).

```{r tpchold08pre, echo = TRUE}
fit.tpch <- tpchreg(oe(event, exposure) ~ sex + region, data = olmtab, time = age)
```

```{r tpchold8, echo = FALSE, results = 'asis'}
cap <- "Proportional hazards with table version of 'oldmort'."
fit.out(fit.tpch, caption = cap, label = "tpchold8")
```


### Testing the proportional hazards assumption

The `pch` model is well suited for a formal test of the proportional hazards model, 
but some trickery is needed with the `eha` package. It is best shown by example, 
and we continue by utilizing the newly created data table `olmtab`.

By omitting the `time` argument in the call to `tpchreg`, an exponential (constant
hazards) model is fitted, and the variable `age` is free to be included as a covariate.

```{r prophaz8, echo = TRUE}
fit.tpch <- tpchreg(oe(event, exposure) ~ age * (sex + region), data = olmtab)
(dr <- drop1(fit.tpch, test = "Chisq"))
```
The age interaction with `sex` is very non-significant, while the effect of region on mortality 
seems to vary significantly with age. It can be illustrated by performing a stratified
(by region) analysis with the `time = age` variable included in the usual way, 
then plotting the hazard functions for the three strata, see Figure 
\@ref(fig:plotstraph8).

```{r straph8, echo = TRUE}
fit.str <- tpchreg(oe(event, exposure) ~ sex + strata(region), 
                   time = age, data = olmtab)
```

```{r plotstraph8, fig.cap = "Hazard functions for three regions, old age mortality, Skellefte책 1860-80."}
op <- par(mfrow = c(1, 2), cex = 0.8)
##
plot(fit.str, fn = "haz", col = 1:3, lty = c(1:2, 4), lwd = 1.5, 
     main = "Hazard functions", xlab = "Age")
abline(h = 0)
plot(fit.str, fn = "cum", col = 1:3, lty = c(1:2, 4), lwd = 1.5, 
     main = "Cumulative hazards functions", xlab = "Age")
abline(h = 0)
par(op)
```

The deviating region is `town`. It is also the smallest region, with no registered 
deaths above age 90 with only 4.7 person years. We also note that mortality in ages 85-90
is highest in the town, while in the younger ages the town region has the lowest mortality.
The zero mortality in ages above age 90 is simply an artefact depending on very few 
observed person years.

Let us perform the same exercise with a larger data set, the Swedish population 1968-2019.
(See also Example \@ref(exm:sw192) in Chapter 3.) We check the hypothesis of 
proportional hazards between women and men, the question is: Is the female advantage of 
the same relative size in all ages?

```{r sweprop8, echo = TRUE}
sp <- swepop
sp$deaths <- swedeaths$deaths
```

Then 

```{r strasw8, echo = TRUE}
fit.swr <- tpchreg(oe(deaths, pop) ~ strata(sex) + I(year - 2000), last = 101,
                   time = age, data = sp)
rr.sex <- exp(tpchreg(oe(deaths, pop) ~ sex + I(year - 2000), last = 101,
                   time = age, data = sp)$coefficients[1])
cumhaz <- hazards(fit.swr, cum = TRUE) # Cumulative hazards
haz <- hazards(fit.swr, cum = FALSE) # NOT Cumulative hazards
```

```{r plotstrasw8, fig.cap = "Mortality ratio for men vs. women, Sweden 1968-2019.", echo = TRUE}
op <- par(mfrow = c(1, 2))
plot(haz$x, haz$y[2, ] / haz$y[1, ], type = "l", ylim = c(1, 3), 
     xlab = "Age", ylab = "Hazard Ratio")
abline(h = 1)
abline(h = rr.sex, lty = 2)
text(5, 1.65, "PH")
plot(cumhaz$x, cumhaz$y[2, ] / cumhaz$y[1, ], type = "l", ylim = c(1, 3), 
     xlab = "Age", ylab = "Cumulative Hazard Ratio")
abline(h = 1)
abline(h = rr.sex, lty = 2)
text(5, 1.65, "PH")
par(op)
```

We note two things: (i) The variation around the proportional hazards estimate (PH)
is huge and (ii) the smoothing effect of accumulation is large, which we should 
keep in mind as a warning when trying to judge proportionality in graphs of cumulative 
hazards functions.




### Choosing the best parametric proportional hazards model


For modeling survival data with parametric proportional hazards models,
the distributions of the function `phreg` in the package **eha** are
available. How to select a suitable parametric model is shown by a couple
of examples using the now familiar data set `oldmort`.


Remember that by design, individuals are followed from the day they are aged
60. In order to calculate the follow-up times, we usually subtract 60 from
the two columns enter and exit. Otherwise, when specifying a
parametric survivor distribution, it would in fact correspond to a
left-truncated (at 60) distribution. However, for a Cox
regression\index{Cox regression}, this  makes no difference.

```{r oldmort6.reg,echo=TRUE}
om <- oldmort
fm <- as.formula("Surv(enter, exit, event) ~ sex + region")
fm0 <- as.formula("Surv(enter - 60, exit - 60, event) ~ sex + region")
fit.w <- phreg(fm, data = oldmort)
o.w <- extractAIC(fit.w)[2]
fit.w0 <- phreg(fm0, data = oldmort)
o.w0 <- extractAIC(fit.w0)[2]
``` 

Here we applied a *Weibull* baseline distribution (the *default*
distribution in `phreg`; by specifying nothing, the Weibull is chosen). Note also the 
way we can store a formula for future use: This is particularly useful when the same 
model will be fitted several times while changing some attribute, like baseline distribution.
Now let us repeat this with the `pch`, `gompertz` and `ev` distributions in the phreg 
package, for both the unconditional and conditional approaches. 

```{r allreg6}
pch <- pchreg(fm, data = oldmort, cuts = c(seq(60, 85, by = 5), 100))
o.pch <- extractAIC(pch)[2]
g <- phreg(fm, data = om,  dist = "gompertz", param = "rate")
o.g <- extractAIC(g)[2]
ev <- phreg(fm, data = om, 
            dist = "ev")
o.ev <- extractAIC(ev)[2]
g0 <- phreg(fm, data = oldmort, 
           dist = "gompertz")
o.g0 <- extractAIC(g0)[2]
ev0 <- phreg(fm, data = oldmort, 
            dist = "ev")
o.ev0 <- extractAIC(ev0)[2]
``` 

Then we compare the AICs and choose the distribution with the
smallest value.

```{r compare6}
source("R/tbl.R")
xx <- c(o.w, o.w0, o.g, o.g0, o.ev, o.ev0, o.pch, o.pch)
xx <- matrix(xx, ncol = 4)
colnames(xx) <- c("Weibull", "Gompertz", "EV", "PCH")
rownames(xx) <- c("Conditional", "Unconditional")
tbl(xx)
``` 

First we note that only for the Weibull distribution it makes a difference which time scale 
(age or duration) is used. The Gompertz distribution gives the best fit, but the *conditional*
Weibull (age as time scale) is very close. We also saw this graphically earlier (Figure \@ref(fig:adjwg8)).

The reason that the `pch` model fares so badly is that it is punished for the large 
number of parameters (eight) it uses to estimate the baseline hazard.

## Accelerated Failure Time Models

The accelerated failure time (AFT) model is best described through relations
between survivor functions. For instance, 
comparing two groups:

   * **Group 0:** $P(T \ge t) = S_0(t)$  (control group)
   * **Group 1:** $P(T \ge t) = S_0(\phi t)$ (treatment group)
  
The model says that treatment *accelerates* failure time by the factor $\phi$.
If $\phi < 1$, treatment is good (prolongs life), otherwise bad.
Another interpretation is that the *median* life length is
  *multiplied* by $1/\phi$ by treatment.

In Figure \@ref(fig:aftph8) the difference between the accelerated failure
time  and the 
proportional hazards models concerning the hazard functions is illustrated.

```{r aftph8,fig.cap="Proportional hazards (left) and accelerated failure time model (right). The baseline distribution is Loglogistic with shape 5 (dashed).",fig.scap="PH and AFT models, Loglogistic distribution",echo=FALSE}
x <- seq(0.001, 3.001, length = 1000)
par(mfrow = c(1, 2))
plot(x, 2 * hllogis(x, shape = 5), type = "l", ylab = "", main = "PH", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
plot(x, 2 * hllogis(2 * x, shape = 5), type = "l", ylab = "", main = "AFT", xlab = "Time")
lines(x, hllogis(x, shape = 5), lty = 2)
```

The AFT hazard is not only multiplied by 2, it is also shifted to the left;
the process time is accelerated. Note how the hazards in the AFT case converges
as time increases. This is usually a sign of the suitability of an AFT model. 



### The AFT regression model
\index{model!parametric!AFT|(}
If $T$ has survivor function $S(t)$ and $T_c = T/c$,  then $T_c$ has
survivor function $S(ct)$.
Then, if $Y = \log(T)$ and $Y_c = \log(T_c)$, the
following relation holds:

\begin{equation*}
Y_c = Y - log(c).
\end{equation*}

With $Y = \epsilon$, $Y_c = Y$, and $\log(c) = -\boldsymbol{\beta} \mathbf{x}$ this can be written in
familiar form:

\begin{equation*}
Y = \boldsymbol{\beta} \mathbf{x} + \epsilon,
\end{equation*}

i.e., an ordinary linear regression model for the log survival times. In
the absence of right censoring and left truncation, this model can be
estimated by least squares. However, the presence of these forms of
incomplete data makes it necessary to rely on maximum likelihood
methods. In **R**, the functions `aftreg` in the package 
  **eha** and the function `survreg` in the package **survival** that
perform the task of fitting AFT models. The package **flexsurv** [@flexsurv] has some useful 
functionality in this area.

Besides differing parametrizations, the main difference between
  `aftreg` and `survreg` is that the latter does not allow for left
truncated data. One reason for this is that left truncation is a much
harder problem to deal with in AFT models than in proportional hazards models.
The reason is that, with a time varying covariate $z(t), t \ge 0$, the AFT model is 

\begin{equation*}
S(t; z) = S_0\biggl(\int_0^t \exp\bigl(\beta z(s)\bigr)ds\biggr),
\end{equation*}

and it is required that $z(s)$ is known for all $s, 0 \le s \le t$. With a left 
truncated observation at $t_0$, say, $z(s)$ is unknown for $0 \le s < t_0$. In `eha`,
this is solved by *assuming* that $z(s) = z(t_0), 0 \le s < t_0$.

A detailed description of the implementation  of the AFT models in `eha` is 
found in Appendix \@ref(app:B).


### AFT modeling in **R**

 We repeat the examples from the proportional hazards section, but with AFT
 models instead. 
 
```{example, label = "oldmort08", name = "Old age mortality", echo = TRUE}
```
 
 For a description of this data set, see above. Here we fit an AFT model
 with the *Weibull* distribution. This should be compared to the proportional
 hazards model with the Weibull distribution, see Table \@ref(tab:weibold8).
 
```{r oldmort8aftpre, results='asis', echo = TRUE}
source("R/fit.out.R")
fm <- as.formula("Surv(enter - 60, exit - 60, event) ~ sex + region")
fit.w1 <- aftreg(fm, id = id, data = oldmort)
```

Note carefully the inclusion of the argument `id`, it is necessary when some 
individuals are represented by more than one record in the data.

```{r oldmort8aft, results = 'asis'}
capt <- "Old age mortality in 19th century Skellefte책, AFT model."
fit.out(fit.w1, caption = capt, label = "oldmort8aft")

maxml <- round(fit.w1$loglik[2])
```


Note that the "Max. log. likelihood", `r maxml`, is not exactly the same, and 
the reason is the presence of time-varying covariate `region`. With no time-varying
covariates, the AFT and the PH models are equivalent. $\ \Box$
\index{Functions!\fun{aftreg}|)}


```{example name = "Length of birth intervals.", echo = TRUE}
```
The data set **fert** in the **R** package **eha** contains
  birth intervals for married women in 19th century Skellefte책. It is
  described in detail in Chapter 1.
  Here only the
  intervals starting with the first birth for each woman are considered.
  First the data are extracted and examined.



```{r fert128}
library(eha)
data(fert)
f12 <- fert[fert$parity == 1, ]
f12$enter <- 0
f12 <- age.window(f12, c(0, 8), surv = c("enter", "next.ivl", "event"))
f12$enter <- NULL
tbl(head(f12), caption = "The fertility data set, first few rows.", fs = 11)
```

Some women never got a second child, for instance the first woman (**id  = 1**) above. 

It turns out that, as is reasonably expected, that the hazard function for time to the 
second birth is first increasing to a maximum, then decreasing. Indeed, we can 
make a crude check of that by estimating the the hazard function without covariates
with the piecewise constant hazard model, which in principle is a non-parametric approach
if the time span is cut in small enough pieces. So we cut the first eight years in half-year-long
pieces.

```{r pchivl8, fig.cap = "Estimated hazard function for waiting time between first and second birth."}
fit.pch <- pchreg(Surv(next.ivl, event) ~ 1, data = f12, cuts = seq(0, 8, by = 0.5))
plot(fit.pch, fn = "haz", lwd = 1.5, main = "")
abline(h = 0)
```

There are two possible candidates for the baseline distribution that has the 
right shape of the hazard function: The *Lognormal* and the
*Loglogistic* distributions. $\ \Box$


### The Lognormal model

The family of *lognormal* distributions is characterized by the fact that
taking the natural logarithm of a random variable from the family gives a
random variable from the family of *Normal* distributions. Both the
hazard and the survivor functions lack closed forms.


```{example, name = "Length of birth intervals, Lognormal model."}
```
  The data are already extracted and examined in the previous example.
  Now, the accelerated failure time (AFT) *lognormal* regression model is examined.

```{r phlognorm8pre, echo = TRUE}
fit.lognorm <- aftreg(Surv(next.ivl, event) ~ age + I(year - 1860) + ses, 
                      data = f12, dist = "lognormal")
```

Note that the argument `id` is *not* used here, that is because there are no 
timevarying covariates for `next.ivl`. The `id` variable in the data set refers to 
*mother's id* and not to specific birth intervals. The data set `f12` is a subset 
of `fert`, it only includes intervals starting with the first birth.

```{r phlognorm8, results = 'asis'}
lab <- "phlognorm8"
cap <- "Length of birth intervals, Lognormal AFT model."
fit.out(fit.lognorm, caption = cap, label = lab)
``` 

The interpretation of the regression coefficients is different from the PH case:
Exponentiated (*lifeAcc*) they measure how much time to event is *accelerated*. 
For instance, if mother's age is increased by one year, the waiting time to the 
next birth is accelerated by the factor `r round(exp(fit.lognorm$coefficient[1]), 4)`,
that is, slowed down slightly. 

It may be more natural to use *expected life* as a comparison rather than 
*accelerated time*, even if one is the reverse of the other. In `aftreg`, it is 
possible to choose `param = lifeExp` for that puropse.

```{r phlognorm28pre, echo = TRUE}
fit.lognorm2 <- aftreg(Surv(next.ivl, event) ~ age + I(year - 1860) + ses, 
                      data = f12, dist = "lognormal", param = "lifeExp")
```

```{r phlognorm28, results = 'asis'}
lab2 <- "phlognorm28"
cap2 <- "Length of birth intervals, Lognormal AFT model, reverse parametrization."
fit.out(fit.lognorm2, caption = cap2, label = lab2)
``` 

As can be seen by comparing Tables \@ref(tab:phlognorm28) and \@ref(tab:phlognorm8), 
the only difference is the signs of the estimated coefficients. And therefore, 
"lifeAcc = 1 / lifeExp". The simple interpretation of the "lifeExp" parameter is that it
multiplies the expected value for the corresponding category.
In Figure \@ref(fig:lognormfert8) the estimated baseline hazard function is shown.

```{r lognormfert8,fig.cap="Estimated lognormal baseline hazard function for length of birth intervals.",echo=FALSE}
##oldpar <- par(mfrow = c(1, 2))
plot(fit.lognorm, fn = "haz", main = "",
     xlab = "Years", ylab = "Hazards")
##plot(fit.lognorm, fn = "cum", main = "Cumulative hazard function")
##par(oldpar)
``` 

It was created using
```r
plot(fit.lognorm, fn = "haz", main = "", 
     xlab = "Years", ylab = "Hazards")
``` 
$\ \Box$


### The Loglogistic model

The family of *loglogistic* distributions is characterized by the fact that
taking the natural logarithm of a random variable from the family gives a
random variable from the family of *Logistic* distributions. Contrary to the 
Lognormal case, both the hazard and the survivor functions have closed forms.


```{example, name = "Length of birth intervals, Loglogistic model."}
```
  The data are already extracted and examined in the previous example.
  Now, the accelerated failure time (AFT) *loglogistic* regression model is examined.

```{r phloglogist8pre, echo = TRUE}
fit.loglogist <- aftreg(Surv(next.ivl, event) ~ age + I(year - 1860) + ses, 
                      data = f12, dist = "loglogistic")
```

```{r phloglogist8, results = 'asis'}
lab <- "phloglogist8"
cap <- "Length of birth intervals, Loglogistic AFT model."
fit.out(fit.loglogist, caption = cap, label = lab)
``` 

Which is the better fit, the lognormal or the loglogistic? Let us compare the maximized 
log likelihoods: For the lognormal it is `r round(fit.lognorm$loglik[2], 2)`, and for the 
loglogistic it is `r round(fit.loglogist$loglik[2], 2)`, so it seems as if the 
*loglogistic* fit is clearly better. Note, though, that this is *not* a formal
statistical test, it is a comparison by the *Akaike* criterion [@akaike74].
This conclusion is also partly supported by a comparison of the two estimated 
baseline hazard 
functions (Figures \@ref(fig:loglogistfert8) and \@ref(fig:lognormfert8)) with the 
estimated crude hazard function in Figure \@ref(fig:pchivl8).


```{r loglogistfert8,fig.cap="Estimated loglogistic baseline hazard function for length of birth intervals.",echo=FALSE}
##oldpar <- par(mfrow = c(1, 2))
plot(fit.loglogist, fn = "haz", main = "",
     xlab = "Years", ylab = "Hazards")
##plot(fit.lognorm, fn = "cum", main = "Cumulative hazard function")
##par(oldpar)
``` 

### The Gompertz model

The *Gompertz* distribution is special in that it can be fit into both the AFT 
and the PH framework. Of course, as we have seen, this also holds for the Weibull 
distribution in a trivial way, the AFT and the PH models are the same, but for 
the Gompertz distribution, the AFT and PH approaches yield different models.

For the AFT framework to be in place in the Gompertz case, it needs to 
be formulated with a rather unfamiliar parametrization, which is called 
*the canonical parametrization* in the package `eha`. It works as follows.
The standard definition of the Gompertz hazard function is

\begin{equation*}
h_r(t; (\alpha, \beta)) = \alpha \exp(\beta t), \quad t > 0; \; \alpha > 0, -\infty < \sigma < \infty.
\end{equation*}

and it is called the *rate* parametrization in `eha`.
As noted earlier, in order for $h_r$ to determine a proper survival distribution, it 
must be required that $\beta \ge 0$. It was also noted that when $\beta = 0$, the
resulting distribution is *exponential*.

The *canonical* definition of the Gompertz hazard function is given by

\begin{equation*}
h_c(t; (\tau, \sigma)) = \frac{\tau}{\sigma} \exp\biggl(\frac{t}{\sigma}\biggr), \quad t > 0; \; \tau, \sigma > 0.
\end{equation*}

The transition from $h_r$ to $h_c$ is given by $\sigma = 1 / \beta, \, \tau = \alpha / \beta$, and 
note that this implies that the rate in the canonical form must be strictly positive. Furthermore,
the exponential special case now appears in the limit as $\sigma \rightarrow \infty$.
In practice this means that when the baseline hazard is only weakly increasing, $\sigma$ is
very large and numerical problems in the estimation process is likely to occur. 

The conclusion of all this is that the AFT Gompertz model is suitable in situations where
the intensity of an event is clearly increasing with time. A good example is adult mortality.

We repeat the PH analysis in Table \@ref(tab:gocofph8), but with the AFT model, see 
Table \@ref(tab:gocofaft8).

```{r gocofaft8, results='asis'}
fit.gaft <- aftreg(Surv(enter - 60, exit - 60, event) ~ sex + region, 
                   id = id, param = "lifeExp", dist = "gompertz", 
                   data = oldmort)
cap = "Old age mortality (60-100), Gompertz AFT model."
lab = "gocofaft8"
fit.out(fit.gaft, caption = cap, label = lab)
```

The expected remaining life at 60 for a man living in the town is *16 years*, 
and for a woman living in the town the expected remaining life is $1.067 \times 16 = 17$ years.

## Proportional hazards or AFT model?

The problem of choosing between a proportional hazards and an accelerated
failure time model (everything else equal) can be solved by comparing the
AIC\index{AIC} [@akaike; @akaike74] of the models. Since the numbers of parameters are equal in
the two 
cases, this amounts to comparing the maximized likelihoods. For instance,
in the case with *old age mortality*:

Let us see what happens with the *Gompertz* AFT model:
Exactly the same procedure as with the *Weibull* distribution, but we have to
specify the Gompertz distribution in the call (remember, the *Weibull*
distribution is the default choice, both for `phreg` and `aftreg`).

```{r repe8}
fit.g <- phreg(Surv(enter, exit, event) ~ sex + region, data = oldmort, 
               dist = "gompertz")
```


Comparing the corresponding result for the proportional hazards and the AFT
models with the Gompertz distribution,
we find that the maximized log likelihood in the former case is
`r round(fit.g$loglik[2], 1)`, compared to
`r round(fit.gaft$loglik[2], 1)` for the latter. This indicates that the
proportional hazards model
  fit is better. Note however that we cannot formally test the proportional
  hazards hypothesis; the two models are not nested.

<!--
A graphical method to compare PH and AFT fits is to plot two cumulative
hazards on a *log-log* scale: A vertical shift implies a PH model, while a 
horizontal shift indicates an AFT model. This is so, because in the AFT case,

\begin{equation*}
\log\bigl\{H\bigl(\log(ct)\bigr)\bigr\} = 
\log\bigl\{H\bigl(\log(c) + log(t)\bigr)\bigr\}.
\end{equation*}
-->

## Discrete time models

There are two ways of looking at discrete duration data; either time is truly
discrete, for instance the number of trials until an event occurs, or an
approximation due to rounding of continuous time data. In a sense all data
are discrete, because it is impossible to measure anything on a continuous
scale with infinite precision, but from a practical point of view it is
reasonable to say that data is discrete when tied events occur
embarrassingly often. 

### Data formats: wide and long

When working with register data, time is often measured in years which
makes it necessary and convenient to work with discrete models. A typical
data format is the so-called *wide* format, where there is one record
(row) per individual, and measurements for many years. We have so far only
worked with the *long* format. The data sets created by `survSplit`
are in long format; there is one record per individual and age category. 
The R work horse in switching back and forth between the long and wide
formats is the function `reshape`. \index{Functions!\fun{reshape}} 
It may look confusing at first, but
if data follow some simple rules, it is quite easy to use `reshape`.

The function `reshape` is typically used with *longitudinal data*,
where there are several measurements at different time points for each
individual. If the data for one individual is registered within one record
(row), we say that data are in wide format, and if there is one record (row)
per time (several records per individual), data are in long format. Using
wide format, the rule is that time-varying variable names must end in a
numeric value indicating at which time the measurement was taken. For
instance, if the variable `civ` (civil status) is noted at times 1, 2,
and 3, there must be variables named `civ.1, civ.2, civ.3`,
respectively. It is optional to use any *separator* between the base
name (`civ`) and the time, but it should be one character or empty. The
"." is what `reshape` expects by default, so using that form
simplifies coding somewhat.

We start by creating an example data set as an illustration. This is
accomplished by starting off with the data set `oldmort` in `eha`
and "trimming" it.

```{r trimort6, echo = TRUE}
data(oldmort)
om <- oldmort[oldmort$enter == 60, ]
om <- age.window(om, c(60, 70))
om$m.id <- om$f.id <- om$imr.birth <- om$birthplace <- NULL
om$birthdate <- om$ses.50 <- NULL
om1 <- survival::survSplit(om, cut = 61:69, start = "enter", end = "exit", 
                 event = "event", episode = "agegrp")
om1$agegrp <- factor(om1$agegrp, labels = 60:69)
om1 <- om1[order(om1$id, om1$enter), ]
rownames(om1) <- 1:NROW(om1)
om1$id <- as.numeric(as.factor(om1$id))
head(om1)
```

This is the long format, each individual has as many records as "presence
ages". For instance, person No. 1 has four records, for the ages 60--63.
The maximum possible No. of records for one individual is 10. We can check
the distribution of No. of records per person by using the function 
  `tapply`: 
```{r recspp6, echo = TRUE}
recs <- tapply(om1$id, om1$id, length)
table(recs)
``` 
It is easier to get to grips with the distribution with a graph, in this
case a *barplot*\index{Functions!\fun{barplot}}, see Figure \@ref(fig:barp6).

```{r barp6, fig.cap = "Barplot of the number of records per person."}
barplot(table(recs))
``` 

Now, let us turn `om1` into a data frame in *wide* format. This is
done with the function `reshape`. First we remove the redundant
variables `enter` and `exit`. 
```{r wideform6, echo = TRUE}
om1$exit <- om1$enter <- NULL
om2 <- reshape(om1, v.names = c("event", "civ", "region"), 
               idvar = "id", direction = "wide", 
               timevar = "agegrp")
names(om2)
``` 
Here there are two time-fixed variables, `id` and `sex`, and three
time-varying variables, `event`, `civ`, and `region`. The
time-varying variables have suffix of the type `.xx`, where `xx` varies
from 60 to 69. 
  
This is how data in wide format usually show up; the suffix may start with
something else than `.`, but it must be a single character, or nothing. 
The real problem is how to switch from wide format to long, because our
survival analysis tools want it that way. The solution is to use
`reshape` again, but with other specifications.

```{r reshap26, echo = TRUE}
om3 <- reshape(om2, direction = "long", idvar = "id", 
               varying = 3:32)
head(om3)
``` 

There is a new variable `time` created, which goes from 60 to 69, one
step for each of the ages. We would
like to have the file sorted primarily by `id` and secondary by time.

```{r sortin6, echo = TRUE}
om3 <- om3[order(om3$id, om3$time), ]
om3[1:11, ]
``` 
Note that all individuals got 10 records here, even those who only are
observed for fewer years. Individual No. 1 is only observed for the ages
60--63, and the next six records are redundant; they will not be used in an
analysis if kept, so it is from a practical point of view a good idea to
remove them.
```{r remove6, echo = TRUE}
NROW(om3)
om3 <- om3[!is.na(om3$event), ]
NROW(om3)
``` 
The data frame shrunk to almost half of what it was originally. First, let
us summarize data.
```{r summm6, echo = TRUE}
summary(om3)
``` 
The key variables in the discrete time analysis are `event` and 
  `time`. For the baseline hazard we need one parameter per value of 
  `time`, so it is practical to transform the continuous variable `time`
to a factor.

```{r turn6, echo = TRUE}
om3$time <- as.factor(om3$time)
summary(om3)
```
The summary now produces a frequency table for `time`.

*Note* that we always want our data to be in *long* format before we start
the analysis, so the important lesson here was how to go from wide to long.
You may find the **tidyr** package [@tidyr] useful if you encounter "untidy" 
data and want to tidy up. 

### Binomial regression with glm

For a given time point and a given individual, the response is whether an
event has occurred or not, that is, it is modeled as a 
\emph{Bernoulli}\index{Distributions!Bernoulli} outcome, which is a special
case of the \emph{binomial} distribution\index{Distributions!binomial}.
The discrete time analysis may now be performed in several ways. Most
straightforward is to run a 
*logistic regression*\index{logistic regression} with `event` as
response through the basic `glm` function with 
`family = binomial(link=cloglog)`. The so-called 
\emph{cloglog} link\index{cloglog link} is
used in order to preserve the proportional hazards property in the underlying, 
real world, continuous time model.

```{r glmreg6, echo = TRUE}
fit.glm <- glm(event ~ sex + civ + region + time, 
               family = binomial(link = cloglog), data = om3)
summary(fit.glm)
``` 
This output is not so pleasant (we do not want the `time` estimates printed), 
but we can anyway see that females (as
usual) have lower mortality than males, that married are better off than
unmarried, and that regional differences maybe are not so large. To get a
better understanding of the statistical significance of the findings we run
`drop1`\index{Functions!\fun{drop1}} on the fit.
```{r glmdrop6}
drop1(fit.glm, test = "Chisq")
``` 
Mildly surprisingly, `civil status` is not that statistically
significant, but `region` (and the other variables) is. The strong
significance of the time variable is of course expected; mortality is
expected to increase with increasing age.

<!--
An equivalent way, with a nicer printed output, is to use the function
\fun{glmmboot}\index{Functions!\fun{glmmboot}} in the package `glmmML`. 
```{r glmmboot6}
fit.boot <- glmmML::glmmboot(event ~ sex + civ + region, cluster = time, 
                     family = binomial(link = cloglog), 
                     data = om3)
fit.boot
``` 
The parameter estimates corresponding to `time` are contained in the
variable $fit\$frail$. They need to be transformed to get the "baseline
hazards". 
```{r calchaz6, echo = TRUE}
haz <- plogis(fit.boot$frail)
haz
``` 
A plot of the hazard function is shown in Figure \@ref(fig:plothaz6).

```{r plothaz6, fig.cap = "Baseline hazards, old age mortality."}
barplot(haz, names.arg = levels(om3$time))
``` 
-->

### Survival analysis with coxreg

By some data manipulation we can also use the function `coxreg` in the package 
`eha` for the analysis. For
that to succeed we need intervals as responses, and the way of doing that
is to add two "fake" variables, `exit` and `enter`. The latter must be
*slightly* smaller than the former: 

```{r mlreg8, echo = TRUE, results = "asis"}
om3$exit <- as.numeric(as.character(om3$time))
om3$enter <- om3$exit - 0.1
cap = "Old age mortality, discrete time analysis."
fit.ML <- coxreg(Surv(enter, exit, event) ~ sex + civ + region, 
                 method = "ml", data = om3, coxph = FALSE)
fit.out(fit.ML, caption = cap, label = "mlreg8")
``` 
\index{Functions!\fun{as.character}}\index{Functions!\fun{as.numeric}}
Plots of the cumulative hazards and the survival function are easily
achieved, see Figures \@ref(fig:cumML6) and \@ref(fig:surML6).

```{r cumML6, fig.cap = "The cumulative hazards, from the coxreg fit."}
plot(fit.ML, fn = "cum", xlim = c(60, 70))
``` 


```{r surML6, fig.cap = "The survival function, from the coxreg fit."}
plot(fit.ML, fn = "surv", xlim = c(60, 70))
``` 

Finally, the proportional hazards assumption can be tested in the discrete
time framework by creating an interaction between `time` and the
covariates in question. It is possible by using `glm`.

```{r testph6, echo = TRUE}
fit2.glm <- glm(event ~ (sex + civ + region) * time, 
                family = binomial(link = cloglog), 
                data = om3)
drop1(fit2.glm, test = "Chisq")
``` 

There is no sign of non-proportionality, that is, no interaction with `time`.
