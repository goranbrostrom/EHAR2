# Explanatory Variables and Regression

```{r setupcovars,include=FALSE}
par(las = 1) # Doesn't work? Later?
knitr::opts_chunk$set(comment = "", message = TRUE, echo = TRUE, cache = FALSE)
options(show.signif.stars = FALSE)
library(eha)
```

This chapter deals with the various forms explanatory variables can take and
their possible interplay. In principle, this applies to all forms of
regression analysis, but here we concentrate on survival models.


Explanatory variables, or *covariates*\index{covariate}, may be of
essentially two 
different types, *continuous* and *discrete*. The discrete type
usually takes only a finite number of distinct values and is called a
`factor` in **R**. A special case of a factor is one that takes
only two distinct values, say 0 and 1. Such a factor is called an
*indicator*, because we can let the value 1 indicate the presence of a
certain property and 0 denote its absence. To summarize, there is 

*    **Covariate**: taking values in an *interval* (*age*, *blood pressure*, *weight*).
*    **Factor**: taking a *finite* number of values (*civil status*, *occupation*, *cohort*).\index{factor}
*    **Indicator**: a factor taking *two* values (*sex*, *born in Ume√•*).
 
 Usually, we do not distinguish between factors and indicators, they are all factors.

## Continuous Covariates

We use the qualifier *continuous* to stress that factors are
excluded, because often the term *covariate* is used as a synonym for
*explanatory variable*. 

Values taken by a continuous covariate are *ordered*. The *effect* on the 
response is by model definition ordered in
the *same* or *reverse* order.
On the other hand, values taken by a factor are 
*unordered*\index{factor!unordered} (but may be defined as 
  ordered\index{factor!ordered} in **R**). 

## Factor Covariates

An explanatory variable that can take only a finite (usually small) number
of distinct values is called a 
*categorical variable*\index{categorical variable}. In **R**
language, it is called a 
*factor*\index{factor}. Examples of such variables are
  *gender*, *socio-economic status*, *birth place*. Students of
statistics have long been taught to create *dummy variables* in such
situations, in the following way: 

*    Given a categorical variable $F$ with $(k+1)$ levels $(f_0, f_1, f_2, \ldots f_k)$ ($k+1$ levels),
*    Create $k$ *indicator* (``dummy'') variables $(I_1, I_2, \ldots I_k)$.

<!--
\begin{tabular}{c|cccccc|l}
$F$   & $I_1$ & $I_2$ & $I_3$ & $\cdots$ & $I_{k-1}$ & $I_k$ & Comment\\ \hline
$f_0$ &  0    &  0    &  0    & $\cdots$ & 0         & 0  & *reference}   \\
$f_1$ &  1    &  0    &  0    & $\cdots$ & 0         & 0     \\
$f_2$ &  0    &  1    &  0    & $\cdots$ & 0         & 0     \\
$f_3$ &  0    &  0    &  1    & $\cdots$ & 0         & 0     \\
$\cdots$ &  $\cdots$  &  $\cdots$  &  $\cdots$  & $\cdots$ & $\cdots$
& $\cdots$     \\
$f_{k-1}$ &  0    &  0    &  0    & $\cdots$ & 1         & 0 \\
$f_k$ &  0    &  0    &  0    & $\cdots$ & 0         & 1
\end{tabular}

-->
The level $f_0$ is the reference category, characterized by that all indicator
variables are zero for an individual with this value. Generally, for
the level, $f_i, \; i = 1,\ldots, k$, the indicator variable $I_i$ is one,
the rest are zero. In other words, for a single individual, at most one
indicator is one, and the rest are zero.

In **R**, there is no need to explicitly create dummy variables, it is done
behind the scenes by the functions 
`factor`\index{Functions!factor} and 
`as.factor`\index{Functions!as.factor}.

Note that a factor with *two* levels, i.e., an indicator
variable\index{indicator variable}, can always be treated as a continuous
covariate, if coded 
numerically (e.g., 0 and 1).

```{example, name="Infant mortality and age of mother",echo=TRUE}
```
Consider a demographic example, the influence of mother's age (a continuous covariate) on infant mortality.
It is considered well-known that a *young* mother means high risk for
the infant, and
also that *old* mother means high risk, compared to
*"in-between-aged"* mothers. So the risk order is not the same (or
reverse) as the age order. 

One solution (not necessarily the best) to this problem is to *factorize*: Let, for instance,

\begin{equation*}
\mbox{mother's age} = \left\{\begin{array}{ll}
                           \mbox{low}, & 15 < \mbox{age} \le 25 \\
                           \mbox{middle}, & 25 < \mbox{age} \le 35 \\
                           \mbox{high}, & 35 < \mbox{age} \le 50
                            \end{array} \right.
\end{equation*}

In this layout, there will be two parameters measuring the deviation from
the reference category, which will be the first category by default.

In **R**, this is easily achieved with the aid of the 
`cut`\index{Functions!cut} function. It
works like this, illustrating it with the data set `child`:

```{r cut4}

age.group <- cut(child$m.age, c(15, 25, 35, 51))
table(age.group, useNA = "ifany")      
```

Note that the created intervals by default are closed to the right and open
to the left. This has consequences for how observations exactly on a
boundary are treated; they belong to the lower-valued interval. The
argument `right` in the call to `cut` can be used switch this
behaviour the other way around. 

Note further that values falling below the smallest value (15 in our
example) or above the largest value (51) are reported as *missing values*\index{missing values}
(`NA` in **R** terminology, `Not Available`).
For further information about the use of the `cut` function, see its
help page.

An infant mortality analysis with the created factor covariate may look like this:

```{r factage4}
ch <- child
ch$age.group <- age.group
ch <- age.window(ch, c(0, 1)) # Right censor at age one.
fit <- coxreg(Surv(enter, exit, event) ~ age.group, data = ch)
print(summary(fit), short = TRUE)
```

Obviously, it is safest for the infants whose mothers' age lies in the span 25--35 
years of age.

$\Box$


## Interactions

The meaning of *interaction* between two explanatory variables is
described by an example, walking through the three possible combinations of covariate types.


```{example, label = "oldmort03", name = "Old age mortality", echo=TRUE}
```


We return to the `oldmort` data set, slightly modified, see Table \@ref(tab:ommod), 
where the first five of a total to 6495 rows are shown. 

```{r ommod, echo = FALSE}
om <- age.window(oldmort, c(60, 85))
om$farmer <- om$ses.50 == "farmer"
om$farmer <- factor(om$farmer, labels = c("no", "yes"))
om <- om[, c("birthdate", "enter", "exit", "event", "sex", "farmer", "imr.birth")]
knitr::kable(head(om, 5), booktabs = TRUE, caption = "Old age mortality, Sundsvall 1860-1880", row.names = FALSE)
```

The sampling frame is a rectangle in the Lexis diagram, see Figure \@ref(fig:lexmortfour). It can be described as all persons born between January 1, 1775 and January 1, 1821, and present in the solid rectangle at some moment.

```{r lexmortfour, fig.cap = "Old age mortality sampling frame, Lexis diagram.", echo = FALSE, fig.height = 5,fig.width=5}
plot(c(1860, 1880), c(60, 60), type = "l", ylim = c(0, 100), xlim = c(1770, 1890), axes = FALSE, lwd = 1.5, xlab = "Year", ylab = "Age")
axis(1, at = c(1775, 1821, 1860, 1881))
axis(2, at = c(0, 60, 85, 100), las = 1)
box()
lines(c(1860, 1881), c(85, 85), lwd = 1.5)
lines(c(1860, 1860), c(60, 85), lwd = 1.5)
lines(c(1881, 1881), c(60, 85), lwd = 1.5)
##
abline(h = c(60, 85), lty = 3)
abline(v = c(1860, 1881), lty = 3)
lines(c(1775, 1860), c(0, 85), lty = 2)
lines(c(1821, 1881), c(0, 60), lty = 2)
abline(h = 0)
```


There are four possible explanatory variables behind survival after age 60 in the example. 

\begin{eqnarray*}
\mbox{farmer} & = & \left\{ \begin{array}{ll}
             \mbox{no} & \\
             \mbox{yes} & \left(e^{\alpha}\right)
            \end{array} \right. \\
%\end{equation*}
%\begin{equation*}
\mbox{sex} & = & \left\{ \begin{array}{ll}
             \mbox{male} & \\
             \mbox{female} & \left(e^{\beta}\right)
            \end{array} \right. \\
\mbox{birthdate } (x_1) & & \mspace{90mu} \left(e^{\gamma_1 x_1}\right) \\
\mbox{IMR at birth } (x_2) & & \mspace{90mu} \left(e^{\gamma_2 x_2}\right) \\
\end{eqnarray*}
The two first are factors, and the last two are continuous covariates. We
go through the three distinct combinations and illustrate the meaning of
interaction in each. $\Box$


### Two factors

Assume that we, in the oldmort example, only have the factors `sex` and `farmer` at our 
disposal as explanatory variables. We may fit a Cox regression model like this:

```{r coxtwofacfit, comment = NA}
fit <- coxreg(Surv(enter, exit, event) ~ sex + farmer, data = om)
```

The output from `coxreg` (the *model fit*) is saved in the object `fit`, which is investigated,
first by the the function `summary`, which here performs successive $\chi^2$ tests of the significance of 
the estimated regression coefficients (the null hypothesis in each case is that the true coefficient 
is zero). 

```{r coxtwofacfit4}
print(summary(fit), short = TRUE)
```


The $p$-value corresponding to the coefficient for `sex` is *very* small, meaning that there 
is a highly significant difference in mortality between women and men. The difference in mortality 
between farmers and non-farmers is also statistically significant, but with a larger $p$-value.

Next, we want to see the *size* of the difference in mortality between women and men, and between farmers 
and non-farmers, so look at the parameter estimates, especially in the column 
"Rel. Risk". It tells us that female mortality is 79.3 per cent of the male mortality, and that the farmer
mortality is 88.0 per cent of the non-farmer mortality. Furthermore, this model 
is *multiplicative* (additive on the log scale), so we can conclude that the mortality of a female farmer is 
$0.793 \times 0.880 \times 100$ = `r round(0.793 * 0.880 * 100, 1)` per cent of that of a male non-farmer.

We can also illustrate the sex difference graphically by a stratified analysis, 
see Figure \@ref(fig:coxtwofaccoefgr).

```{r coxtwofaccoefgr, fig.cap = "Cumulative hazards for men and women.", fig.height = 3}
par(las = 1)
fit2 <- coxreg(Surv(enter, exit, event) ~ strata(sex) + farmer, 
              data = om)
plot(fit2, fun = "cumhaz", xlim = c(60, 85), 
     lty = 1:2, xlab = "Age")
abline(h = 0)
```

Obviously the proportionality assumption is well met.


An obvious question to ask is: Is the sex difference in mortality the same among farmers as among non-farmers? In other words: Is there an *interaction* between sex and farmer/non-farmer regarding the effect on mortality?

In order to test for an interaction in **R**, the plus ($+$) sign in the formula is changed to a multiplication sign ($*$):

```{r multsign}
fit4 <- coxreg(Surv(enter, exit, event) ~ sex * farmer, 
              data = om)
print(x <- summary(fit4), short = TRUE)
```

Note first that the output looks different when interactions are present: The first part is a chisquare test of the significance of the interaction(s), then the usual table with parameter coefficients are presented with one difference: The Wald, and not the LR p-values are given. The small LR $p$-value (`r round(x$coefficients[3, 5], 3) * 100` per cent) indicates that there is a *statistically* significant (at the 5 per cent level) interaction effect. The *size* of the interaction effect is seen in the table of estimated coefficients. Note that the reference is a *non-farmer man*,  so the "main" effect for `sex == female` is in fact comparing a non-farming woman to a non-farming man with the result that the woman has a mortality that is $74.3$ per cent of that of a corresponding man. When comparing a farming woman to a farming man, we have to take the interaction into account, and the result is that a farming woman has a risk that is $0.743 \times  1.338 \times 100 = 99.4$ per cent of the risk for the farming man. Conclusion: Among farmers, the sex difference in mortality is negligible, but in the rest of the population it is large. How the *causality* works here is of course a completely different matter. We may guess that farmers are often married, for instance, we do not find any unmarried female farmers in our data.

In cases like this, it is often best to perform separate analyses for women and men, or, alternatively, for farmers and non-farmers. Do that as an exercise and interpret your results.


### One factor and one continuous covariate

Now the covariates `sex` (factor) and `birthdate` (continuous) are in focus. First the model without interaction is fitted.

```{r facconmul}
fit5 <- coxreg(Surv(enter, exit, event) ~ sex + birthdate, data = om)
print(summary(fit5), short = TRUE)
```

Obviously `sex` is statistically significant, while `birthdate` is not. We go 
directly to the estimated coefficients after adding an interaction term to the model:

```{r facconmulint}
fit6 <- coxreg(Surv(enter, exit, event) ~ sex * birthdate, data = om)
round(summary(fit6)$coefficients[, 1:2], 5)
```

What is going on here? The main effect of `sex` is huge! The answer lies in the 
interpretation of the coefficients: 
(i) To evaluate the effect on mortality for an 
individual with given values of the covariates, we must involve the interaction
coefficient. For instance, a female born on January 2, 1801 (`birthdate` = `r round(toTime("1801-01-02"), 3)`)
will have the relative risk 

$$\exp(9.12285 - 0.00327 \times 1801.003 - 0.00519 \times 1801.003) = 0.00185$$

compared to a man with `birthdate`= 0, the reference values! Obviously, it is ridiculous to extrapolate the model 1800 years back in time, but this is nevertheless the correct way to interpret the coefficients. So, at `birthdate` = 0, women's mortality is 9162 times higher than that of men!
(ii) With interactions involved, it is strongly recommended to *center* continuous covariates. In our example, we could do that by subtracting 1810 (say) from `birthdate`. That makes `sex = "male", birthdate = 1800` the new *reference values*. The result is

```{r centering, echo = FALSE}
om$birthdate <- om$birthdate - 1810
fit7 <- coxreg(Surv(enter, exit, event) ~ sex * birthdate, data = om)
(res <- round(summary(fit7)$coefficients[, 1:2], 5))
```

```{r echo=FALSE}
int0 <- 0
int1 <- res[1, 1]
sl0 <- res[2, 1]
sl1 <- sl0 + res[3, 1]
```

This is more reasonable-looking: At the beginning of the year 1810, woman had a mortality of 76.8 per cent of that of men.

To summarize: The effects can be illustrated by two curves, one for men and one for women. The coefficients for `sex`, 0 and `r int1`, are the respective intercepts, and `r sl0` and 
`r sl1` are the slopes. See Figure \@ref(fig:figgraphill3) for a graphical illustration.

```{r figgraphill3, fig.cap="The effect of birthdate on relative mortality for men and women. Reference: Men, birthdate = 1810.", echo=FALSE, fig.height = 4}
x <- seq(min(om$birthdate), max(om$birthdate), length = 1000)
y0 <- exp(int0 + x * sl0)
y1 <- exp(int1 + x * sl1)
plot(x, y0, type = "l", ylim = c(min(y0, y1), max(y0, y1)), lty = 4, xlab = "Birthdate.", ylab = "Relative risk", axes = FALSE)
att <- c(-30, -20, -10, 0, 10)
axis(1, at = att, labels = att + 1810)
axis(2, las = 1)
box()
lines(x, y1, lty = 2)
points(0, 1)
abline(h = 1)
abline(v = 0)
text(-10, 1.06, "Men")
text(-10, 0.89, "Women")
```


### Two continuous covariates

In our example data, the covariates `birthdate` and `imr.birth` are continuous, and we get

```{r botcont}
fit8 <- coxreg(Surv(enter, exit, event) ~ birthdate * imr.birth, data = om)
(res <- round(summary(fit8)$coefficients[, 1:2], 5))
```

The interpretation of the coefficients are: The reference point is `birthdate = 0` (1810) and `imr.birth = 0`, and if `birthdate = x` and `imr.birth = y`, then the relative risk is  
$$ \exp(-0.03024 x + 0.02004 y + 0.00158 x y),$$
analogous to the calculation in the case with one continuous and one factor covariate.

## Interpretation of parameter estimates

In the proportional hazards model the parameter estimates are logarithms of
risk ratios relative to the baseline hazard. The precise interpretations of
the coefficients for the two types of covariates are discussed. 
The conclusion is that $e^\beta$ has a more direct and intuitive
interpretation than $\beta$ itself.

### Continuous covariate 

If $x$ is a continuous covariate, and
$h(t; x) = h_0(t)e^{\beta x}$,
then

\begin{equation*}
\frac{h(t; x+1)}{h(t; x)} = \frac{h_0(t)e^{\beta (x+1)}}
{h_0(t)e^{\beta x}} = e^\beta.
\end{equation*}
so the risk increases with a factor $e^\beta$, when $x$ is
increased by one unit. In other words,
$e^\beta$ is a *relative risk*\index{relative risk} (or a *hazard
  ratio*\index{hazard ratio}, which often is a preferred term in certain professions).

### Factor

For a factor covariate, in the usual coding with a reference category,
$e^\beta$ is the relative risk compared to that reference
category.


## Model selection

In regression modeling, there is often several competing models for
describing data. In general, there are no strict rules for ``correct
selection''. However, for *nested* models\index{nested model}, there
are some formal guidelines. For a precise definition of this concept, see
Appendix A. 


### Model selection in general

Some general advise regarding model selection is given here. 

*    Remember, there are no *true* models, only some *useful*
  ones. This statement is attributed to G.E.P. Box.
*    More than one model may be useful.
*    Keep *important* covariates in the model.
*    Avoid automatic stepwise procedures!
*    If interaction terms are present, the corresponding main
terms must be there. (There are some exceptions to this rule, see for instance 
the example about infant and maternal mortality in Chapter 10.)

