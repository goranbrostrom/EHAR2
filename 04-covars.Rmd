# Explanatory Variables and Regression

```{r setupcovars,include=FALSE}
par(las = 1) # Doesn't work? Later?
knitr::opts_chunk$set(comment = "", message = TRUE, echo = TRUE, cache = FALSE)
options(show.signif.stars = FALSE)
```

This chapter deals with the various forms explanatory varibles can take and
thier possible interplay. In principle, this applies to all forma of
regression analysis, but here we concentrate on survival models.


Explanatory variables, or *covariates*\index{covariate}, may be of
essentially two 
different types *continuous* and *discrete*. The discrete type
usually takes only a finite number of distinct values and is called a
`factor` in **R**. A special case of a factor is one that takes
only two distinct values, say 0 and 1. Such a factor is called an
*indicator*, because we can let the value 1 indicate the presence of a
certain property and 0 denote its absence. To summarize, there is 

*    **Covariate**: taking values in an *interval* (e.g., *age*, *blood pressure*).
*    **Factor**: taking a *finite* number of values (e.g., *civil status*, *occupation*).\index{factor}
*    **Indicator**: a factor taking *two* values (e.g., *gender*).


## Continuous Covariates

We use the qualifier *continuous* to stress that factors are
excluded, because often the term *covariate* is used as a synonym for
*explanatory variable*. 

Values taken by a continuous covariate are *ordered*. The *effect* on the 
response is by model definition ordered in
the *same* or *reverse* order.
On the other hand, values taken by a factor are 
*unordered*\index{factor!unordered} (but may be defined as 
  ordered\index{factor!ordered} in **R**). 

## Factor Covariates

An explanatory variable that can take only a finite (usually small) number
of distinct values is called a 
*categorical variable*\index{categorical variable}. In **R**
language, it is called a 
*factor*\index{factor}. Examples of such variables are
  *gender*, *socio-economic status*, *birth place*. Students of
statistics have long been taught to create *dummy variables* in such
situations, in the following way: 

*    Given a categorical variable $F$ with $(k+1)$ levels $(f_0, f_1, f_2, \ldots f_k)$ ($k+1$ levels),
*    Create $k$ *indicator* (``dummy'') variables $(I_1, I_2, \ldots I_k)$.

<!--
\begin{tabular}{c|cccccc|l}
$F$   & $I_1$ & $I_2$ & $I_3$ & $\cdots$ & $I_{k-1}$ & $I_k$ & Comment\\ \hline
$f_0$ &  0    &  0    &  0    & $\cdots$ & 0         & 0  & *reference}   \\
$f_1$ &  1    &  0    &  0    & $\cdots$ & 0         & 0     \\
$f_2$ &  0    &  1    &  0    & $\cdots$ & 0         & 0     \\
$f_3$ &  0    &  0    &  1    & $\cdots$ & 0         & 0     \\
$\cdots$ &  $\cdots$  &  $\cdots$  &  $\cdots$  & $\cdots$ & $\cdots$
& $\cdots$     \\
$f_{k-1}$ &  0    &  0    &  0    & $\cdots$ & 1         & 0 \\
$f_k$ &  0    &  0    &  0    & $\cdots$ & 0         & 1
\end{tabular}

-->
The level $f_0$ is the reference category, characterized by that all indicator
variables are zero for an individual with this value. Generally, for
the level, $f_i, \; i = 1,\ldots, k$, the indicator variable $I_i$ is one,
the rest are zero. In other words, for a single individual, at most one
indicator is one, and the rest are zero.

In **R**, there is no need to explicitly create dummy variables, it is done
behind the scenes by the functions 
`factor`\index{Functions!factor} and 
`as.factor`\index{Functions!as.factor}.

Note that a factor with *two* levels, i.e., an indicator
variable\index{indicator variable}, can always be treated as a continuous
covariate, if coded 
numerically (e.g., 0 and 1).

```{example, name="Infant mortality and age of mother",echo=TRUE}
```
Consider a demographic example, the influence of mother's age (a continuous covariate) on infant mortality.
It is considered well-known that a *young* mother means high risk for
the infant, and
also that *old* mother means high risk, compared to
*"in-between-aged"* mothers. So the risk order is not the same (or
reverse) as the age order. 

One solution (not necessarily the best) to this problem is to *factorize*: Let, for instance,

\begin{equation*}
\mbox{mother's age} = \left\{\begin{array}{ll}
                           \mbox{low}, & 15 < \mbox{age} \le 25 \\
                           \mbox{middle}, & 25 < \mbox{age} \le 35 \\
                           \mbox{high}, & 35 < \mbox{age}
                            \end{array} \right.
\end{equation*}

In this layout, there will be two parameters measuring the deviation from
the reference category, which will be the first category by default.

In **R**, this is easily achieved with the aid of the 
`cut`\index{Functions!cut} function. It
works like this:

```{r cut}
age <- rnorm(100, 30, 6)
age.group <- cut(age, c(15, 25, 35, 48))
summary(age.group)      
```

Note that the created intervals by default are closed to the right and open
to the left. This has consequences for how observations exactly on a
boundary are treated; they belong to the lower-valued interval. The
argument `right` in the call to `cut` can be used switch this
behaviour the other way around. 

Note further that values falling below the smallest value (15 in our
example) or above the largest value (48) are reported as *missing values*\index{missing values}
(`NA` in **R** terminology, `Not Available`).

For further information about the use of the `cut` function, see the
help page. $\Box$


## Interactions

The meaning of *interaction* between two explanatory variables is
described by an example, walking through the three possible combinations of covariate types.


```{example, label = "oldmort03", name = "Old age mortality", echo=TRUE}
```


We return to the `oldmort` data set, slightly modified, see Table \@ref(tab:ommod), 
where the first five of a total to 6495 rows are shown. 

```{r ommod, echo = FALSE}
om <- age.window(oldmort, c(60, 85))
om$farmer <- om$ses.50 == "farmer"
om$farmer <- factor(om$farmer, labels = c("no", "yes"))
om <- om[, c("birthdate", "enter", "exit", "event", "sex", "farmer", "imr.birth")]
knitr::kable(head(om, 5), booktabs = TRUE, caption = "Old age mortality, Sundsvall 1860-1880", row.names = FALSE)
```

The sampling frame is a rectangle in the Lexis diagram, see Figure \@ref(fig:lexmort). It can be described as all persons born between January 1, 1775 and January 1, 1881, and present in the solid rectangle at some moment.

```{r lexmort, fig.cap = "Old age mortality sampling frame, Lexis diagram.", echo = FALSE}
plot(c(1860, 1880), c(60, 60), type = "l", ylim = c(0, 100), xlim = c(1770, 1890), axes = FALSE, lwd = 1.5, xlab = "Year", ylab = "Age")
axis(1, at = c(1775, 1821, 1860, 1881))
axis(2, at = c(0, 60, 85, 100), las = 1)
box()
lines(c(1860, 1881), c(85, 85), lwd = 1.5)
lines(c(1860, 1860), c(60, 85), lwd = 1.5)
lines(c(1881, 1881), c(60, 85), lwd = 1.5)
abline(h = c(60, 85), lty = 3)
abline(v = c(1860, 1881), lty = 3)
lines(c(1775, 1860), c(0, 85), lty = 2)
lines(c(1821, 1881), c(0, 60), lty = 2)
abline(h = 0)
```
There are four possible explanatory variables behind survival after age 60 in the example. 
\begin{eqnarray*}
\mbox{farmer} & = & \left\{ \begin{array}{ll}
             \mbox{no} & \\
             \mbox{yes} & \left(e^{\alpha}\right)
            \end{array} \right. \\
%\end{equation*}
%\begin{equation*}
\mbox{sex} & = & \left\{ \begin{array}{ll}
             \mbox{male} & \\
             \mbox{female} & \left(e^{\beta}\right)
            \end{array} \right. \\
\mbox{birthdate } (x_1) & & \mspace{90mu} \left(e^{\gamma_1 x_1}\right) \\
\mbox{IMR at birth } (x_2) & & \mspace{90mu} \left(e^{\gamma_2 x_2}\right) \\
\end{eqnarray*}
The two first are factors, and the last two are continuous covariates. We
go through the three distinct combinations and illustrate the meaning of
interaction in each. $\Box$


### Two factors

Assume that we, in the oldmort example, only have the factors `sex` and `farmer` at our 
disposal as explanatory variables. We may fit a Cox regression model like this:

```{r coxtwofacfit, comment = NA}
fit <- coxreg(Surv(enter, exit, event) ~ sex + farmer, data = om)
summary(fit)
```


The output from `coxreg` (the *model fit*) is saved in the object `fit`, which is investigated,
first by the the function `summary`, which here performs successive $\chi^2$ tests of the significance of 
the estimated regression coefficients (the null hypothesis in each case is that the true coefficient 
is zero). 

The $p$-value corresponding to the coefficient for `sex` is *very* small, meaning that there 
is a highly significant difference in mortality between women and men. The difference in mortality 
between farmers and non-farmers is also statistically significant, but with a larger $p$-value.

Next, we want to see the *size* of the difference in mortality between women and men, and between farmers 
and non-farmers, so look at the parameter estimates, especially in the column 
"Rel. Risk". It tells us that female mortality is 79.3 per cent of the male mortality, and that the farmer
mortality is 88.0 per cent of the non-farmer mortality. Furthermore, this model 
is *multiplicative* (additive on the log scale), so we can conclude that the mortality of a female farmer is 
$0.793 \times 0.880 \times 100$ = `r round(0.793 * 0.880 * 100, 1)` per cent of that of a male non-farmer.

We can also illustrate the sex difference graphically by a stratified analysis, 
see Figure \@ref(fig:coxtwofaccoefgr).

```{r coxtwofaccoefgr, fig.cap = "Cumulative hazards for men and women."}
fit2 <- coxreg(Surv(enter, exit, event) ~ strata(sex) + farmer, 
              data = om)
plot(fit2, fun = "cumhaz", xlim = c(60, 85), 
     lty = 1:2, xlab = "Age")
abline(h = 0)
```

Obviously the proportionality assumption is well met.

<!--
It can also be formally tested as follows:

```{r testprop}
fit3 <- coxph(Surv(enter, exit, event) ~ sex + farmer, 
              data = om)
(pczp <- cox.zph(fit3))
```

First look at the $p$-value corresponding to *GLOBAL*: it gives an overall verdict of the proportionality
assumption. Values below 0.05 (say) are suspicious: It tells us that some variable is bad, and it is time 
to look at the individual $p$-values. There are no problems here.

As always, you should interpret these tests with care. With large data sets, you will frequently get small $p$-values, and still the graph shows a reasonable agreement with proportionality. Then do not bother about the proportionality.
-->

An obvious question to ask is: Is the sex difference in mortality the same among farmers as among non-farmers? In other words: Is there an *interaction* between sex and farmer/non-farmer regarding the effect on mortality?

In order to test for an interaction in **R**, the plus ($+$) sign in the formula is changed to a multiplication sign ($*$):

```{r multsign}
fit4 <- coxreg(Surv(enter, exit, event) ~ sex * farmer, 
              data = om)
(x <- summary(fit4, digits = 3))
```

Note first that the output looks different when interactions are present: The first part is a chisquare test of the significance of the interaction(s), then the usual table with parameter coefficients are presented with one difference: The Wald, and not the LR p-values are given. The small LR $p$-value (`r round(x$coefficients[3, 5], 3) * 100` per cent) indicates that there is a *statistically* significant (at the 5 per cent level) interaction effect. The *size* of the interaction effect is seen in the table of estimated coefficients. Note that the reference is a *non-farmer man*,  so the "main" effect for `sex == female` is in fact comparing a non-farming woman to a non-farming man with the result that the woman has a mortality that is $74.3$ per cent of that of a corresponding man. When comparing a farming woman to a farming man, we have to take the interaction into account, and the result is that a farming woman has a risk that is $0.743 \times  1.338 \times 100 = 99.4$ per cent of the risk for the farming man. Conclusion: Among farmers, the sex difference in mortality is negligible, but in the rest of the population it is large. How the *causality* works here is of course a completely different matter. We may guess that farmers are often married, for instance, we do not find any unmarried female farmers in our data.

In cases like this, it is often best to perform separate analyses for women and men, or, alternatively, for farmers and non-farmers. Do that as an exercise and interpret your results.


### One factor and one continuous covariate

Now the covariates `sex` (factor) and `birthdate` (continuous) are in focus. First the model without interaction is fitted.

```{r facconmul}
fit5 <- coxreg(Surv(enter, exit, event) ~ sex + birthdate, data = om)
summary(fit5)
```

Obviously `sex` is statistically significant, while `imr.birth` is not. We go directly to the estimated coefficients after adding an interaction term to the model:

```{r facconmulint}
fit6 <- coxph(Surv(enter, exit, event) ~ sex * birthdate, data = om)
round(summary(fit6)$coefficients[, 1:2], 5)
```

What is going on here? The main effect of `sex` is huge! The answer lies in the interpretation of the coefficients: 
(i) To evaluate the effect on mortality for an 
individual with given values of the covariates, you must involve the interaction coefficient. For instance, a female born on January 2, 1801 (`birthdate` = `r round(toTime("1801-01-02"), 3)`)
will have the relative risk 

$$\exp(9.12285 - 0.00327 \times 1801.003 - 0.00519 \times 1801.003) = 0.00185$$

compared to a man with `birthdate`= 0, the reference values! Obviously, it is ridiculous to extrapolate the model 1800 years back in time, but this is nevertheless the correct way to interpret the coefficients. So, at `birthdate` = 0, women's mortality is 9162 times higher than that of men!
(ii) With interactions involved, it is strongly recommended to *center* continuous covariates. In our example, we could do that by subtracting 1810 (say) from `birthdate`. That makes `sex = "male", birthdate = 1800` the new *reference values*. The result is

```{r centering, echo = FALSE}
om$birthdate <- om$birthdate - 1810
fit7 <- coxph(Surv(enter, exit, event) ~ sex * birthdate, data = om)
(res <- round(summary(fit7)$coefficients[, 1:2], 5))
```

```{r echo=FALSE}
int0 <- 0
int1 <- res[1, 1]
sl0 <- res[2, 1]
sl1 <- sl0 + res[3, 1]
```

This is more reasonable-looking: At the beginning of the year 1810, woman had a mortality of 76.8 per cent of that of men.

To summarize: The effects can be illustrated by two curves, one for men and one for women. The coefficients for `sex`, 0 and `r int1`, are the respective intercepts, and `r sl0` and 
`r sl1` are the slopes. See Figure \@ref(fig:figgraphill3) for a graphical illustration.

```{r figgraphill3, fig.cap="The effect of birthdate on relative mortality for men and women. Reference: Men, birthdate = 1810.", echo=FALSE, fig.height = 4}
x <- seq(min(om$birthdate), max(om$birthdate), length = 1000)
y0 <- exp(int0 + x * sl0)
y1 <- exp(int1 + x * sl1)
plot(x, y0, type = "l", ylim = c(min(y0, y1), max(y0, y1)), lty = 4, xlab = "Birthdate.", ylab = "Relative risk", axes = FALSE)
att <- c(-30, -20, -10, 0, 10)
axis(1, at = att, labels = att + 1810)
axis(2, las = 1)
box()
lines(x, y1, lty = 2)
points(0, 1)
abline(h = 1)
abline(v = 0)
text(-10, 1.06, "Men")
text(-10, 0.89, "Women")
```


### Two continuous covariates

In our example data, the covariates `birthdate` and `imr.birth` are continuous, and we get

```{r botcont}
fit8 <- coxph(Surv(enter, exit, event) ~ birthdate * imr.birth, data = om)
(res <- round(summary(fit8)$coefficients[, 1:2], 5))
```

The interpretation of the coefficients are: The reference point is `birthdate = 0` (1810) and `imr.birth = 0`, and if `birthdate = x` and `imr.birth = y`, then the relative risk is  
$$ \exp(-0.03024 x + 0.02004 y + 0.00158 x y),$$
analogous to the calculation in the case with one continuous and one factor covariate.

## Interpretation of parameter estimates

In the proportional hazards model the parameter estimates are logarithms of
risk ratios relative to the baseline hazard. The precise interpretations of
the coefficients for the two types of covariates are discussed. 
The conclusion is that $e^\beta$ has a more direct and intuitive
interpretation than $\beta$ itself.

### Continuous covariate 

If $x$ is a continuous covariate, and
$h(t; x) = h_0(t)e^{\beta x}$,
then

\begin{equation*}
\frac{h(t; x+1)}{h(t; x)} = \frac{h_0(t)e^{\beta (x+1)}}
{h_0(t)e^{\beta x}} = e^\beta.
\end{equation*}
so the risk increases with a factor $e^\beta$, when $x$ is
increased by one unit. In other words,
$e^\beta$ is a *relative risk*\index{relative risk} (or a *hazard
  ratio*\index{hazard ratio}, which often is a preferred term in certain professions).

### Factor

For a factor covariate, in the usual coding with a reference category,
$e^\beta$ is the relative risk compared to that reference
category.


## Model selection

In regression modeling, there is often several competing models for
describing data. In general, there are no strict rules for ``correct
selection''. However, for *nested* models\index{nested model}, there
are some formal guidelines. For a precise definition of this concept, see
Appendix A. 


### Model selection in general

Some general advise regarding model selection is given here. 

*    Remember, there are no \emph{true} models, only some *useful*
  ones. This statement is attributed to G.E.P.\ Box.
*    More than one model may be useful.
*    Keep \emph{important} covariates in the model.
*    Avoid automatic stepwise procedures!
*    If interaction effects are present, the corresponding main
effects *must* be there.


## Doing it in **R**
\index{Functions!coxreg|(}

We utilize the male mortality data, Skellefteå 1800--1820, to
illustrate the aspects of an ordinary Cox regression. Males aged 20 (exact)
are sampled between 1820 and 1840 and followed to death or reaching the age
of 40, when they are right censored. So in effect male mortality in the
ages 20--40 are studied. The survival time starts counting at zero for each
individual at his 21st birthdate (that is, at exact age 20 years).

There are two **R** functions that are handy for a quick look at a data
frame, `str`\index{str} and `head`\index{head}. The first give a
summary description of the *structure* of an **R** object, often a
*data frame* \index{data frame}

```{r str_cov,echo=TRUE}
library(eha) #Loads also the data frame 'mort'
str(mort)
```

```{r getd_cov,echo=FALSE}
n.obj <- NROW(mort)
```
First, there is information about the data frame: It *is* a 
`data.frame`, with six variables measured on `r n.obj` objects. Then
each 
variable is individually described: name, type, and a few of the first values.
The values are usually rounded to a few digits. The *Factor* line is
worth noticing: It is of course a `factor` covariate, it takes two
levels, `lower` and `upper`. Internally, the levels are coded 1, 2,
respectively. Then (by default), `lower` (with internal code equal to
1) is the `reference category`. If desired, this can be changed with the
`relevel` function.

```{r relevel_cov}
mort$ses2 <- relevel(mort$ses, ref = "upper")
str(mort)
```
Comparing the information about the variables `ses` and `ses2`, you
find that the codings, and also the reference categories, are
reversed. Otherwise the variables are identical, and any analysis involving
any of
them as an explanatory variable would lead to identical *conclusions*,
but not identical parameter estimates, see below.

The function `head` prints the first few lines (observations) of a data
frame  (there is also a corresponding `tail` function that prints a few
of the *last* rows). 

```{r str3_cov}
head(mort)
```
Apparently, the variables `ses` and `ses2` are identical, but we know
that behind the scenes they are formally different; different coding and
(therefore) different reference category. This shows up in analyzes
involving the two variables, one at a time.

First the analysis is run with `ses`.

```{r mortreg_cov}
res <- coxreg(Surv(enter, exit, event) ~ ses + birthdate, 
              data = mort)
res
```
Then the variable `ses2` is inserted instead.

```{r mortreg2_cov}
res2 <- coxreg(Surv(enter, exit, event) ~ ses2 + birthdate, 
               data = mort)
res2
```
Notice the difference; it is only formal. The substantial results are
completely equivalent.

### Likelihood Ratio Test

If we judge the result by the Wald $p$-values, it seems as if both
variables are highly significant. To be sure it is advisable to apply the
likelihood ratio test. In **R**, this is easily achieved by applying the 
  `drop1` function on the result, `res`,  of the Cox regression.

```{r drop_cov}
drop1(res, test = "Chisq")
```
In fact, we have performed *two* likelihood ratio tests here. First,
the effect of removing `ses` from the full model. The $p$-value for
this test is very small, $p = 0.00005998$. The scientific notation,
5.998e-05 means that the decimal point should be moved five steps to the
left (that's the minus sign). The reason for this notation is essentially
space-saving. Then, the second LR test concerns the absence or presence of
`birthdate` in the full model. This also gives a small $p$-value, $p =
0.006972$. 

The earlier conclusion is not altered; both variables are highly significant.

### The estimated baseline cumulative hazard function
\index{cumulative hazard function|(} 

The estimated baseline cumulative function (see Equation \@ref(eq:bashaz)) is best
plotted by the `plot` command, see Figure \@ref(fig:basl).

```{r basl_cov,fig=TRUE,echo=FALSE, fig.cap="Estimated cumulative baseline hazard function."}
plot(res)
```

The figure shows the baseline cumulative hazard function for a
*reference* individual (i.e., with the value zero on all covariates). 
\index{cumulative hazard function|)} 
