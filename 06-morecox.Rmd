# More on Cox Regression

```{r setupcovars6,include=FALSE}
par(las = 1) # Doesn't work? Later?
knitr::opts_chunk$set(comment = "", message = TRUE, echo = TRUE, cache = FALSE)
options(show.signif.stars = FALSE, warn = 1)
library(eha)
library(kableExtra)
```

Vital concepts like
*time-dependent covariates*, *communal covariates*, handling of
*ties*, *model checking*, *sensitivity analysis*, etc., 
are introduced in this chapter.


## Time-Varying Covariates

Only a special kind of time-varying covariates can be treated in **R** by the
packages `eha` and `survival`, and that is so-called
*piecewise constant* functions. How this is done is best described by
an example.

```{example name = "Civil status"}
```
The covariate (factor) `civil status` (called  `civst` in the **R** data
frame) is an explanatory variable in a
mortality study, which changes value from 0 to 1 at marriage. How should this be
coded in the data file? The 
solution is to create *two* records (for individuals that marry),
  each with a *fixed* value of $civ\_st$:

1.   Original record: $(t_0, t, d, x(s), t_0 < s \le t)$, married
at time $T$, $t_0 < T < t$:

$$
\text{civst}(s) = \left\{\begin{array}{ll}
             \text{unmarried} , & s < T \\
              \text{married}, & s \ge T
              \end{array}\right.
$$

2.    First new record: $(t_0, T, 0, \text{unmarried})$, *always censored*.

3.    Second new record: $(T, t, d, \text{married})$.


The data file will contain two records like (with $T = 30$) what you can see
in Table \@ref(tab:timevar).

```{r timevar,echo=FALSE}
source("R/tbl.R")
x <- data.frame(id = c(23, 23),
                enter = c(0, 30),
                exit = c(30, 80),
                event = c(0, 1),
                civst = as.factor(c("unmarried", "married"))
                )
tbl(x, caption = "The coding of a time-varying covariate.")
```


In this way, a time-varying covariate can always be handled by utilizing
left truncation\index{left truncation} and 
right censoring\index{censoring!right}. See also Figure \@ref(fig:civ) for
an illustration.  

```{r civ,fig=TRUE,echo=FALSE,fig.cap="A time-varying covariate (unmarried or married). Right censored and left truncated at age 30 (at marriage).",fig.scap="A time-varying covariate"}
#plot(rnorm(100))
plot(1, 2, type = "n", xlim = c(0, 90), ylim = c(0, 3), xlab = "Age",
     ylab = "", yaxt="n", xaxt = "n")
lines(c(0, 30), c(1,1))
points(30, 1, pch = "c", cex = 1.5)
lines(c(30, 80), c(2, 2))
points(30, 2, pch = "|")
points(83, 2, pch = "+", cex = 1.5)
text(13, 1.2, labels = "civst = unmarried", cex = 1)
text(52, 2.2, labels = "civst = married", cex = 1)
abline(v = 0)
abline(v = 30, lty = 2)
axis(1, at = c(0, 30, 60, 80))
```

And note
that this situation is formally equivalent to a situation with *two
  individuals*, one unmarried and one married. The first one is right
censored at exactly the same time as the second one enters the study (left
truncation).  $\ \Box$

A word of caution: Marriage status may be interpreted as an
*internal* covariate, i.e., the change of marriage status is
individual, and may depend on health status. For instance, maybe only
healthy persons get married. So, the risk is that *health status* acts as a 
confounder in the relation between marriage and survival. Generally, 
the use of time dependent internal covariates is dangerous, and one should always think 
of possible confounding or reverse causality taking place when allowing for it.

## Communal Covariates

Communal\index{communal covariate} (external)\index{external
  covariate} covariates are covariates that vary in time 
outside any individual, and is common to all individuals at each specific
*calendar* time. In econometric literature, such a variable is often
called *exogenous*\index{exogenous covariate}. This could be viewed
upon as a special case of a time-varying covariate, but without the danger
of reversed causality that was discussed above. 

```{example, name = "Did food prices affect mortality in the 19th century?", echo = TRUE}
```
For some 19th century parishes in southern Sweden, yearly time series of
food prices are available. In this example we use deviation from the log
trend in annual rye prices, shown in Figure \@ref(fig:logrye).


```{r logrye,fig=TRUE,echo=FALSE,fig.cap = "Log rye price deviations from trend, 19th century southern Sweden.", fig.scap="Log rye price deviations from trend", fig.height=3}
library(eha)
par(las = 1)
plot(logrye$year, logrye$foodprices, xlab = "Year", ylab = "Deviation",
     type = "s", axes =  FALSE)
axis(1)
ttts <- c(-0.4, 0, 0.4)
axis(2, at = ttts, labels = um(ttts))
box()
abline(h = 0)
```

The idea behind this choice of transformation is to focus on deviations
from the "normal" as a proxy for *short-term variation in economic stress*. 
We illustrate the idea with a subset of the built-in data set `scania`, 
and we show the information for individual No. 1.

```{r scand,echo=FALSE}
library(eha)
scand <- scania[, c("id", "enter", "exit", "event", 
                    "birthdate", "sex", "ses")]
scand[scand$id == 1, ]
```

Now, to put on the food prices as a time-varying covariate, use the function
`make.communal`. We show what happens to individual No. 1.

```{r communal}
scand <- make.communal(scand, logrye[, 2, drop = FALSE], 
                        start = 1801.75)
scand[scand$id == 1, ]
```

A new variable, `foodprices` is added, and each individual's duration
is split up in one year long intervals (except the first and last). This is
the way of treating `foodprices` as a time-varying covariate. The analysis is then 
straightforward.

```{r prepanalcomm6}
fit <- coxreg(Surv(enter, exit, event) ~ ses + sex + foodprices, 
                data = scand)
```

```{r analcomm6, results='asis', echo = FALSE}
source("R/fit.out.R")
fit.out(fit, caption = "Foodprices and old age mortality, 19th century Scania.",
        label = "analcomm6")
```

Socio-economic status and sex do not matter much, but food prices do; the
effect is almost significant at the 5 per cent level. See Table \@ref(tab:analcomm6). $\ \Box$


## Tied Event Times

Tied event times cannot, in theory, occur with continuous-time data, but it
is impossible to measure duration and life lengths with infinite
precision. Data are always more or less rounded, tied event times occur
frequently in practice. This may cause problems (biased estimates) if
occurring too frequently. There are a few ways to handle tied data, and the
so-called exact method considers all possible permutations of the tied
event times in each risk set. It works as shown in the following example.

```{example, name = "Likelihood contribution at tied event time"}
```
$R_i = \{1, 2, 3\}$, 1 and 2 are events; two
possible orderings:
  
\begin{eqnarray*}
L_i(\boldsymbol{\beta}) & = & \frac{\psi(1)}{\psi(1) + \psi(2) + \psi(3)} \times
            \frac{\psi(2)}{\psi(2) + \psi(3)} \\ & + &
\frac{\psi(2)}{\psi(1) + \psi(2) + \psi(3)} \times
            \frac{\psi(1)}{\psi(1) + \psi(3)} \\
 & = & \frac{\psi(1)\psi(2)}{\psi(1) + \psi(2) + \psi(3)}
 \left\{\frac{1}{\psi(2) + \psi(3)}
            + \frac{1}{\psi(1) + \psi(3)}\right\}
\end{eqnarray*}
$\ \Box$

The main drawback with the exact method is that it easily becomes
unacceptably slow, because of the huge number of permutations that may be
necessary to consider. It is however available in the function `coxph` in the
`survival` package as an option.

Fortunately, there are a few excellent approximations, most notably
Efron's\index{Efron's approximation}, which is the default method in most
survival packages in 
**R**. Another common approximation, due to Breslow\index{Breslow approximation}, 
is the default in some
statistical software and also possible to choose in the `eha` and 
  `survival` packages in **R**. Finally, there is no cost involved in using
these approximations in the case of no ties at all; they will all give
identical results.

With too much ties in the data, there is always the possibility to use
discrete time methods. One simple way of doing it is to use the option 
`method = "ml"` in
the `coxreg` function in the **R** package `eha`.

```{example, name = "Birth intervals"}
```
	We look at length of birth intervals
  between first and second births for married women in 19th century
  northern Sweden, a subset of the data set 'fert', available in the
  `eha`   package.  
Four runs with `coxreg` are performed, with all the possible ways of
treating ties. 

```{r 5ties, echo=TRUE}
library(eha)
first <- fert[fert$parity == 1, ]
## Default method (not necessary to specify method = "efron"
fit.e <- coxreg(Surv(next.ivl, event) ~ year + age, data = first, 
                method = "efron")
## Breslow
fit.b <- coxreg(Surv(next.ivl, event) ~ year + age, data = first, 
                method = "breslow")
## The hybrid mppl:
fit.mp <- coxreg(Surv(next.ivl, event) ~ year + age, data = first, 
                 method = "mppl", coxph = FALSE)
## True discrete:
fit.ml <- coxreg(Surv(next.ivl, event) ~ year + age, data = first, 
                 method = "ml", coxph = FALSE)
```

Then we compose a table of the four results, see Table \@ref(tab:5compt). 

```{r 5compt,echo=FALSE}
source("R/tbl.R")
out <- rbind(fit.e$coefficients, fit.b$coefficients, 
             fit.mp$coefficients, fit.ml$coefficients)
rownames(out) <- c("Efron", "Breslow", "MPPL", "ML")
colnames(out) <- c("year", "age")
tbl(um(round(out, 4)), fs = 11, rownames = TRUE,
    caption = "Comparison of four methods of handling ties, estimated coefficients.")
##knitr::kable(out, booktabs = TRUE, digits = 4, 
  ##    caption = "Comparison of four methods of handling ties, estimated coefficients.")
```


There are almost no difference in results between the four methods. For the
`exact` method mentioned above, the number of permutations is 
$\sim 7 \times 10^{192}$, which is impossible to handle.$\ \Box$

With the help of the function `risksets` in `eha`, it is easy to
check the composition of risk sets in general and the frequency of ties in
particular. 
```{r 5rs,echo=TRUE}
rs <- risksets(Surv(first$next.ivl, first$event))
tt <- table(rs$n.event)
tt
```
This output says that `r tt[1]` risk sets have only one event each (no
ties), `r tt[2]` risk sets have exactly two events each, etc.

The object `rs` is a `list` with seven components. Two of them are
`n.event`, which counts the number of events in each risk set, and `size`, 
which gives the size (number of individuals under observation) of
each risk set. Both these vectors are ordered after the `risktimes`,
which is another component of the list. For the rest of the components, see
the help page for `risksets`.

It is easy to produce the *Nelson-Aalen plot*\index{Nelson-Aalen plot}
with the output from `risksets`, see Figure \@ref(fig:nalplot6).
```{r nalplot6, echo = TRUE, fig.cap = "Nelson-Aalen plot with risksets.", fig.height = 4}
par(las = 1)
plot(rs$risktimes, cumsum(rs$n.event / rs$size), type = "s", 
     xlab = "Duration (years)", ylab = "Cum. hazards")
abline(h = 0)
```

One version of the corresponding survival function is shown in Figure \@ref(fig:kmout6).
```{r kmout6,fig.height = 3,fig.cap="Survival plot with the aid of the function risksets.",fig.scap="Survival plot with the aid of the function risksets"}
par(las = 1)
sur <- exp(-cumsum(rs$n.event / rs$size))
plot(rs$risktimes, sur, type = "s", 
     xlab = "Duration (years)", ylab = "Surviving fraction")
abline(h = 0)
```

## Stratification {#strat_6}
\index{stratification}\index{stratum}
Stratification means that data is split up in groups  called
*strata*, and a separate partial 
likelihood function is created for each stratum, but with common values on
the regression parameters corresponding to the common 
explanatory variables. In the estimation, these partial likelihoods are
multiplied together, and the product is treated as a likelihood
function. Thus, there is one restriction on the parameters, they are the same
across strata.

There are typically two reasons for stratification. First, if the
proportionality assumption does not hold for a
factor covariate, a way out is to stratify along it. Second, a factor may
have too many levels, so that it is inappropriate to treat is as an
ordinary factor. This argument is similar to the one about using a frailty
model (Chapter \@ref(causality-and-matching)). Stratification is also a useful tool with
matched data, see Chapter \@ref(competing).

When a *factor* does not produce proportional hazards between
categories, *stratify* on the categories. For tests of the
proportionality assumption, see Section 6.7.1. 

```{example name = "Birth intervals"}
```  
  For the birth interval data, we stratify on `ses` in the Cox regression:

```{r strafr6, results='asis'}
library(eha)
source("R/fit.out.R")
fert1 <- fert[fert$parity == 1, ]
levels(fert1$parish) <- c("Jörn", "Norsjö", "Skellefteå")
fert1$parish <- relevel(fert1$parish, ref = "Skellefteå")
fert1$age.25 <- fert1$age - 25       # Center
fert1$year.1860 <- fert1$year - 1860 # Center
fit <- coxreg(Surv(next.ivl, event) ~ age.25 + year.1860 +
              prev.ivl + parish + strata(ses), 
              data = fert1)
fit.out(fit, caption = "Birth intervals, 19th century Skellefteå.", 
        label = "strafr6")
```

The results in Table \@ref(tab:strafr6) show that calendar time is unimportant, 
while mother's age and the length of the previous interval, from marriage to 
first birth, both are important with birth intensity decreasing with both.
Also, note that the continuous covariates are *centered* around a reasonable value 
in the range of both. It will not affect the rest of the table in this case, but 
with *interactions* involving either it would.

Next plot the fit, see Figure \@ref(fig:bi25). The non-proportionality
pattern is clearly visible. Note the line `par(lwd = 1.5)`: It magnifies the
*line widths* by 50 per cent in the plot, compared to default. And the values on 
the $y$ axis refers to a 25-year-old mother giving birth in Skellefteå 1860.

```{r bi25, fig.cap = "Cumulative hazards by socio-economic status, second birth interval."}
par(lwd = 1.5, cex = 0.8)
plot(fit, ylab = "Cumulative hazards", xlab = "Years")
```


## Sampling of Risk Sets

Some of the work by @bl95 is implemented in **eha**. The idea
behind sampling of risk sets is that with huge data sets, in each risk sets
there will be a few (often only one) events, but many survivors. From an
efficiency point of view, not much will be lost by using only a random
fraction of the survivors in each risk set. The gain is of course
computational speed and memory saving. How it is done in **eha** is shown
by an example.

```{example, name = "Sampling of risk sets, male mortality."}
```

For the  `male mortality` data set, we compare a full analysis with one where
  only four survivors are sampled in each risk set.
```{r 5sample}
fit <- coxreg(Surv(enter, exit, event) ~ ses, 
              data = mort)
fit.4 <- coxreg(Surv(enter, exit, event) ~ ses, 
                max.survs = 4, data = mort)
f1 <- coefficients(summary(fit))[c(1, 3)]
f4 <- coefficients(summary(fit.4))[c(1, 3)]
out <- rbind(f1, f4)
colnames(out) <- c("Coef", "se(Coef)")
rownames(out) <- c("Original", "Sample")
round(out, 4)
```

The results are comparable and a gain in execution time was noted. Of
course, with the small data sets we work with here, the difference is of no
practical importance.$\ \Box$

It is worth noting that the function `risksets` has an argument
`max.survs`, which, when it is set, sample survivors for each risk set
in the data. The output can then be used as input to `coxreg`, see the
relevant help pages for more information.

## Residuals

Residuals are generally the key to judgment of model fit to data. It is
easy to show how it works with *linear regression*.

```{example name = "Linear regression"}
```

Figure \@ref(fig:6lr) shows a scatter plot of bivariate data and a fitted
straight line (left panel) and residuals versus fitted values (right panel). 

```{r 6lr, fig.cap = "Residuals in linear regression.", echo=FALSE, fig.height = 3.5}
oldpar <- par(mfrow = c(1, 2), las = 1)
x <- rnorm(100, sd = 0.5)
y <- rnorm(100, mean = x)
fit <- lm(y ~ x)
plot(x, y)
abline(fit)
plot(fit, 1)
par(oldpar)
```

The residuals are the vertical distances between the line and the
points.$\ \Box$

Unfortunately, this kind of simple and intuitive graph does not exist for results
from a Cox regression analysis. However, there are a few ideas how to
accomplish something similar. The main idea goes as follows. If $T$ is a
survival time, and $S(t)$ the corresponding survivor function (continuous
time), then it is a mathematical fact that $U = S(T)$ is uniformly
distributed on the interval $(0, 1)$. Continuing, this implies that
$-\log(U)$ is exponentially distributed with rate (parameter) 1.

It is thus shown that $H(T)$ ($H$ is the cumulative hazard function
for $T$) is exponentially distributed with rate 1. This motivates the
\emph{Cox-Snell} residuals, given a sample $T_1, \ldots, T_n$ of survival
times,

\begin{equation}
r_{Ci} = \hat{H}(T_i; \mathbf{x}_i) = \exp(\boldsymbol{\beta} \mathbf{x}_i)\hat{H}_0(T_i),\quad i =
1, \ldots, n,
\end{equation}
which should behave like a censored sample
from an exponential distribution with parameter 1, if the model is
good. If, for some $i$, $T_i$ is censored, so is the residual. 

### Martingale residuals

A drawback with the Cox-Snell residuals is that they contain censored
values. An attempt to correct the censored residuals leads to the so-called
*martingale* residuals. The idea is simple; it builds on the
"lack-of-memory" property of the exponential distribution. The expected
value is 1, and by adding 1 to the *censored* residuals, they become
predictions (estimates) of the corresponding uncensored values. then
finally a twist is added: Subtract 1 from *all* values and multiply by
$-1$, leading to the definition of the martingale residuals.

\begin{equation}
r_{Mi} = \delta_i - r_{Ci}, i = 1, \ldots, n.
\end{equation}
where $\delta_i$ is the indicator of event for the $i$:th
observation. These residuals may be 
interpreted as the *Observed* minus the *Expected* number of
events for each individual.
Direct plots of the martingale residuals tend to be less informative,
especially with large data sets. 

```{example name = "Plot of martingale residuals"} 
```
The data set `kidney` from the **survival** package is used. For more
information about it, read its help page in **R**. For each patient, two
survival times are recorded, but only one will be used here. That is
accomplished with the function `duplicated`.
```{r 6kid, echo = TRUE}
library(survival)
head(kidney)
k1 <- kidney[!duplicated(kidney$id), ]
fit <- coxreg(Surv(time, status) ~ disease + age + sex, 
              data = k1)
```
The *extractor function*\index{extractor function}
`residuals`\index{Functions!\fun{residuals}} is used to extract the 
martingale residuals from the fit.
See Figure \@ref(fig:6mres).

```{r 6mres, fig.cap = "Martingale residuals from a fit of the kidney data.",echo=FALSE}
source("R/unicode.R")
par(las = 1)
ojs <- c(-2, -1, 0, 1)
plot(residuals(fit), ylab = "Residuals", axes = FALSE)
axis(1)
axis(2, at = ojs, labels = um(ojs))
box()
```

With a large data set, the plot will be hard to interpret, see
Figure \@ref(fig:6bigdata), which shows the martingale residuals from a fit
of the *male mortality* (`mort`) data in the package **eha**.

```{r 6bigdata,fig.cap = "Martingale residuals from a fit of the male mortality data.", echo=FALSE}
data(mort)
par(las = 1)
fit <- coxreg(Surv(enter, exit, event) ~ birthdate + ses, data = mort)
plot(fit$residuals, ylab = "Residuals", axes = FALSE)
axis(1)
ojs <- c(-0.5, 0.0, 0.5, 1.0)
axis(2, at = ojs, labels = um(ojs))
box()
```

It is clear that the residuals are grouped in two clusters. The
positive ones are connected to the observations with an event, while the
negative ones are corresponding to the censored observations. It is hard to
draw any conclusions from plots like this one. However, the residuals are
used in functions that evaluate goodness-of-fit, for instance the function 
`cox.zph` in the **survival** package. $\ \Box$


## Checking Model Assumptions {#ass_6}

### Proportionality

The proportionality assumption\index{proportionality assumption} is
extremely important to check in the proportional hazards models. We have
already seen how violations of the assumption can be dealt with
(stratification).

We exemplify with the `births' data set in the **eha** package.

```{example name = "Birth intervals, proportionality assumption"}
```
  
  In order
  to keep observations reasonably independent, we concentrate on one
  specific birth interval per woman, the interval between the second and
  third birth. That is, in our sample are all women with at least two
  births, and we monitor each woman from the day of her second delivery
  until the third, or if that never happens, throughout her observable
  fertile period, i.e, to age 50 or to loss of follow-up. The latter will
  result in a right-censored interval.
```{r 6bir.prop}
fert2 <- fert[fert$parity == 2, ]
```
Then we save the fit from a Cox regression performed by the function `coxph` in the
**survival** package (important!),

```{r birfull}
fit <- survival::coxph(Surv(next.ivl, event) ~ ses + age + 
                           year + parish, 
             data = fert2)
```
and check the proportionality assumption. It is done with the function
`cox.zph`\index{Functions!\fun{cox.zph}} in the **survival** package.
```{r tabzph}
prop.full <- survival::cox.zph(fit)
prop.full
```

In the table above, look first at the last row, *GLOBAL*. It is the
result of a test of the global null hypothesis that proportionality
holds. The small $p$-value tells us that we have a big problem
with the proportionality assumption. Looking further up, we see two possible
problems, the variables `age` and `ses`. Unfortunately, `age`
is a continuous variable. A categorical 
variable\index{categorical variable} can be stratified upon, but now 
we have to categorize first. 

Let us first investigate the effect of stratifying on `ses`. 
```{r 6crefit1, echo = TRUE}
fit1 <- coxph(Surv(next.ivl, event) ~ strata(ses) + 
               age + year + parish, data = fert2) 
```
and plot the result, see Figure \@ref(fig:6strses).

```{r 6strses,fig.cap = "Stratification on socio-economic status, third birth interval data.",echo=FALSE}
par(lwd = 1.5, las = 1)
plot(survival::survfit(fit1), fun = "cumhaz", lty = 1:4)
legend("bottomright", legend = levels(fert2$ses), lty = 1:4)
```

```{r silent6, echo=FALSE}

fit1 <- survival::coxph(Surv(next.ivl, event) ~ strata(ses) + 
              age + year + parish, data = fert2) 

```
The non-proportionality is visible, some curves cross. The test of
proportionality of the stratified model shows that we still have a problem
with `age`.
```{r proage}
fit1.zph <- survival::cox.zph(fit1)
print(fit1.zph)
```
Since `age` is continuous covariate, we may have to categorize
it. First its distribution is checked with a histogram, see
Figure \@ref(fig:6histage). 

```{r 6histage,fig.cap = "The distribution of mother's age, third birth interval data."}
hist(fert2$age, main = "", xlab = "age")
```

The range of `age` may reasonably be split into four equal-length
intervals with the `cut` function.
```{r cutfour, echo = TRUE}
fert2$qage <- cut(fert2$age, 4)
fit2 <- survival::coxph(Surv(next.ivl, event) ~ strata(ses) + 
              qage + year + parish, data = fert2) 
```
and then the proportionality assumption is tested again.
```{r propagain}
fit2.zph <- survival::cox.zph(fit2)
fit2.zph
```

The result is now reasonable, with the high age group deviating
slightly. This is not so strange; fertility becomes essentially zero in the
age range in the upper forties, and therefore it is natural that the
proportionality assumption is violated. One way of dealing with the
problem, apart from stratification, would be to analyze the age groups
separately.  $\ \Box$


### Log-linearity

In the Cox regression model, the effect of a covariate on the hazard is
*log-linear*, that is, it affects the *log hazard* linearly. Sometimes a
covariate needs to be transformed to fit into the pattern. The first
question is then: Should the covariate be transformed in the analysis? A
graphical test can be done by
plotting the martingale residuals from a *null fit* against the covariate,
See Figure \@ref(fig:6loglin). 
```{r 6loglin,fig.cap = "Plot of residuals against the explanatory variable $x$.",echo=FALSE}
set.seed(101121)
par(las = 1)
x <- rnorm(100)
tid <- rexp(100, exp(1.5 * x^2))
event <- rep(1, 100)
fit <- coxph(Surv(tid, event) ~ 1)
plot(x, residuals(fit), axes = FALSE)
oj1 <- seq(-3, 2, by = 1)
axis(1, at = oj1, labels = um(oj1))
oj2 <- seq(-4, 1, by = 1)
axis(2, at = oj2, labels = um(oj2))
box()
lines(lowess(x, residuals(fit)))
```


The function `lowess`\index{Functions!\fun{lowess}} fits a "smooth"
curve to a scatterplot. 
The curvature is clearly visible, let us see what happens if we make the
plot with $x^2$ instead of $x$. (Note that we now are cheating!) See
Figure \@ref(fig:6loglin2). 

```{r 6loglin2,fig.cap = "Plot of residuals against the explanatory variable $x^2$."}
par(las = 1)
plot(x^2, residuals(fit), axes = FALSE, xlab = expression(x^2))
axis(1)
ah <- seq(-4, 1, by = 1)
axis(2, at = ah, labels = um(ah))
box()
lines(lowess(x^2, residuals(fit)))
```

This plot indicates more that an increasing $x$ is associated with an
increasing risk for the event to occur.


## Fixed Study Period Survival

Sometimes survival up to some fixed time is of interest. For instance, in
medicine five-year survival may be used as a measure of the success of a
treatment. The measure is then the probability for a patient to survive
five years after treatment.

This is simple to implement; it is a binary 
experiment\index{binary experiment}, where for
each patient survival or not survival is recorded (plus covariates, such as
treatment). The only complicating factor is incomplete observations, i.e.,
right censoring. In that case it is recommended to use ordinary Cox
regression anyway. Otherwise, logistic regression with a suitable link
function works well. Of course, as mentioned earlier, the *cloglog*
link is what gets you closest to the proportional hazards model, but
usually it makes very little difference. See @kp02, pages 334--336, for
slightly more detail.


## Left or Right Censored Data.

Sometimes the available data is either left or right censored, i.e.,
for each individual $i$, we know survival of $t_i$ or not, i.e., data
$$(t_i, \delta_i), \quad i = 1, \ldots n, $$
$\delta_i = 1$ if survival (died after $t_i$) and $\delta_i = 0$ if not
(died before $t_i$).
It is still possible to estimate the survivor function $S$ nonparametrically.
The likelihood function is
\begin{equation*}
L(S) = \prod_{i=1}^n \left\{S(t_i)\right\}^{\delta_i}
                     \left\{ 1 - S(t_i)\right\}^{1 - \delta_i}
\end{equation*}
How to maximize this under the restrictions $t_i < t_j \Rightarrow S(t_i) \ge
S(t_j)$ with the
*EM algorithm* is shown by @gw92.
\index{Functions!\fun{coxreg}|)}
\index{Cox regression|)}

## The Weird Bootstrap

In small to medium-sized samples, the estimated parameters and their standard
errors may be not so reliable, and in such cases the bootstrap technique can be
useful. There are a couple of ways to implement it in Cox regression, 
and here the *weird bootstrap*\index{weird bootstrap}\index{bootstrap} is presented.

The resampling procedure is done risk set by riskset. If a specific risk set 
of size $n$ contains $d$ events, $d$ "new" events are drawn from the $n$ members 
of the risk set, without replacement, and with probabilities proportional to 
their scores. Then the regression parameter is estimated and saved in the 
bootstrap sample. This procedure is repeated a suitable number of times.

The "weird" part of the resampling procedure is the fact that it is performed 
independently over risk sets, that is, one individual who is present in two risk
sets may well have an event (for instance, die) in *both* risk sets. That is, 
the risk sets are treated as independent entities in this respect. See @abgk93 
for more detail.

The weird bootstrap is implemented in the `coxreg` function in the **eha** package.
As an example, child mortality in 19th century Sweden is analyzed with 
respect to the sex difference in mortality. 

```r
fit <- coxreg(Surv(enter, exit, event) ~ sex, boot = 300, 
              data = child)
b_hat <- fit$coefficents[1]
b_se <- sqrt(fit$var[1, 1])
b_sample <- fit$bootstrap[1, ]
```

This fitting procedure takes a fairly long time, around 19 minutes on a well equipped 
laptop (2018). The bootstrap result is a matrix where the number of rows is equal to the
number of estimated parameters (one in this case), and the number of columns is
equal to the number of bootstrap replicates (300 in this case). The `child` data set 
is fairly large with `r length(unique(child$id))` children and `r sum(child$event)`
deaths. The number of risk sets is 2497, with sizes between 20000 and 27000, and 
up to 60 events in one risk set.

```{r readboot6, echo = FALSE}
library(eha)
fit <- readRDS("bootfit.rds")
b_hat <- fit$coefficients[1]
b_se <- sqrt(fit$var[1, 1])
b_sample <- fit$bootstrap
```

Now it is up to the user to do whatever she usually does with bootstrap samples.
For instance, a start could be to get a grip of the distribution of  the bootstrap
sample around the parameter estimate, see Figure \@ref(fig:bootdist6).

```{r bootdist6, fig.cap = "Bootstrap sample distribution around the estimate."}
par(las = 1)
plot(density(b_sample - b_hat, bw = 0.005), 
     xlab = "Deviation", main = "", axes = FALSE)
xh <- c(-0.05, 0.00, 0.05)
axis(1, at = xh, labels = um(xh))
axis(2)
box()
abline(h = 0, v = 0)
```

The mean of the bootstrap sample is $-0.084$ and its sample 
standard deviation is `r round(sd(b_sample), 3)`. We can compare with the output 
of the standard  Cox regression, see Table \@ref(tab:bootc6).

```{r bootc6, echo = FALSE, results = 'asis'}
source("R/fit.out.R")
fit.out(fit, caption = "Child mortality, ordinary Cox regression.", label = "bootc6")
```

The mean is very close to the actual estimate, but the bootstrap standard error
is slightly smaller than the estimated standard error.

For more on the bootstrap, see @efrtib93, @hinkley88, and @hawi91. 